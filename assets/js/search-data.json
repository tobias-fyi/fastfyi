{
  
    
        "post0": {
            "title": "Automated Image Background Removal with Python",
            "content": ". Introduction . TL;DR . The goal of this tutorial is to describe one method of automating the process of cutting out objects (things, people, pets, etc.) from images and combining them to make a collage of sorts. . First, I go through creating binary masks for one or more objects in an image by using a class of computer vision algorithms called image segmentation. Binary mask(s) in hand(s), I go through one method (technically two, actually) of using said binary mask(s) to extract or remove part(s) of an image. Next, I do some basic image transformations (rotate, crop, and scale) on the resulting cutout. Finally, I paste the cutout on top of another image to make a collage. . Rather than drawing binary masks by hand or using proprietary software like Photoshop to manipulate and transform images, I&#39;ll show you how to automate the process using completely free, open-source tools. Namely, we&#39;ll be using Python along with a few open-source libraries: . NumPy | OpenCV - opencv-python | PyTorch - Detectron2 | Pillow | . . The Problem . Selecting and separating parts of an image can be a tedious, time-consuming process. Anyone who&#39;s done a fair amount of tinkering with image manipulation using a program like Photoshop knows the struggle. . Although modern tools make the process easier, wouldn&#39;t it be nice if there was a way to automate the process? . Creating &quot;Paws&quot; . As an example, say I&#39;d like to cut out my cat Hobbes from a photo in order to &quot;Photoshop&quot; him into a different image. Here&#39;s the photo of Hobbes I&#39;ll be using. . . I think his position is perfect for creating &quot;Hawbbes&quot; (Jaws + Hobbes)...meh I&#39;ll call it &quot;Paws&quot;. By cutting him out and rotating him a bit, he could be pasted onto an underwater shot of someone swimming and he could live his dream of being a fierce sharkitty. . Here&#39;s an image I found on Unsplash that would work as the background onto which Hobbes, once cut out of the image above, can be pasted. . . Basically, in order to cut Hobbes out of the photo above, I&#39;ll have to make all the pixels in the image transparent except for the ones representing Hobbes. Then I&#39;ll crop, rotate, scale, and superimpose the resulting image on top of the underwater shot such that Hobbes roughly takes up the bottom half. . . . Solution . Image Masking . To accomplish this manually, I could spend anywhere from a few minutes to a few hours outlining Hobbes in the original image to create a mask — masking the image. The time investment depends on how easily-separable the subject is from the rest of the image, how accurate I want the cut to be, and what tools are available to me. . Regarding that last point, the magicians at Adobe have done some rather impressive black magic with Photoshop, giving users very quick and very effective methods for selecting parts of an image. However, the goal of this post is to accomplish this programmatically, without the use of any closed-source software. . A mask is basically a method of distinguishing/selecting/separating pixels. If you&#39;ve never heard the term used this way before, one way to think about it is with masking tape and paint. Typically, one would put masking tape—i.e. create a &quot;mask&quot;—around areas on a wall that should not be painted. This is essentially what a mask is doing in any photo manipulation software: indicating what areas of an image to affect or transform (or which areas not to do so). . Here&#39;s the image of Hobbes with the image segmentation-generated masks overlayed on top of it (which we&#39;ll be creating later) showing, obviously, where Hobbes is in the image. It doesn&#39;t really matter that the model thinks he&#39;s a dog — we won&#39;t be using the predicted class, only the mask. And the mask is still good enough for our purposes. . . A binary mask is a method of masking which uses a two-tone color scheme, to indicate the areas of an image to be affected and not affected. By overlaying a binary mask on top of the original image, the boundaries between the two colors can be used to affect the different areas of the image differently, whether that is making pixels transparent (removing them) or applying some sort of effect or transformation. . The white area in the image below shows the same coordinates as the orange one above, converted into a binary mask. While I&#39;ve only spent any significant time with Photoshop, I&#39;d imagine any decent image manipulation software can work with binary masks similarly to how we&#39;ll be working with them. . . Computer vision . In order to generate binary masks based on the content of the image, the algorithm must be somewhat intelligent. That is, it must be able to process the image in such a way that it can recognize where the foreground is and draw a polygon around it with some degree of accuracy. . Luckily, there are a number of deep learning models that will do just that. The field is called Computer Vision, and the class of algorithm used in this article is known as image segmentation. . Don&#39;t worry if you don&#39;t have any experience with this type of thing, or even if you don&#39;t necessarily want to get experience with it. Modern machine learning tooling makes it incredibly quick and easy to get a model up and predicting with pre-trained weights. Though if you want to understand what&#39;s going on, it will likely help to know a bit of Python programming. . One caveat: the pre-trained models will usually work well with classes of objects that were in their training data. The model weights used in this post were trained on the COCO dataset, which contains 80 object classes. Depending on what the object in the foreground is that you are trying to extract, you may or may not need to extend the model with a custom dataset and training session. That is a topic for another post. . Detectron2 . The deep learning framework used here is PyTorch, developed by Facebook AI Research (FAIR). More specifically, we&#39;ll use a computer vision framework, also developed by FAIR, called Detectron2. . Although the primary framework used in this article is Detectron2, this process should be translatable to other image segmentation models as well. In fact, I&#39;ll be adding an addendum to this post in which I&#39;ll go over using Matterport&#39;s TensorFlow-based implementation of Mask R-CNN to accomplish the exact same thing. . Heck, while I&#39;m at it, I might as well do it with fastai as well. . End Result . I know you&#39;ve been dying to see the end result of the whole process. . Without any further ado, I present to you, Paws! . . [[Caption :: All of that was done with code. Pretty neat, eh?]] With that, let&#39;s get into how this masterpiece was created. . . . Setup . Install Detectron2 and other dependencies . As mentioned in the introduction, the framework we&#39;ll be using for image segmentation is called Detectron2. The following cells install and set up Detectron2 in a Google Colab environment (pulled from the official Detectron2 getting started notebook). If you don&#39;t want to use Colab for whatever reason, either play around with installation and setup or refer to the installation instructions. . The other top-level dependencies needed for this tutorial: . NumPy | opencv-python | Pillow | . The nice thing about Colab is all of these come pre-installed. Oh yeah, you also get free access to a GPU. Thanks, Googs! . Again, simply click the &quot;Open in Colab&quot; badge at the top of this page, then hit File &gt; Save a copy in Drive, which does exactly what it says: saves a copy of the notebook to your Google Drive. In addition, you can open an ephemeral copy of the notebook without saving it first by hitting File &gt; Open in playground mode. . Once you have everything installed, we can start with some imports and configuration. . # === Some imports and setup === # # Setup Detectron2 logger import detectron2 from detectron2.utils.logger import setup_logger setup_logger() # Common libraries import numpy as np import os, json, cv2, random # Only needed when running in Colab from google.colab.patches import cv2_imshow # Detectron2 utilities from detectron2 import model_zoo from detectron2.engine import DefaultPredictor from detectron2.config import get_cfg from detectron2.utils.visualizer import Visualizer from detectron2.data import MetadataCatalog, DatasetCatalog . . . . Running a pre-trained Detectron2 model . Most, if not all, open-source deep learning frameworks have a set of pre-trained weights available to use. The creators of the frameworks will conduct a series of training sessions on the most commonly-used datasets in order to benchmark the performance of their algorithms. Luckily for everyone else, they typically provide the results of this training in the form of weights, which can be loaded into the model and be used for inference immediately. . For many tasks, including recognizing and outlining an image of a cat, pre-trained weights will work fine. The model weights used in this post were trained on the popular COCO dataset, which contains 80 object classes, including cats. If, for example, we wanted to do the same thing with whales or one specific person, we&#39;d have to do some custom training. . I will be publishing a companion blog post to this one about training Detectron2 on a custom dataset. Once that is published, I&#39;ll link to it here. If there&#39;s no link yet, I haven&#39;t published it yet. . If you&#39;re curious about custom training now, the Detectron2 &quot;Getting Started&quot; Colab notebook also goes through one way of doing so. . Image loading and image arrays . The first thing we need in order to use the model is an image on which it can be used. . We first download the image to the local filesystem using wget, then load it into memory using cv2 (opencv-python). . !wget https://raw.githubusercontent.com/tobias-fyi/assetstash/master/visual/images/img_seg_bin_mask/01_hobbes.jpg im = cv2.imread(&quot;./01_hobbes.jpg&quot;) . --2020-07-20 19:35:16-- https://raw.githubusercontent.com/tobias-fyi/assetstash/master/visual/images/img_seg_bin_mask/01_hobbes.jpg Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ... Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 344670 (337K) [image/jpeg] Saving to: ‘01_hobbes.jpg’ 01_hobbes.jpg 100%[===================&gt;] 336.59K --.-KB/s in 0.05s 2020-07-20 19:35:16 (7.01 MB/s) - ‘01_hobbes.jpg’ saved [344670/344670] . If you think about what a digital image actually is, it makes sense to represent it as a matrix — each row corresponds to a row of pixels, and each column a column of pixels in the image. Technically, images would be considered a 3-dimensional array, because they have width, height, and depth (number of channels). . Depending on if the image has three channels (typically RGB: red, green, blue) or four (typically RGBA: same plus an alpha channel), the values at each row-column index (or cell, like in a spreadsheet, in case that helps you visualize it) indicate the intensities of each of the color channels (and transparency, in the case of 4-channel images) for each pixel. . Thus, after the image is loaded, it really is just an array of numbers and can be utilized and manipulated just like any other array. For example, in order to rotate the image, a linear transformation can be applied to the image matrix to &quot;rotate&quot; the pixel values within the matrix. . Here is an example of a single row in the array representing the image of Hobbes is shown. . # === Look at the image, in array form === # print(&quot;Image dimensions:&quot;, im.shape) print(&quot; nImage array - first row of 3-value sub-arrays:&quot;) im[0] . . Image dimensions: (800, 600, 3) Image array - first row of 3-value sub-arrays: . array([[172, 192, 209], [188, 208, 225], [119, 137, 154], ..., [137, 151, 149], [139, 153, 151], [142, 156, 154]], dtype=uint8) . # === Look at the image, rendered === # cv2_imshow(im) . . . Inference with Detectron2 . After the image is loaded, we&#39;re ready to use Detectron2 to run inference on the image and find the mask of Hobbes. Running inference means generating predictions from the model. In the case of image segmentation, the model is making a prediction for each pixel, providing its best guess at what class of object each one belongs to, if any. . We create a Detectron2 config and instantiate a DefaultPredictor, which is then used to run inference. . Just a heads-up: the first time this runs, it will automatically attempt to start downloading the pre-trained weights — a ~180mb pickle file. That&#39;s a lot of pickles... . In addition to downloading and configuring the weights, the threshold is set for the minimum predicted probability. In other words, the model will only output a prediction if it is certain enough — the probability assigned to the prediction is above the threshold. . # Add project-specific config (e.g., TensorMask) here if you&#39;re # not running a model in Detectron2&#39;s core library cfg = get_cfg() cfg.merge_from_file(model_zoo.get_config_file(&quot;COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml&quot;)) cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5 # set threshold for this model # Find a model from detectron2&#39;s model zoo. You can use the https://dl.fbaipublicfiles... url as well cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(&quot;COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml&quot;) predictor = DefaultPredictor(cfg) outputs = predictor(im) . model_final_f10217.pkl: 178MB [00:05, 29.7MB/s] . By default, the output of the model contains a number of results, including the predicted classes, coordinates for the bounding boxes (object detection), and mask arrays (image segmentation), along with various others, such as pose estimation (for people). More information on the types of predictions made by Detectron2 can be found in the documentation. . We are really only interested in the one mask outlining Mr. Hobbes here, though will also need to extract the IDs for the predicted classes in order to select the correct mask. If the image only has one type of object, then this part isn&#39;t really necessary. But when there are many different classes in a single image, it&#39;s important to be certain which object we are extracting. . First, let&#39;s take a look at how the model did. . v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=0.8) out = v.draw_instance_predictions(outputs[&quot;instances&quot;].to(&quot;cpu&quot;)) cv2_imshow(out.get_image()[:, :, ::-1]) . . Extracting the mask . The &quot;dog&quot; is the first id in the .pred_classes list. Therefore, the mask that we want is the first one in the .pred_masks tensor (array). . Those colored areas are the &quot;masks&quot;, which can be extracted from the output of the model and used to manipulate the image in neat ways. First, we&#39;ll need to get the array holding the mask. . In this case, as can be seen below, each mask is a 2-dimensional array of Boolean values, each one representing a pixel. If a pixel has a &quot;True&quot; value, that means it is inside the mask, and vice-versa. . print(outputs[&quot;instances&quot;].pred_classes) print(outputs[&quot;instances&quot;].pred_masks) . tensor([16, 57, 57], device=&#39;cuda:0&#39;) tensor([[[False, False, False, ..., False, False, False], [False, False, False, ..., False, False, False], [False, False, False, ..., False, False, False], ..., [False, False, False, ..., False, False, False], [False, False, False, ..., False, False, False], [False, False, False, ..., False, False, False]], [[False, False, False, ..., False, False, False], [False, False, False, ..., False, False, False], [False, False, False, ..., False, False, False], ..., [False, False, False, ..., False, False, False], [False, False, False, ..., False, False, False], [False, False, False, ..., False, False, False]], [[False, False, False, ..., False, False, False], [False, False, False, ..., False, False, False], [False, False, False, ..., False, False, False], ..., [False, False, False, ..., False, False, False], [False, False, False, ..., False, False, False], [False, False, False, ..., False, False, False]]], device=&#39;cuda:0&#39;) . # List of all classes in training dataset (COCO) # predictor.metadata.as_dict()[&quot;thing_classes&quot;] # The class we are interested in predictor.metadata.as_dict()[&quot;thing_classes&quot;][16] . &#39;dog&#39; . # Find the index of the class we are interested in # First, convert to numpy array to allow direct indexing class_ids = np.array(outputs[&quot;instances&quot;].pred_classes.cpu()) class_index = np.where(class_ids == 16) # Find index where class ID is 16 # Use that index to index the array of masks and boxes mask_tensor = outputs[&quot;instances&quot;].pred_masks[class_index] print(mask_tensor.shape) mask_tensor . torch.Size([1, 800, 600]) . tensor([[[False, False, False, ..., False, False, False], [False, False, False, ..., False, False, False], [False, False, False, ..., False, False, False], ..., [False, False, False, ..., False, False, False], [False, False, False, ..., False, False, False], [False, False, False, ..., False, False, False]]], device=&#39;cuda:0&#39;) . hobbes_mask = mask_tensor.cpu() print(&quot;Before:&quot;, type(hobbes_mask)) print(hobbes_mask.shape) hobbes_mask = np.array(hobbes_mask[0]) print(&quot;After:&quot;, type(hobbes_mask)) print(hobbes_mask.shape) . Before: &lt;class &#39;torch.Tensor&#39;&gt; torch.Size([1, 800, 600]) After: &lt;class &#39;numpy.ndarray&#39;&gt; (800, 600) . hobbes_mask . array([[False, False, False, ..., False, False, False], [False, False, False, ..., False, False, False], [False, False, False, ..., False, False, False], ..., [False, False, False, ..., False, False, False], [False, False, False, ..., False, False, False], [False, False, False, ..., False, False, False]]) . . Converting to a binary mask . Now that we&#39;ve run inference on the image and retrieved the mask array, it&#39;s time to turn that array into a binary mask. While I won&#39;t be using this particular binary mask directly, it can be downloaded as a png and/or edited and used to various ends. . # The &quot;True&quot; pixels will be converted to white and copied onto the black background background = np.zeros(hobbes_mask.shape) background.shape . (800, 600) . bin_mask = np.where(hobbes_mask, 255, background).astype(np.uint8) print(bin_mask.shape) bin_mask . (800, 600) . array([[0, 0, 0, ..., 0, 0, 0], [0, 0, 0, ..., 0, 0, 0], [0, 0, 0, ..., 0, 0, 0], ..., [0, 0, 0, ..., 0, 0, 0], [0, 0, 0, ..., 0, 0, 0], [0, 0, 0, ..., 0, 0, 0]], dtype=uint8) . cv2_imshow(bin_mask) . . Using the binary mask to cut out Hobbes . In order to use numpy operations between the mask and image, the dimensions of the mask must match the image. The image array has three values for each pixel, indicating the values of red, green, and blue (RGB) that the pixel should render. Therefore, the mask must also have three values for each pixel. To do this, I used a NumPy method called np.stack to basically &quot;stack&quot; three of the masks on top of one another. . Once the dimensions match, another NumPy method, np.where, can be used to copy or extract only the pixels contained within the area of the mask. I created a blank background onto which those pixels are copied. . # Split into RGB (technically BGR in OpenCV) channels b, g, r = cv2.split(im.astype(&quot;uint8&quot;)) # Create alpha channel array of ones # Then multiply by 255 to get the max transparency value a = np.ones(hobbes_mask.shape, dtype=&quot;uint8&quot;) * 255 print(b.shape, g.shape, r.shape, a.shape) . (800, 600) (800, 600) (800, 600) (800, 600) . # We want the image to be fully opaque at this point a . array([[255, 255, 255, ..., 255, 255, 255], [255, 255, 255, ..., 255, 255, 255], [255, 255, 255, ..., 255, 255, 255], ..., [255, 255, 255, ..., 255, 255, 255], [255, 255, 255, ..., 255, 255, 255], [255, 255, 255, ..., 255, 255, 255]], dtype=uint8) . # Rejoin with alpha channel that&#39;s always 1, or non-transparent rgba = [b, g, r, a] # Both of the lines below accomplish the same thing im_4ch = cv2.merge(rgba, 4) # im_4ch = np.stack([b, g, r, a], axis=2) print(im_4ch.shape) cv2_imshow(im_4ch) . (800, 600, 4) . # Create 4-channel blank background bg = np.zeros(im_4ch.shape) print(&quot;BG shape:&quot;, bg.shape) # Create 4-channel mask mask = np.stack([hobbes_mask, hobbes_mask, hobbes_mask, hobbes_mask], axis=2) print(&quot;Mask shape:&quot;, mask.shape) # Copy color pixels from the original color image where mask is set foreground = np.where(mask, im_4ch, bg).astype(np.uint8) # Check out the result cv2_imshow(foreground) . BG shape: (800, 600, 4) Mask shape: (800, 600, 4) . . The Roundabout Method . This is that &quot;second&quot; method I talked about in the introduction. . This is how I added a fourth channel to the image after the fact, once the colored pixels had been copied onto a black background. While this method works, I&#39;m sure you can think of one primary issue with it. . It took me too long to realize this, but by using a black background and the method below, which converts all black pixels to transparent, any pixels brought over from the original image that also happened to be black were converted to transparent. . That&#39;s why I decided to refactor into the method above. . However, I felt like I should leave it in anyways, as it still has some potentially useful code in it. For example, in the case when the image cannot be converted to four channels beforehand. . bg = np.zeros(im.shape) bg.shape . (800, 600, 3) . mask = np.stack([hobbes_mask, hobbes_mask, hobbes_mask], axis=2) . foreground = np.where(mask, im, bg).astype(np.uint8) . # i.e. add the alpha channel and convert black pixels to alpha tmp = cv2.cvtColor(foreground.astype(&quot;uint8&quot;), cv2.COLOR_BGR2GRAY) _, alpha = cv2.threshold(tmp, 0, 255, cv2.THRESH_BINARY) b, g, r = cv2.split(foreground.astype(&quot;uint8&quot;)) rgba = [b, g, r, alpha] dst2 = cv2.merge(rgba, 4) # Look at the result, if needed # cv2_imshow(dst2) . . . Image manipulation with Python . Now, this image can be saved (as a PNG to preserve the alpha channel/transparency) and simply overlayed onto another image. Or, even better, the image can be used directly (as it is now, in the form of an array), scaled, rotated, moved, then pasted overtop of the other image. . At first I was going to use Photoshop to overlay Hobbes and make him look like a super dangerous sharkitty. But then I remembered the goal of this post, and decided to do it programmatically with Python. . The primary library I&#39;ll be using to manipulate images is Pillow. . from PIL import Image # Use plt to display images import matplotlib.pyplot as plt %matplotlib inline . !wget https://raw.githubusercontent.com/tobias-fyi/assetstash/master/visual/images/img_seg_bin_mask/05_jaws_kinda.jpg -q -O 05_jaws_kinda.jpg . jaws_img = Image.open(&quot;05_jaws_kinda.jpg&quot;) # Dimensions of background image (600, 900) will be useful later print(jaws_img.size) plt.imshow(jaws_img) . (600, 900) . &lt;matplotlib.image.AxesImage at 0x7f97f3cd0748&gt; . . Rotation . I decided to rotate the image matrix directly using OpenCV, prior to loading it into Pillow. There is a .rotate method on Pillow&#39;s Image class as well, which could accomplish the same thing. Either way works — I just wanted to learn how to do it with OpenCV. . # Found here: https://stackoverflow.com/a/9042907/10589271 # There is another (potentially easier) way to do it with Pillow, using Image.rotate() def rotate_image(image, angle): image_center = tuple(np.array(image.shape[1::-1]) / 2) rot_mat = cv2.getRotationMatrix2D(image_center, angle, 1.0) result = cv2.warpAffine(image, rot_mat, image.shape[1::-1], flags=cv2.INTER_LINEAR) return result . fg_rotated = rotate_image(foreground, 45) cv2_imshow(fg_rotated) . Load into Pillow . Once the image is rotated, it needs to be cropped and scaled appropriately. I decided to use Pillow for these transformations. For some reason that I have not looked into yet, OpenCV images are in BGR (blue, green, red) format instead of the virtually universal RGB format. Thus, in order to load the image from cv2 into Pillow without reversing the blues and reds, the color first must be converted to RGB. . Once converted, it can simply be loaded into Pillow as a PIL.Image object, which contains a suite of useful methods for transformation and more. . # Convert color from BGRA to RGBA fg_rotated_fixed = cv2.cvtColor(fg_rotated, cv2.COLOR_BGRA2RGBA) # Load into PIL.Image from array in memory hobbes_rotated = Image.fromarray(fg_rotated_fixed) plt.imshow(hobbes_rotated) . &lt;matplotlib.image.AxesImage at 0x7f706946a470&gt; . Crop . I manually defined the coordinates of the box used to crop the image by eyeballing it (shout out to Matplotlib for including tick marks on rendered images). A more automated method for doing this would be to extract the bounding box coordinates from the model output, and use that to crop the image before doing any other transformations. I will add this to a later iteration of this tutorial (if you&#39;re reading this, it likely hasn&#39;t been implemented yet). . box = (0, 80, 480, 500) crop = hobbes_rotated.crop(box2) print(crop.size) plt.imshow(crop) . (480, 420) . &lt;matplotlib.image.AxesImage at 0x7f70693ac208&gt; . Resize . Next, the cropped and rotated image must be resized in order to fill up the entire width of the background onto which it will be pasted. The reason for this is simply because the dimensions of the &quot;paste&quot; box must exactly match that of the &quot;copy&quot; box. It must be explicit — i.e. it&#39;s not like Photoshop where any of the image falling outside the boundaries of the &quot;canvas&quot; is cropped or simply left out. . width = jaws_img.size[0] scale = width / crop.size[0] # Calculate scale to match width height = int(scale * crop.size[1]) # Scale up height accordingly new_size = (width, height) # Resize! resized = crop.resize(new_size) print(resized.size) plt.imshow(resized) . (600, 525) . &lt;matplotlib.image.AxesImage at 0x7f7069ca30b8&gt; . Save! . Finally, with the rotated, cropped, and resized image of Hobbes all ready to go, we can paste it onto the background and save the result! . paws = jaws_img.copy() # Paste box dimensions have to exactly match the image being pasted paste_box = (0, paws.size[1] - resized.size[1], paws.size[0], paws.size[1]) paws.paste(resized, paste_box, mask=resized) plt.imshow(paws) . &lt;matplotlib.image.AxesImage at 0x7f706c1ae550&gt; . paws.save(&quot;06_paws.jpg&quot;) . . . . Final Thoughts . What a masterpiece! . Potential improvements . As with just about any process, there are always aspects that could be changed in an effort to improve efficiency or ease-of-use. I will keep adding to this list of potential improvements as I think or hear of them: . Fully automated the crop by using the bounding box from the model output | . Further Work . There are a couple of additional adjacent projects to this that I&#39;d like to work on at some point in the future, both to help others use the method outlined above, and to give me practice with various other aspects of the development lifecycle. . The two projects I have in mind are essentially the same thing, accomplished different ways. First, I&#39;d like to integrate this method into a Python package and put it on PyPI so it can be installed with Pip and easily customized/used from the command line or in other scripts. Second, I want to build a simple web app and API that would allow anyone to upload an image, choose a class of object to extract (or remove), run the model, then download the image with the background (and/or other objects) removed. . As I work on these, I&#39;ll put links here. .",
            "url": "https://tobias-fyi.github.io/fastfyi/blog/remove-bg-python",
            "relUrl": "/blog/remove-bg-python",
            "date": " • Jul 20, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Trash Panda",
            "content": ". Introduction . The Problem . You have an object in your hand that you intend to throw away. When you think about it as you&#39;re walking to the bins, you realize you actually don&#39;t know whether this type of object is recyclable or not. Maybe it is made of multiple different materials, or of an uncommon or unrecognizable material. . You&#39;re in the middle of an important project, and it&#39;s crunch time—no extra time available to spend researching. You end up throwing it in the recycling because it...well, it seems like something that would be recyclable. With the decision made and action taken, you return to your important project, forgetting all about what just transpired. . I&#39;d bet that most who are reading this have had an experience like this. . The priceless time and energy spent researching how to properly dispose of every single item can add up. However, the US is in something of a recycling crisis at the moment, partially due to the low quality of our recyclable waste—it tends to be very intermixed with non-recyclables. . Ever since China&#39;s National Sword legislation in 2017, which vastly reduced the amount of foreign recycling—particularly low-quality—the country would accept, recycling companies in the US have been forced to grapple with this quality issue. The cost of recycling increases when more trash is intermingled with it, as more sorting has to occur prior to processing. Whether it is more expensive machines or higher cost of labor, sorting costs money. . While the domestic recycling infrastructure will (hopefully) grow to meet the increasing demand, the best way to solve a problem is to address the source of the issue, not the symptoms. One key reason for the low quality recycling is simply a lack of easily accessible information. Even with the power of modern search engines at our fingertips, finding relevant recycling information can take a long time, as what exactly constitutes recycling changes depending on area and company. . The simple fact is that most people don&#39;t want to spend the additional time it takes (at least up front) to have good recycling habits. So why not simply remove that additional time from the equation? . . The Solution . The goal was to build an app that helps to foster better recycling habits by reducing the the effort needed to find accurate and relevant information on how to properly dispose of any given item of waste. To make this possible, we needed to reduce the friction so much so that looking up how to dispose of something that a user is holding in their hand is just as quick and easy as debating for a few moments on what bin it goes in. . Put another way, our goal was to reduce the cognitive tax of getting relevant recycling information so much that disposing of every item of waste properly, regardless of what it is, becomes effortless. . Our stakeholder envisioned that the user would simply snap a photo of something they are about to toss. Then, the app&#39;s computer vision (object detection) functionality would recognize the object and automatically pull up the relevant information on how it should be disposed of according to the user&#39;s location and available services. The user would know immediately if the item should be thrown in the trash, recycle, or compost, or if it is recyclable only at an offsite facility. For the latter case, the user would be able to view a list of nearby facilities that accept the item or material. . The result of this vision is a progressive web app (PWA) called Trash Panda, which does just that. You can try out the app on your mobile device now by following the link below. . The Trash Panda app (meant for mobile) . A note on PWAs . For those who aren&#39;t familiar, a PWA is basically a web app that can both be used via the browser and downloaded to the home screen of a mobile device. Google has been moving to fully support PWAs, meaning Trash Panda is available on the Play Store right now. Of course the benefit of a PWA is you don&#39;t actually have to download it at all if you don&#39;t want to. You can use it directly from the browser. . Apple is pretty far behind in their support of PWAs. As a result, the behavior on an iOS device is not ideal. For those on iOS, be sure to use Safari. And when taking a picture of an item, you have to exit out of the video window before pressing the normal shutter button. . You&#39;ll figure it out—we believe in you! . The Team (and My Role On It) . For eight weeks near the beginning of 2020, I worked with a remote interdisciplinary team to bring the vision of Trash Panda to life. . . Trash Panda is by far the most ambitious machine learning endeavor I had yet embarked on. Indeed, it was the largest software project I&#39;d worked on in just about every respect: time, team, ambition, breadth of required knowledge. As such, it provided to me many valuable, foundational experiences that I&#39;ll surely keep with me throughout my entire career. . I seriously lucked out on the team for this project. Every single one of them was hard-working, thoughtful, friendly—a pleasure to work with. The team included myself and three other machine learning engineers, four web developers, and two UX designers (links to all of their respective sites in the Final Thoughts section below). Our stakeholder Trevor Clack, who came up with the idea for the app and pitched it to Labs, also worked on the project as a machine learning engineer. . Trevor&#39;s Trash Panda blog post We all pushed ourselves throughout each and every day of the eight weeks to make Trevor&#39;s vision come to life, learning entirely new technologies, frameworks, skills, and processes along the way. . For example, the web developers taught themselves how to use GraphQL, along with a variety of related/dependent technology. On the machine learning side of things, none of us had significant applied experience with computer vision (CV) going into the project. We&#39;d spent a few days studying and working with it in the Deep Learning unit of our Lambda School curriculum. But that was more to expose us to it, rather than covering the entire process in-depth. We had only the shallowest of surface scratches compared to what was ultimately needed to meet the vision set out for us. . As the machine learning engineers on the team, we were responsible for the entire process of getting an object detection system built, trained, deployed, and integrated with the app. Basically, we were starting from scratch, both in the sense of a greenfield project and of us being inexperienced with CV. . Of course, CV is still machine learning—many steps in the process are similar to any other supervised machine learning project. But working with images comes with its own suite of unique challenges that we had to learn how to overcome. . We split up the work as evenly as possible, given our initially limited knowledge of the details, with some steps being split up between some or all of us, and other steps having a sole owner. . The first step for which I was solely responsible included building a system to automatically remove the background from images (or extract the foreground, depending on how you look at it). Essentially, when tasked with figuring out a way to automate the process of removing the background from images so they could be auto-labeled via a script written by Trevor, I built a secondary pipeline with a pre-trained image segmentation model. More details can be found in the Automated Background Removal section below. . Furthermore, I was responsible for building and deploying the object detection API. I built the API using Flask, containerized it with Docker, and deployed it to AWS Elastic Beanstalk. I go into a little more detail in the Deployment section below, though I will be digging into the code and process of the API much more in a separate blog post. . All members of the machine learning team contributed to the gathering and labeling of the dataset. To this end, each of us ended up gathering and labeling somewhere in the range of 20,000 images, for a total of over 82,000. . . The Data . As is the case with most, if not all, machine learning projects, we spent the vast majority of the time gathering and labeling our dataset. Before we could get into gathering the actual data, however, we needed to define what data we needed to gather and how we were going to gather it. . Classifications . As also seems to be the case with most, if not all, projects in general, we were almost constantly grappling with scope management. In an ideal world, our model would be able to recognize any object that anyone would ever want to throw away. But in reality is this is practically impossible, particularly within the 8 weeks we had to work on Trash Panda. I say &quot;practically&quot; because I&#39;m sure if a company dedicated enough resources to the problem, eventually it could be solved, at least to some degree. . Fortunately, we were granted an API key from Earth911 (shoutout to them for helping out!) to utilize their recycling center search database. At the time we were working with it, the database held information on around 300 items—how they should be recycled based on location, and facilities that accept them if they are not curbside recyclable. They added a number of items when we were already most of the way done with the project, and have likely added more since then. . We had our starting point for the list of items our system should be able to recognize. However, the documentation for the neural network architecture we&#39;d decided to use suggested that to create a robust model, it should be trained with at least 1,000 instances (in this case, images) of each of the classes we wanted it to detect. . Gathering 300,000 images was also quite a bit out of the scope of the project at that point. So, the DS team spent many hours reducing the size of that list to something a little more manageable and realistic. . The main method of doing so was to group the items based primarily on visual similarity. We knew it was also out of the scope of our time with the project to train a model that could tell the difference between #2 plastic bottles and #3 plastic bottles, or motor oil bottles and brake fluid bottles. . . Given enough time and resources, who knows? Maybe we could train a model that accurately recognizes 300+ items and distinguishes between similar-looking items. But we had to keep our scope realistic to be sure that we actually finished something useful in the time we had. . We also considered the items that 1) users would be throwing away on a somewhat regular basis, and 2) users would usually either be unsure of how to dispose of properly or would dispose of properly. More accuracy on the important and/or common items would be more valuable to users. Some items were not grouped. . By the end of this process, we managed to cluster and prune the original list of about 300 items and materials down to 73. . Image Data Pipelines . We figured that in order to get through gathering and labeling 70,000+ images with only four people, within our timeframe, and without any budget whatsoever, we had to get creative and automate as much of the process as possible. . As explained below, the image gathering step required significant manual labor. However, we had a plan in place for automating most of the rest of the process. Below is a general outline of the image processing and labeling pipeline we built. . Rename the images to their md5sum hash to avoid duplicates and ensure unique filenames | Resize the images to save storage (and processing power later on) | Discern between transparent background and non-transparent background | If image is non-transparent, remove the background | Automatically draw bounding boxes around the object (the foreground) | If image is transparent, add a background | The beauty of automation is once the initial infrastructure is built and working, the volume can be scaled up indefinitely. The auto-labeling functionality was not perfect by any means. But it still felt great watching this pipeline rip through large batches of images. . Gather . The first part of the overall pipeline was gathering the images—around 1,000 for each of the 73 classes. This was a small pipeline in its own right, which unfortunately involved a fair bit of manual work. . Timothy built the piece of the image gathering pipeline that allowed us to at least automate some of it—the downloading part. Bing ended up being the most fruitful source of images for us. Before starting we expected to use Google Images, but pivoted when it turned out that Bing&#39;s API was much more friendly to scraping and/or programmatically downloading. . Timothy&#39;s blog post about the Trash Panda project can be found here: Games by Tim - Trash Panda. . We used his script, which in turn used a Python package called Bulk-Bing-Image-downloader, to gather the majority of images. (I say majority because we also used some images from Google&#39;s Open Images Dataset and COCO.) . The gathering process was a mixture of downloading and sifting. As we were pulling images straight from the internet, irrelevant images inevitably found their way into the search queries somehow. For each class, we went through a iterative (and somewhat tedious) loop until we had around 1,000 images of that class. The steps were simple, yet time-consuming: . Gather a batch of images (usually several hundred at a time) from Bing using a script written by Timothy | Skim through them, removing irrelevant and/or useless images | The latter was what made this pipeline difficult to fully automate, as we couldn&#39;t think of any good way of automatically vetting the images. (Though I did write a script to help me sift through images as quickly as possible, which will be the topic of a future blog post.) We had to be sure the images we were downloading in bulk actually depicted the class of item in question. . As they say, &quot;garbage in, garbage out.&quot; . And in case you weren&#39;t aware, the internet is full of garbage. . Annotate . To train an object detection model, each image in the training dataset must be annotated with rectangular bounding boxes (or, more accurately, the coordinates that define the bounding box) surrounding each of the objects belonging to a class that we want the model to recognize. These are used as the label, or target, for the model—i.e. what the model will be trying to predict. . Trevor came up with an idea to automate the labeling part of the process—arguably the most time-intensive part. Basically, the idea was to use images that feature items over transparent backgrounds. All of the major search engines allow an advanced search for transparent images. If the item is the only object in the image, it is relatively simple and straightforward to write a script that draws a bounding box around it. . If you&#39;d like some more detail on this, Trevor wrote a blog post about it. . automated bbox . Automated Background Removal . There was one big issue with this auto-labeling process. Finding a thousand unique images of a single class of object is already something of a task. And depending on the object, finding that many without backgrounds is virtually impossible. . For the images that had backgrounds, we would either have to manually label them, or find a way to automate the process and build it into the pipeline. Because the script to label images without backgrounds was already written and working, we decided to find a way of automatically removing the background from images. . This is the part of the pipeline that I built. . I&#39;ll give a brief overview here of how I built a system for automatically removing backgrounds from images. If you&#39;re curious about the details, I wrote a separate blog post on the topic: . Automated Image Background Removal with Python I tested out a few different methods of image background removal, with varying degrees of success. The highest quality results came from creating a batch task in Adobe Photoshop, as whatever algorithm powers its &quot;Select Foreground&quot; functionality is very consistent and accurate. However, we could not use this method because it could not be added to the pipeline—I was the only one with a license, meaning the speed of my computer and internet could cause a major bottleneck. . Another method I tested out was the OpenCV implementation of an algorithm called GrabCut. Long story short, I wasn&#39;t able to get the quality we needed from it, as can be seen below. . . The main issue is that the algorithm is &quot;interactive&quot;. That is, it uses the coordinates of a rectangle surrounding the object in the image for which the foreground should be extracted. For best results, the coordinates are generated manually for each image. The tighter that rectangle surrounds the foreground, the better the outline will be. . The above image is my attempt at simply using the entire image as the rectangle. As can be seen below, the result was much better when I tightened the rectangle around the can. I tried for many hours to find an automatable solution to this, but could not get results that were quite good enough. I decided to move onto other strategies. . . Ultimately, I ended up building a short image processing pipeline that utilized a pre-trained image segmentation model (similar to object detection) to find the object(s) in the image. I initially built the pipeline with a library called Detectron2, based on PyTorch. However, after running into some issues with it, I decided to reimplement the pipeline using Mask R-CNN, based on TensorFlow. . . Part of the output of the image segmentation model is a series of coordinates that describe an outline of the object(s) in the image. I used that as a binary mask to define the area of the image that should be kept, making the rest of it transparent. . . Unfortunately, I did not have much time to spend on improving the performance of the image segmentation model, and as a result there was still a fair amount of manual labeling to be done after the pipeline. I could (and should) have trained the image segmentation model using a small subset of images from each class. This would&#39;ve made the output mask much more accurate and reduced the time spent fixing the labels afterwards. . As it was, using only the pretrained weights, there were some object classes that it performed very well on, while for others it did not. . Running the Pipeline . As with building the pipeline, we split up the 73 classes evenly amongst the four of us (around 18 classes each) and got to work gathering and labeling the images. . If we&#39;d had a couple more weeks to spend improving the pipeline, this process likely would not have taken so long or been so tedious. As it was, we spent the better part of 4 weeks gathering and labeling the dataset. . I believe the pipeline did save us enough time to make it worth the extra time it took to build. Plus, I got to learn and apply some cool new tech! . . The Model . Architecture . The neural network architecture we used to train the main object detection system used in the app is called YOLOv3: You Only Look Once, version 3. . . YOLOv3 is a state-of-the-art single-shot object detection system originally written in C. One of the main reasons we used this one in particular was its speed—a benefit of single-shot algorithms. With the YOLO algorithms, object detection can be run in real time. For example, it could be run on a live security camera feed, detecting when someone enters a property. . Training . The vast majority of the work we put into this model was building a high-quality dataset. We used transfer learning to give the model an initial training kickstart. We were able to benefit from previous training done by the algorithm&#39;s developers by loading the weights from convolutional layers that were trained on ImageNet. . Our model was trained on a GPU-enabled AWS Sagemaker instance. After about 60 hours, our model reached a total average precision of 54.71%. . As expected, the model performs much better on certain classes of objects. The more easily recognizable classes (tires, printers, disks, digital cameras, plastic bottles) had average precisions in the 80-90% range. . On the other hand, the lower precision object classes were usually those that took on a wider variety of shapes, textures, and colors. For example, musical instruments, food waste, office supplies. It makes sense that, given a similar amount of training data, classes like this would be more difficult to distinguish. . Here is the breakdown for the 13,000 weights - mAP (mean average precision): . detections_count = 82369, unique_truth_count = 10934 class_id = 0, name = aerosol_cans, ap = 54.87% (TP = 54, FP = 45) class_id = 1, name = aluminium_foil, ap = 42.11% (TP = 32, FP = 22) class_id = 2, name = ammunition, ap = 55.38% (TP = 61, FP = 49) class_id = 3, name = auto_parts, ap = 41.70% (TP = 31, FP = 21) class_id = 4, name = batteries, ap = 62.19% (TP = 92, FP = 44) class_id = 5, name = bicycles, ap = 79.86% (TP = 86, FP = 22) class_id = 6, name = cables, ap = 64.81% (TP = 76, FP = 40) class_id = 7, name = cardboard, ap = 52.99% (TP = 50, FP = 41) class_id = 8, name = cartridge, ap = 70.16% (TP = 68, FP = 25) class_id = 9, name = cassette, ap = 53.45% (TP = 13, FP = 10) class_id = 10, name = cd_cases, ap = 80.11% (TP = 30, FP = 3) class_id = 11, name = cigarettes, ap = 29.43% (TP = 38, FP = 56) class_id = 12, name = cooking_oil, ap = 69.23% (TP = 61, FP = 25) class_id = 13, name = cookware, ap = 70.76% (TP = 81, FP = 83) class_id = 14, name = corks, ap = 57.99% (TP = 55, FP = 32) class_id = 15, name = crayons, ap = 52.43% (TP = 44, FP = 25) class_id = 16, name = desktop_computers, ap = 75.17% (TP = 34, FP = 12) class_id = 17, name = digital_cameras, ap = 93.40% (TP = 120, FP = 15) class_id = 18, name = disks, ap = 90.14% (TP = 90, FP = 25) class_id = 19, name = doors, ap = 0.00% (TP = 0, FP = 0) class_id = 20, name = electronic_waste, ap = 73.97% (TP = 50, FP = 16) class_id = 21, name = eyeglasses, ap = 21.75% (TP = 24, FP = 37) class_id = 22, name = fabrics, ap = 52.17% (TP = 69, FP = 52) class_id = 23, name = fire_extinguishers, ap = 30.32% (TP = 22, FP = 30) class_id = 24, name = floppy_disks, ap = 78.26% (TP = 83, FP = 53) class_id = 25, name = food_waste, ap = 19.16% (TP = 28, FP = 22) class_id = 26, name = furniture, ap = 3.45% (TP = 0, FP = 0) class_id = 27, name = game_consoles, ap = 58.90% (TP = 45, FP = 19) class_id = 28, name = gift_bags, ap = 65.97% (TP = 72, FP = 48) class_id = 29, name = glass, ap = 72.21% (TP = 149, FP = 49) class_id = 30, name = glass_container, ap = 0.00% (TP = 0, FP = 0) class_id = 31, name = green_waste, ap = 57.38% (TP = 61, FP = 55) class_id = 32, name = hardware, ap = 24.78% (TP = 28, FP = 62) class_id = 33, name = hazardous_fluid, ap = 79.06% (TP = 85, FP = 10) class_id = 34, name = heaters, ap = 71.18% (TP = 54, FP = 50) class_id = 35, name = home_electronics, ap = 32.91% (TP = 83, FP = 92) class_id = 36, name = laptop_computers, ap = 41.66% (TP = 95, FP = 55) class_id = 37, name = large_appliance, ap = 5.91% (TP = 7, FP = 38) class_id = 38, name = lightbulb, ap = 28.36% (TP = 61, FP = 28) class_id = 39, name = medication_containers, ap = 59.85% (TP = 88, FP = 46) class_id = 40, name = medications, ap = 55.37% (TP = 68, FP = 39) class_id = 41, name = metal_cans, ap = 52.21% (TP = 82, FP = 24) class_id = 42, name = mixed_paper, ap = 37.32% (TP = 64, FP = 43) class_id = 43, name = mobile_device, ap = 63.24% (TP = 151, FP = 97) class_id = 44, name = monitors, ap = 39.15% (TP = 75, FP = 77) class_id = 45, name = musical_instruments, ap = 20.67% (TP = 44, FP = 42) class_id = 46, name = nail_polish, ap = 84.06% (TP = 105, FP = 30) class_id = 47, name = office_supplies, ap = 7.01% (TP = 30, FP = 52) class_id = 48, name = paint, ap = 47.56% (TP = 54, FP = 25) class_id = 49, name = paper_cups, ap = 76.20% (TP = 85, FP = 32) class_id = 50, name = pet_waste, ap = 31.60% (TP = 5, FP = 1) class_id = 51, name = pizza_boxes, ap = 61.07% (TP = 69, FP = 38) class_id = 52, name = plastic_bags, ap = 31.45% (TP = 39, FP = 36) class_id = 53, name = plastic_bottles, ap = 77.49% (TP = 303, FP = 92) class_id = 54, name = plastic_caps, ap = 22.99% (TP = 5, FP = 1) class_id = 55, name = plastic_cards, ap = 57.51% (TP = 45, FP = 14) class_id = 56, name = plastic_clamshells, ap = 74.12% (TP = 36, FP = 16) class_id = 57, name = plastic_containers, ap = 71.93% (TP = 104, FP = 47) class_id = 58, name = power_tools, ap = 75.21% (TP = 77, FP = 58) class_id = 59, name = printers, ap = 88.06% (TP = 106, FP = 62) class_id = 60, name = propane_tanks, ap = 62.89% (TP = 36, FP = 15) class_id = 61, name = scrap_metal, ap = 0.00% (TP = 0, FP = 0) class_id = 62, name = shoes, ap = 83.53% (TP = 64, FP = 12) class_id = 63, name = small_appliances, ap = 80.61% (TP = 93, FP = 34) class_id = 64, name = smoke_detectors, ap = 79.73% (TP = 65, FP = 10) class_id = 65, name = sporting_goods, ap = 3.34% (TP = 0, FP = 0) class_id = 66, name = tires, ap = 83.31% (TP = 77, FP = 11) class_id = 67, name = tools, ap = 69.69% (TP = 113, FP = 37) class_id = 68, name = toothbrushes, ap = 2.92% (TP = 1, FP = 1) class_id = 69, name = toothpaste_tubes, ap = 59.78% (TP = 35, FP = 9) class_id = 70, name = toy, ap = 49.70% (TP = 47, FP = 24) class_id = 71, name = vehicles, ap = 4.51% (TP = 1, FP = 4) class_id = 72, name = water_filters, ap = 47.91% (TP = 36, FP = 21) class_id = 73, name = wood, ap = 76.15% (TP = 105, FP = 51) class_id = 74, name = wrapper, ap = 72.45% (TP = 67, FP = 18) for conf_thresh = 0.25, precision = 0.65, recall = 0.41, F1-score = 0.50 for conf_thresh = 0.25, TP = 4507, FP = 2430, FN = 6427, average IoU = 51.08 % IoU threshold = 50 %, used Area-Under-Curve for each unique Recall mean average precision (mAP@0.50) = 0.523220, or 52.32 % Total Detection Time: 564 Seconds Set -points flag: `-points 101` for MS COCO `-points 11` for PascalVOC 2007 (uncomment `difficult` in voc.data) `-points 0` (AUC) for ImageNet, PascalVOC 2010-2012, your custom dataset . . Deployment . In order to utilize our hard-earned trained weights for inference in the app, I built an object detection API with Flask and used Docker to deploy it to AWS Elastic Beanstalk. . The trained weights were loaded into a live network using OpenCV, which can then be used for inference (prediction; detecting objects). Once a user takes a photo in the app, it is encoded and sent to the detection API. The API decodes the image and passes it into the live network running with the trained weights. The network runs inference on the image and sends back the class of item with the highest predicted probability. . Again, I go into much greater detail on the API in a separate blog post (link to come, once it&#39;s published). . . Final Thoughts . Potential Improvements . Although we had eight weeks for this project, we were starting with a completely green field. Therefore, at least the first two weeks were dedicated to discussions and planning. . The result of scope management is features and ideas left unimplemented. At the beginning, the entire team brainstormed a lot about potential ideas for the app. And throughout the project, the machine learning team was always thinking and discussing potential improvements. Here is a brief summary of some of them. . The first and most obvious one is to simply expand the model by gathering and labeling an even wider variety and even greater numbers of images. By breaking out the grouped items into more specific classes and gathering more images, the model could be trained to distinguish between some of the visually similar (though materially different) objects. . Furthermore, there are some low-hanging fruit that would likely make the model more robust. One of those that we did not get to explore due to lack of time is image augmentation. In a nutshell, image augmentation is a method of artificially growing an image dataset by &quot;creating&quot; new images from the existing ones by transforming or otherwise manipulating them. . These augmentations could include changing the color, crop, skew, rotation, and even combining multiple images. The labels are also transformed to match the location and size of the transformed objects in each image. Not only does this process help create more training data, but it can also help the model extract more of the underlying features of the objects, allowing it to be more robust to a greater variety of lighting conditions and camera angles. . Another way to increase the size and variety of the dataset would be to add a feature that gathers the images (and labels) taken by the user. After every prediction that is served, the user could be presented with a box containing two buttons, one indicating that the prediction was correct, the other, incorrect. The correct ones could go straight to the server to wait until the next model training round. And if the user indicates that the predicted class was incorrect, they could be presented with an interface to draw a box around the object and choose the correct class. Then that image and label would be sent to the server to be used in training. . The last idea was one that the entire team discussed at one point, though is a feature that would likely only be possible with some kind of financial backing. The idea is to implement a system that recognizes when many of a similar type of object are detected—those need to be recycled at the same facility, for example—and can pay drivers to pick all of those items up and take them to the proper facilities. In other words, be the &quot;UBER for recycling&quot;. . Team Links . I said it before and I&#39;ll say it again: the team I worked with on this project was top-notch. Here they are in all of their glory, with all of their portfolios/blogs/GitHubs (those that have them) in case you want to check out their other work. . Machine learning . Tobias Reaper | Trevor Clack | Vera Mendes | Timothy Hsu | . Web development . Mark Halls | Mark Artishuk | Colin Bazzano | Carlo Lucido | . User Experience . Kendra McKernan | Lynn Baxter | . Other Links . I linked to this video above but figured I&#39;d include it here as well. It&#39;s the video that Trevor created to pitch the idea (originally called &#39;Recycle This?&#39;) to the Lambda Labs coordinators. Also included is another video he created to demo the final product. I was one of the presenters of our app for the final demos to both our cohort and to the entire school. Links to those videos are below. Here is a video (thanks to Trevor) showing the progress of the interface at various stages throughout the project. . As always, thanks for reading, and I&#39;ll see you in the next one! .",
            "url": "https://tobias-fyi.github.io/fastfyi/workshop/trash-panda",
            "relUrl": "/workshop/trash-panda",
            "date": " • Mar 8, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://tobias-fyi.github.io/fastfyi/jupyter/test",
            "relUrl": "/jupyter/test",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://tobias-fyi.github.io/fastfyi/markdown/test-markdown-post",
            "relUrl": "/markdown/test-markdown-post",
            "date": " • Jan 14, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Post Here",
            "content": ". Introduction . The Problem . Reddit is an expansive site. Anyone who has spent any significant amount of time on it knows what I mean. There is a subreddit for seemingly every topic anyone could ever want to discuss or even think about (and many that most do not want think about). . Reddit is a powerful site; a tool for connecting and sharing information with like- or unlike-minded individuals around the world. When used well, it can be a very useful resource. . On the other hand, the deluge of information that&#39;s constantly piling into the pages of can be overwhelming and lead to wasted time. As with any tool, it can be used for good or for not-so-good. . A common problem that Redditors experience, particularly those who are relatively new to the site, is where to post content. Given that there are subreddits for just about everything, with wildly varying degrees of specificity it can be quite overwhelming trying to find the best place for each post. . Just to illustrate the point, some subreddits get weirdly specific. I won&#39;t go into the really weird or NSFW, but here are some good examples of what I mean by specific: . r/Borderporn | r/BreadStapledtoTrees | r/birdswitharms | r/totallynotrobots | . ...need I go on? (If you&#39;re curious and/or want to be entertained indefinitely, here is a thread with these and much, much more.) . Most of the time when a post is deemed irrelevant to a particular subreddit, it will simply be removed by moderators or a bot. However, depending on the subreddit and how welcoming they are to newbies, sometimes it can lead to very unfriendly responses and/or bans. . So how does one go about deciding where to post or pose a question? . Post Here aims to take the guesswork out of this process. . . The Solution . The goal with the Post Here app, as mentioned, is to provide a tool that makes it quick and easy to find the most appropriate subreddits for any given post. A user would simply provide the title and text of the their prospective post and the app would provide the user with a list of subreddit recommendations. . Recommendations are produced by a model attempts to predict which subreddit a given post would belong to. The model was built using Scikit-learn, and was trained on a large dataset of reddit posts. In order to serve the recommendations to the web app, an API was built using Flask and deployed to Heroku. . The live version of the app is linked below. . Post Here:The Subreddit Suggester . My Role . I worked on the Post Here app with a remote, interdisciplinary team of data scientists, machine learning engineers, and web developers. I was one of two machine learning engineers on the team, responsible for the entire process of building and training the machine learning models. The two data scientists on the team were primarily responsible for building and deploying the API. . The main challenge we ran into, which directed much of the iterative process, was scope management. . At this point in my machine learning journey, this was one of the larger datasets that I&#39;d taken on. Uncompressed, the dataset we used was over 800mb of mostly natural language text. The dataset and the time constraint—we had less than four full days of work to finish the project—were the primary causes of the challenges we ended up facing. . With such a dataset, one important concept we had to keep in mind was the curse of dimensionality, which is basically a title for the various problems and phenomena that occur when dealing with extremely highly dimensional datasets. When processed, a natural language dataset of this size would likely fall prey to this curse and may prove somewhat unwieldy without large amounts of processing power. . I ended up researching and applying various methods of addressing this problem in order to fit the processing/modeling pipeline on the free Heroku Dyno, with a memory limit of 500mb, while preserving adequate performance. Many of our deployments failed because the pipeline, when loaded into memory on the server, exceeded that limit. . One important tradeoff we had to wrangle with was how much, and in what ways we could limit the dataset—i.e. how many classes to try and predict, and how many observations per class to include when training. The original dataset contains data for 1,000 subreddits. It was not within the scope of a a four-day project to build a classification model of a caliber that could accurately classify 1,000 classes. . In the beginning, we did try to build a basic model trained on all 1,000 classes. But with the time and processing power I had, it proved to be untenable. In the end, we settled for a model that classified text into 305 subreddits with a test precision-at-k of .75, .88, and .92 for &#39;k&#39; of 1, 3, and 5, respectively. . Imports and Configuration . import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns import os import janitor . from sklearn.model_selection import train_test_split from sklearn.preprocessing import LabelEncoder from sklearn.ensemble import RandomForestClassifier from sklearn.naive_bayes import MultinomialNB from sklearn.feature_selection import chi2, SelectKBest # === NLP Imports === # from sklearn.feature_extraction.text import TfidfVectorizer . # Configure pandas display settings pd.options.display.max_colwidth = 100 # Set random seed seed = 92 . . The Data . The dataset we ended up using to train the recommendation system is called the Reddit Self-Post Classification Task dataset, available on Kaggle thanks to Evolution AI. The full dataset clocks in at over 800mb, containing 1,013,000 rows: 1,000 posts each from 1,013 subreddits. . For more details on the dataset, including a nice interactive plot of all of the subreddits, refer to Evolution AI&#39;s blog post. . Wrangling and Exploration . First, I needed to reduce the size of the dataset. I defined a subset of 12 categories which I thought were most relevant to the task at hand, and used that list to do the initial pruning. Those 12 categories left me with 305 unique subreddits and 305,000 rows. The list I used was as follows: . health | profession | electronics | hobby | writing/stories | advice/question | social_group | stem | parenting | books | finance/money | travel | . Next, I took a random sample of those 305,000 rows. The result was a dataset with 91,500 rows, now consisting of between 250 and 340 rows per subreddit. If I tried to use all of the features (tokens, or words) that resulted from this corpus, even in its reduced state, it would still result in a serialized vocabulary and/or model too large for our free Heroku Dyno. However, the features used in the final model can be chosen based on how useful they are for the classification. . According to the dataset preview on Kaggle, there are quite a large number of missing values in each of the features—12%, 25%, and 39% of the subreddit, title, and selftext columns, respectively. However, I did not find any sign of those null values in the dataset nor mention of them in the dataset&#39;s companion blog post or article. I chocked it up to an error in the Kaggle preview. . Finally, I went about doing some basic preprocessing to get the data ready for vectorization. As described in the description page on Kaggle, newline and tab characters were replaced with their HTML equivalents, &lt;lb&gt; and &lt;tab&gt;. I removed those and other HTML entities using a simple regular expression. I also concatenated title and selftext into a single text feature in order to process them together. . rspct = pd.read_csv(&quot;assets/data/rspct.tsv&quot;, sep=&quot; t&quot;) print(rspct.shape) rspct.head(3) . (1013000, 4) . id subreddit title selftext . 0 6d8knd | talesfromtechsupport | Remember your command line switches... | Hi there, &lt;lb&gt;The usual. Long time lerker, first time poster, be kind etc. Sorry if this isn&#39;t the right place...&lt;lb&gt;&lt;lb&gt;Alright. Here&#39;s the story. I&#39;m an independent developer who produces my ow... | . 1 58mbft | teenmom | So what was Matt &quot;addicted&quot; to? | Did he ever say what his addiction was or is he still chugging beers while talking about how sober he is?&lt;lb&gt;&lt;lb&gt;Edited to add: As an addict myself, anyone I know whose been an addict doesn&#39;t drin... | . 2 8f73s7 | Harley | No Club Colors | Funny story. I went to college in Las Vegas. This was before I knew anything about motorcycling whatsoever. Me and some college buddies would always go out on the strip to the dance clubs. We alwa... | . Nulls . Kaggle says that 12%, 25%, and 39% of the subreddit, title, and selftext columns are null, respectively. If that is indeed the case, they did not get read into the dataframe correctly. However, it could be an error on Kaggle&#39;s part, seeing as there is no mention of these anywhere else in the description or blog post or article, nor sign of them during my explorations. . rspct.isnull().sum() . id 0 subreddit 0 title 0 selftext 0 dtype: int64 . Preprocessing . To prune the list of subreddits, I&#39;ll load in the subreddit_info.csv file, join, then choose a certain number of categories (category_1) to filter on. . info = pd.read_csv(&quot;assets/data/subreddit_info.csv&quot;, usecols=[&quot;subreddit&quot;, &quot;category_1&quot;, &quot;category_2&quot;]) print(info.shape) info.head() . (3394, 3) . subreddit category_1 category_2 . 0 whatsthatbook | advice/question | book | . 1 CasualConversation | advice/question | broad | . 2 Clairvoyantreadings | advice/question | broad | . 3 DecidingToBeBetter | advice/question | broad | . 4 HelpMeFind | advice/question | broad | . rspct = pd.merge(rspct, info, on=&quot;subreddit&quot;).drop(columns=[&quot;id&quot;]) print(rspct.shape) rspct.head() . (1013000, 5) . subreddit title selftext category_1 category_2 . 0 talesfromtechsupport | Remember your command line switches... | Hi there, &lt;lb&gt;The usual. Long time lerker, first time poster, be kind etc. Sorry if this isn&#39;t the right place...&lt;lb&gt;&lt;lb&gt;Alright. Here&#39;s the story. I&#39;m an independent developer who produces my ow... | writing/stories | tech support | . 1 talesfromtechsupport | I work IT for a certain clothing company and they use iPod Touchs for scanning some items | [ME]- Thank you fro calling Store support, this is David. How may I help you?&lt;lb&gt;&lt;lb&gt;[Store]- Yeah, my iPod is frozen&lt;lb&gt;&lt;lb&gt;[ME]- Okay, can I have you hold down the power and the home button at t... | writing/stories | tech support | . 2 talesfromtechsupport | It... It says right there on the screen...? | Hi guys! &lt;lb&gt;&lt;lb&gt;&amp;amp;nbsp;&lt;lb&gt;&lt;lb&gt;LTL, FTP - all that jazz. Starting you off with a short one.&lt;lb&gt;&lt;lb&gt;&amp;amp;nbsp;&lt;lb&gt;&lt;lb&gt;I&#39;m the senior supporter at a smaller tech company with clients all over t... | writing/stories | tech support | . 3 talesfromtechsupport | The computers not working. FIX IT NOW! | Hey there TFTS! This is my second time posting. I don&#39;t work for any tech support company, but I do have friends, family and teachers at school that have no idea how stuff works.&lt;lb&gt;&lt;lb&gt;This tale ... | writing/stories | tech support | . 4 talesfromtechsupport | A Storm of Unreasonableness | Usual LTL, FTP. I have shared this story on a different site, but after reading TFTS for sometime I figured it&#39;d belong here as well. &lt;lb&gt;&lt;lb&gt;This is from when I worked at a 3rd party call center ... | writing/stories | tech support | . rspct.isnull().sum() # That&#39;s a good sign . subreddit 0 title 0 selftext 0 category_1 0 category_2 0 dtype: int64 . rspct[&quot;category_1&quot;].value_counts() . video_game 100000 tv_show 68000 health 58000 profession 56000 software 52000 electronics 51000 music 43000 sports 40000 sex/relationships 31000 hobby 30000 geo 29000 crypto 29000 company/website 28000 other 27000 anime/manga 26000 drugs 23000 writing/stories 22000 programming 21000 arts 21000 autos 20000 advice/question 18000 education 17000 animals 17000 politics/viewpoint 16000 social_group 16000 card_game 15000 food/drink 15000 stem 14000 hardware/tools 14000 parenting 13000 religion/supernatural 13000 books 12000 appearance 11000 finance/money 10000 board_game 9000 meta 9000 movies 7000 rpg 7000 travel 5000 Name: category_1, dtype: int64 . keep_cats = [ &quot;health&quot;, &quot;profession&quot;, &quot;electronics&quot;, &quot;hobby&quot;, &quot;writing/stories&quot;, &quot;advice/question&quot;, &quot;social_group&quot;, &quot;stem&quot;, &quot;parenting&quot;, &quot;books&quot;, &quot;finance/money&quot;, &quot;travel&quot;, ] # Prune dataset to above categories # Overwriting to save memory rspct = rspct[rspct[&quot;category_1&quot;].isin(keep_cats)] print(rspct.shape) print(&quot;Unique subreddits:&quot;, len(rspct[&quot;subreddit&quot;].unique())) rspct.head(2) . (305000, 5) Unique subreddits: 305 . subreddit title selftext category_1 category_2 . 0 talesfromtechsupport | Remember your command line switches... | Hi there, &lt;lb&gt;The usual. Long time lerker, first time poster, be kind etc. Sorry if this isn&#39;t the right place...&lt;lb&gt;&lt;lb&gt;Alright. Here&#39;s the story. I&#39;m an independent developer who produces my ow... | writing/stories | tech support | . 1 talesfromtechsupport | I work IT for a certain clothing company and they use iPod Touchs for scanning some items | [ME]- Thank you fro calling Store support, this is David. How may I help you?&lt;lb&gt;&lt;lb&gt;[Store]- Yeah, my iPod is frozen&lt;lb&gt;&lt;lb&gt;[ME]- Okay, can I have you hold down the power and the home button at t... | writing/stories | tech support | . rspct = rspct.sample(frac=.3, random_state=seed) print(rspct.shape) rspct.head() . (91500, 5) . subreddit title selftext category_1 category_2 . 594781 stepparents | Ex Wants Toddler Son (2M) to Meet Her AP/SO - x-post from /r/divorce | Quick background: My soon-to-be ex-wife (26F) and I (27M) have been separated for about 5 months now. She has been in a serious relationship her AP (23M) whom she met and cheated on me with 6 mont... | parenting | step parenting | . 617757 bigseo | Do we raise our pricing? | I took a management role at an agency. We&#39;re way, way under $500/mo for SEO pricing - and I&#39;m embarrassed to say that we&#39;re hurting for business. Seems to me that it&#39;s a struggle to get clients to... | profession | seo | . 642368 chemistry | Mac vs. PC? | Hello, all! I am currently a senior in high school and in the fall I will be going to SUNY Geneseo, majoring in chemistry and minoring in mathematics. &lt;lb&gt;&lt;lb&gt;Geneseo requires it’s students to get... | stem | chemistry | . 325221 migraine | Beer as an aural abortive? | Hiya folks,&lt;lb&gt;&lt;lb&gt;I&#39;ve been a migraine sufferer pretty much my whole life. For me intense auras, numbness, confusion, the inability to speak or see is BY FAR the worst aspect of the ordeal. When ... | health | migraine | . 524939 MouseReview | Recommend office mouse | I was hoping you folks could help me out. Here&#39;s my situation and requirements:&lt;lb&gt;&lt;lb&gt;* I don&#39;t play games at all&lt;lb&gt;* Budget $30.00 or less&lt;lb&gt;* Shape as close to old Microsoft Intellimouse Opti... | electronics | computer mouse | . # Concatenate title and selftext rspct[&quot;text&quot;] = rspct[&quot;title&quot;] + &quot; &quot; + rspct[&quot;selftext&quot;] # Drop categories rspct = rspct.drop(columns=[&quot;category_1&quot;, &quot;category_2&quot;, &quot;title&quot;, &quot;selftext&quot;]) . # NOTE: takes a couple minutes to run rspct[&quot;text&quot;] = rspct[&quot;text&quot;].str.replace(&quot;(&lt;lb&gt;)*|(&lt;tab&gt;)*|(&amp;amp;)*|(nbsp;)*&quot;, &quot;&quot;) rspct.head() . subreddit text . 594781 stepparents | Ex Wants Toddler Son (2M) to Meet Her AP/SO - x-post from /r/divorce Quick background: My soon-to-be ex-wife (26F) and I (27M) have been separated for about 5 months now. She has been in a serious... | . 617757 bigseo | Do we raise our pricing? I took a management role at an agency. We&#39;re way, way under $500/mo for SEO pricing - and I&#39;m embarrassed to say that we&#39;re hurting for business. Seems to me that it&#39;s a s... | . 642368 chemistry | Mac vs. PC? Hello, all! I am currently a senior in high school and in the fall I will be going to SUNY Geneseo, majoring in chemistry and minoring in mathematics. Geneseo requires it’s students to... | . 325221 migraine | Beer as an aural abortive? Hiya folks,I&#39;ve been a migraine sufferer pretty much my whole life. For me intense auras, numbness, confusion, the inability to speak or see is BY FAR the worst aspect o... | . 524939 MouseReview | Recommend office mouse I was hoping you folks could help me out. Here&#39;s my situation and requirements:* I don&#39;t play games at all* Budget $30.00 or less* Shape as close to old Microsoft Intellimou... | . subreddits = rspct[&quot;subreddit&quot;].unique() print(len(subreddits)) subreddits[:50] . 305 . array([&#39;stepparents&#39;, &#39;bigseo&#39;, &#39;chemistry&#39;, &#39;migraine&#39;, &#39;MouseReview&#39;, &#39;Malazan&#39;, &#39;Standup&#39;, &#39;preppers&#39;, &#39;Invisalign&#39;, &#39;whatsthisplant&#39;, &#39;CrohnsDisease&#39;, &#39;KingkillerChronicle&#39;, &#39;OccupationalTherapy&#39;, &#39;churning&#39;, &#39;Libraries&#39;, &#39;acting&#39;, &#39;eczema&#39;, &#39;Allergies&#39;, &#39;bigboobproblems&#39;, &#39;AskAnthropology&#39;, &#39;psychotherapy&#39;, &#39;WayfarersPub&#39;, &#39;synthesizers&#39;, &#39;StopGaming&#39;, &#39;stopsmoking&#39;, &#39;eroticauthors&#39;, &#39;amazonecho&#39;, &#39;TalesFromThePizzaGuy&#39;, &#39;rheumatoid&#39;, &#39;homestead&#39;, &#39;VoiceActing&#39;, &#39;FinancialCareers&#39;, &#39;Sleepparalysis&#39;, &#39;ProtectAndServe&#39;, &#39;short&#39;, &#39;Fibromyalgia&#39;, &#39;teaching&#39;, &#39;PlasticSurgery&#39;, &#39;insomnia&#39;, &#39;PLC&#39;, &#39;rapecounseling&#39;, &#39;peacecorps&#39;, &#39;paintball&#39;, &#39;autism&#39;, &#39;Nanny&#39;, &#39;Plumbing&#39;, &#39;Epilepsy&#39;, &#39;asmr&#39;, &#39;fatpeoplestories&#39;, &#39;Magic&#39;], dtype=object) . rspct[&quot;subreddit&quot;].value_counts() . Dreams 340 Gifts 337 HFY 333 Cubers 333 cassetteculture 333 ... foreignservice 265 WritingPrompts 263 immigration 263 TryingForABaby 262 Physics 250 Name: subreddit, Length: 305, dtype: int64 . . Modeling . Label Encoding . # This process naively transforms each class of the target into a number le = LabelEncoder() # Instantiate a new encoder instance le.fit(y_train) # Fit it on training label data # Transform both using the trained instance y_train = le.transform(y_train) y_val = le.transform(y_val) y_test = le.transform(y_test) y_train[:8] . array([ 92, 140, 65, 90, 278, 65, 272, 212]) . Vectorization . A vectorizer is used to extract numerical features (information) from a corpus of natural language text. I used a bag-of-words method of vectorization, which for the most part, disregards grammar. . The output of this vectorizer is a document-term matrix, with the documents (observations, or rows) on one axes and the terms (words, bigrams) on the other. This matrix can be thought of as a sort of vocabulary, or text-number translator. . At first, the &quot;vocabulary&quot; derived from the corpus using the vectorizer was the largest object when serialized. Luckily, there are many options and parameters available to reduce its size, most of which are simply different methods for reducing the number of features (terms) it contains. . One option is to put a hard limit of 100,000 on the number of features in the vocabulary. This is a simple, naive limit on the generated features, and thus, the resulting vocabulary size. . I decided to remove stopwords before vectorization in hopes that this would reduce the size of the vector vocabulary. To my initial surprise, removing the stop words (using NLTK&#39;s list) actually increased the size of the serialized vocab from 59mb to 76mb. . After some consideration, I found this to be a reasonable result. I figured that many of the stop words are short (&quot;I&quot;, &quot;me&quot;, &quot;my&quot;, etc.), and their removal caused the average length of words (and therefore bigrams as well) in the vocab to increase. While this may not account for the entirety of the difference, this provides some intuition as to why there is a difference. . Although the vocab without stop words was larger, I ended up using it anyways because it provided an extra ~0.01 in the precision-at-k score of the final model. . lengths = [] three_or_below = [] for word in stop_words: lengths.append(len(word)) if len(word) &lt;= 4: three_or_below.append(len(word)) print(f&quot;There are {len(stop_words)} stop words in the list.&quot;) print(f&quot;{len(three_or_below)} are 4 chars long or shorter.&quot;) print(f&quot;Average length is: {np.mean(lengths)}.&quot;) . There are 179 stop words in the list. 109 are 4 chars long or shorter. Average length is: 4.229050279329609. . tfidf = TfidfVectorizer( max_features=100000, min_df=10, ngram_range=(1,2), stop_words=stop_words, # Use nltk&#39;s stop words ) # Fit the vectorizer on the feature column to create vocab (doc-term matrix) vocab = tfidf.fit(X_train) # Get sparse document-term matrices X_train_sparse = vocab.transform(X_train) X_val_sparse = vocab.transform(X_val) X_test_sparse = vocab.transform(X_test) X_train_sparse.shape, X_val_sparse.shape, X_test_sparse.shape . ((65880, 63588), (7320, 63588), (18300, 63588)) . Feature Selection . As mentioned previously, the size of the corpus means the dimensionality of the featureset after vectorization will be very high. I passed in 100,000 as the maximum number of features to the vectorizer, limiting the initial size of the vocab. However, the features would have to be reduced more before training the model, as it is generally not good practice to have a larger number of features (100,000) than observations (91,500). . To reduce it down from that 100,000, I used a process called select k best, which does exactly what it sounds like: selects a certain number of the best features. The key aspect of this process is how to measure the value of the features; how to find which ones are the &quot;best&quot;. The scoring function I used in this case is called ch2 (chi-squared). . This function calculates chi-squared statistics between each feature and the target, measuring the dependence, or correlation, between them. The intuition here is that features which are more correlated with the target are more likely to be useful to the model. . I played around with some different values for the maximum number of features to be selected. Ultimately, I was once again limited by the size of the free Heroku Dyno, and settled on 20,000. This allowed the deployment to go smoothly while retaining enough information for the model to have adequate performance. . selector = SelectKBest(chi2, k=20000) selector.fit(X_train_sparse, y_train) X_train_select = selector.transform(X_train_sparse) X_val_select = selector.transform(X_val_sparse) X_test_select = selector.transform(X_test_sparse) X_train_select.shape, X_val_select.shape, X_test_select.shape . ((65880, 20000), (7320, 20000), (18300, 20000)) . Model validation . In this case, the model has a target that it is attempting to predict—a supervised problem. Therefore, the performance can be measured on validation and test sets. . To test out the recommendations I copied some posts and put them through the prediction pipeline to see what kinds of subreddits were getting recommended. For the most part, the predictions were decent. . The cases where the recommendations were a little less than ideal happened when I pulled example posts from subreddits that were not in the training data. The model generally did a good job recommending similar subreddits. . Baseline . For the baseline model, I decided to go with a basic random forest. This choice was somewhat arbitrary, though I was curious to see how a random forest would do with such a high target cardinality (number of classes/categories). . The baseline precision-at-k metrics for the random forest on the validation set were .54, .63, and .65, for k of 1, 3, and 5, respectively. . def precision_at_k(y_true, y_pred, k=5): y_true = np.array(y_true) y_pred = np.array(y_pred) y_pred = np.argsort(y_pred, axis=1) y_pred = y_pred[:, ::-1][:, :k] arr = [y in s for y, s in zip(y_true, y_pred)] return np.mean(arr) . rfc = RandomForestClassifier(max_depth=32, n_jobs=-1, n_estimators=200) rfc.fit(X_train_select, y_train) . RandomForestClassifier(max_depth=32, n_estimators=200, n_jobs=-1) . y_pred_proba_rfc = rfc.predict_proba(X_val_select) # For each prediction, find the index with the highest probability y_pred_rfc = np.argmax(y_pred_proba_rfc, axis=1) y_pred_rfc[:10] . array([296, 139, 177, 78, 12, 177, 161, 216, 40, 31]) . print(&quot;Validation scores:&quot;) print(&quot; precision@1 =&quot;, np.mean(y_val == y_pred_rfc)) print(&quot; precision@3 =&quot;, precision_at_k(y_val, y_pred_proba_rfc, 3)) print(&quot; precision@5 =&quot;, precision_at_k(y_val, y_pred_proba_rfc, 5)) . Validation scores: precision@1 = 0.5368852459016393 precision@3 = 0.6282786885245901 precision@5 = 0.6502732240437158 . Multinomial Naive Bayes . Multinomial naive Bayes is a probabilistic learning method for multinomially distributed data, and one of two classic naive Bayes algorithms used for text classification. I decided to iterate with this algorithm because it is meant for text classification tasks. . The precision-at-k metrics for the final Multinomial naive Bayes model on the validation set were .76, .88, and .9188, for k of 1, 3, and 5, respectively. Performance on the test set was nearly identical: .75, .88, and .9159. . nb = MultinomialNB(alpha=0.1) nb.fit(X_train_select, y_train) . MultinomialNB(alpha=0.1) . Evaluate on validation set . y_pred_proba_val = nb.predict_proba(X_val_select) # For each prediction, find index with highest probability y_pred_val = np.argmax(y_pred_proba_val, axis=1) y_pred_val[:10] . array([274, 139, 57, 78, 12, 17, 151, 216, 40, 171]) . print(&quot;Validation scores:&quot;) print(&quot; precision@1 =&quot;, np.mean(y_val == y_pred_val)) print(&quot; precision@3 =&quot;, precision_at_k(y_val, y_pred_proba_val, 3)) print(&quot; precision@5 =&quot;, precision_at_k(y_val, y_pred_proba_val, 5)) . Validation scores: precision@1 = 0.7599726775956284 precision@3 = 0.8834699453551913 precision@5 = 0.9188524590163935 . Evaluate on test set . y_pred_proba_test = nb.predict_proba(X_test_select) # For each prediction, find index with highest probability y_pred_test = np.argmax(y_pred_proba_test, axis=1) y_pred_test[:10] . array([ 97, 199, 116, 249, 43, 203, 263, 275, 96, 27]) . print(&quot;Test scores:&quot;) print(&quot; precision@1 =&quot;, np.mean(y_test == y_pred_test)) print(&quot; precision@3 =&quot;, precision_at_k(y_test, y_pred_proba_test, 3)) print(&quot; precision@5 =&quot;, precision_at_k(y_test, y_pred_proba_test, 5)) . Test scores: precision@1 = 0.7498360655737705 precision@3 = 0.8834426229508197 precision@5 = 0.9159562841530055 . Recommendations . The API should return a list of recommendations, not a single prediction. To accomplish this, I wrote a function that returns the top 5 most likely subreddits and their respective probabilities. . # The main functionality of the predict API endpoint def predict(title: str, submission_text: str, return_count: int = 5): &quot;&quot;&quot;Serve subreddit predictions. Parameters - title : string Title of post. submission_text : string Selftext that needs a home. return_count : integer The desired number of recommendations. Returns - Python dictionary formatted as follows: [{&#39;subreddit&#39;: &#39;PLC&#39;, &#39;proba&#39;: 0.014454}, ... {&#39;subreddit&#39;: &#39;Rowing&#39;, &#39;proba&#39;: 0.005206}] &quot;&quot;&quot; # Concatenate title and post text fulltext = str(title) + str(submission_text) # Vectorize the post -&gt; sparse doc-term matrix post_sparse = vocab.transform([fulltext]) # Feature selection post_select = selector.transform(post_sparse) # Generate predicted probabilities from trained model proba = nb.predict_proba(post_select) # Wrangle into correct format proba_dict = (pd .DataFrame(proba, columns=[le.classes_]) # Classes as column names .T # Transpose so column names become index .reset_index() # Pull out index into a column .rename(columns={&quot;level_0&quot;: &quot;name&quot;, 0: &quot;proba&quot;}) # Rename for aesthetics .sort_values(by=&quot;proba&quot;, ascending=False) # Sort by probability .iloc[:return_count] # n-top predictions to serve .to_dict(orient=&quot;records&quot;) ) proba_json = {&quot;predictions&quot;: proba_dict} return proba_json . title_science = &quot;&quot;&quot;Is there an evolutionary benefit to eating spicy food that lead to consumption across numerous cultures throughout history? Or do humans just like the sensation?&quot;&quot;&quot; post_science = &quot;&quot;&quot;I love spicy food and have done ever since I tried it. By spicy I mean HOT, like chilli peppers (we say spicy in England, I don&#39;t mean to state the obvious I&#39;m just not sure if that&#39;s a global term and I&#39;ve assumed too much before). I love a vast array of spicy foods from all around the world. I was just wondering if there was some evolutionary basis as to why spicy food managed to become some widely consumed historically. Though there seem to It way well be that we just like a tingly mouth, the simple things in life.&quot;&quot;&quot; science_recs = predict(title_science, post_science) science_recs . {&#39;predictions&#39;: [{&#39;name&#39;: &#39;GERD&#39;, &#39;proba&#39;: 0.009900622287634142}, {&#39;name&#39;: &#39;Allergies&#39;, &#39;proba&#39;: 0.009287774623361566}, {&#39;name&#39;: &#39;ibs&#39;, &#39;proba&#39;: 0.009150308633162811}, {&#39;name&#39;: &#39;AskAnthropology&#39;, &#39;proba&#39;: 0.009028660140513678}, {&#39;name&#39;: &#39;fatpeoplestories&#39;, &#39;proba&#39;: 0.00851982441049019}]} . title_pc = &quot;&quot;&quot;Looking for help with a build&quot;&quot;&quot; post_pc = &quot;&quot;&quot;I posted my wants for my build about 2 months ago. Ordered them and when I went to build it I was soooooo lost. It took 3 days to put things together because I was afraid I would break something when I finally got the parts together it wouldn’t start, I was so defeated. With virtually replacing everything yesterday it finally booted and I couldn’t be more excited!&quot;&quot;&quot; post_pc_recs = predict(title_pc, post_pc, 10) post_pc_recs . {&#39;predictions&#39;: [{&#39;name&#39;: &#39;lego&#39;, &#39;proba&#39;: 0.008418484170536294}, {&#39;name&#39;: &#39;rccars&#39;, &#39;proba&#39;: 0.008112076951648648}, {&#39;name&#39;: &#39;MechanicalKeyboards&#39;, &#39;proba&#39;: 0.0078335440606017}, {&#39;name&#39;: &#39;fightsticks&#39;, &#39;proba&#39;: 0.007633958584830632}, {&#39;name&#39;: &#39;Luthier&#39;, &#39;proba&#39;: 0.00716176615193545}, {&#39;name&#39;: &#39;modeltrains&#39;, &#39;proba&#39;: 0.007088134228361153}, {&#39;name&#39;: &#39;cade&#39;, &#39;proba&#39;: 0.007058109839673285}, {&#39;name&#39;: &#39;vandwellers&#39;, &#39;proba&#39;: 0.006700262151491209}, {&#39;name&#39;: &#39;cosplay&#39;, &#39;proba&#39;: 0.006536648725434882}, {&#39;name&#39;: &#39;homestead&#39;, &#39;proba&#39;: 0.006166832450007183}]} . post_title = &quot;&quot;&quot;What to do about java vs javascript&quot;&quot;&quot; post = &quot;&quot;&quot;I am a new grad looking for a job and currently in the process with a company for a junior backend engineer role. I was under the impression that the position was Javascript but instead it is actually Java. My general programming and &quot;leet code&quot; skills are pretty good, but my understanding of Java is pretty shallow. How can I use the next three days to best improve my general Java knowledge? Most resources on the web seem to be targeting complete beginners. Maybe a book I can skim through in the next few days? Edit: A lot of people are saying &quot;the company is a sinking ship don&#39;t even go to the interview&quot;. I just want to add that the position was always for a &quot;junior backend engineer&quot;. This company uses multiple languages and the recruiter just told me the incorrect language for the specific team I&#39;m interviewing for. I&#39;m sure they&#39;re mainly interested in seeing my understanding of good backend principles and software design, it&#39;s not a senior lead Java position.&quot;&quot;&quot; # === Test out the function === # post_pred = predict(post_title, post) # Default is 5 results post_pred . {&#39;predictions&#39;: [{&#39;name&#39;: &#39;cscareerquestions&#39;, &#39;proba&#39;: 0.516989539243874}, {&#39;name&#39;: &#39;devops&#39;, &#39;proba&#39;: 0.031462691062989795}, {&#39;name&#39;: &#39;interviews&#39;, &#39;proba&#39;: 0.02846504725703069}, {&#39;name&#39;: &#39;datascience&#39;, &#39;proba&#39;: 0.024227300545057697}, {&#39;name&#39;: &#39;bioinformatics&#39;, &#39;proba&#39;: 0.017516176338177075}]} . title_book = &quot;Looking for books with great plot twists&quot; # This one comes from r/suggestmeabook post2 = &quot;&quot;&quot;I&#39;ve been dreaming about writing my own stort story for a while but I want to give it an unexpected ending. I&#39;ve read lots of books, but none of them had the plot twist I want. I want to read books with the best plot twists, so that I can analyze what makes a good plot twist and write my own story based on that points. I don&#39;t like romance novels and I mostly enjoy sci-fi or historical books but anything beside romance novels would work for me, it doesn&#39;t have to be my type of novel. I&#39;m open to experience after all. I need your help guys. Thanks in advance.&quot;&quot;&quot; # === This time with 10 results === # post2_pred = predict(title_book, post2, 10) post2_pred . {&#39;predictions&#39;: [{&#39;name&#39;: &#39;suggestmeabook&#39;, &#39;proba&#39;: 0.4070015062748489}, {&#39;name&#39;: &#39;writing&#39;, &#39;proba&#39;: 0.14985778378113648}, {&#39;name&#39;: &#39;eroticauthors&#39;, &#39;proba&#39;: 0.07159411817054702}, {&#39;name&#39;: &#39;whatsthatbook&#39;, &#39;proba&#39;: 0.06062653422250441}, {&#39;name&#39;: &#39;ComicBookCollabs&#39;, &#39;proba&#39;: 0.027277418056905547}, {&#39;name&#39;: &#39;Malazan&#39;, &#39;proba&#39;: 0.019514923212723943}, {&#39;name&#39;: &#39;TheDarkTower&#39;, &#39;proba&#39;: 0.017162701613834493}, {&#39;name&#39;: &#39;DestructiveReaders&#39;, &#39;proba&#39;: 0.0151031907793204}, {&#39;name&#39;: &#39;WoT&#39;, &#39;proba&#39;: 0.011165890302931272}, {&#39;name&#39;: &#39;readyplayerone&#39;, &#39;proba&#39;: 0.007566597361383115}]} . Model deployment . As mentioned, the model, vocab, and feature selector were all serialized using Python&#39;s pickle module. In the Flask app, the pickled objects are loaded and ready for use, just like that. . I will go over the details of how the Flask app was set up in a separate blog post. . . Final Thoughts . For me, the most important and valuable aspects of this project were mainly surrounding the challenge of scope management. I constantly had to ask myself, &quot;What is the best version of this I can create given our limitations?&quot; . At first, I thought it would be feasible to predict all of the 1,000+ subreddits in the data, and wasted hours of valuable time attempting to do so. While I had tested various strategies of reducing the complexity of the model, the performance was rather terrible when it was trained on 100 or less examples of each of the complete list of subreddits. . The data scientist who I primarily worked with (we had one data scientist in addition to him and one other machine learning engineer, both of whom did not contribute significantly to the project) kept telling me that I should try reducing the number of classes first, allowing for more examples of each class and fewer classes for the model to predict. . Ultimately, this is the strategy that worked best, and I wasted valuable time by not listening to him the first few times he recommended that strategy. Good teamwork requires the members being humble and listening, something that I have taken to heart since the conclusion of this project. . Scope Management, Revisited . As time was very short while building this initial recommendation API, there are many things that we wished we could have done but simply did not have the time. Here are a few of the more obvious improvements that could be made. . The first, and most obvious one, is to simply deploy to a more powerful server, such as one hosted on AWS Elastic Beanstalk or EC2. This way, we could use the entire dataset to train an optimal model without worrying (as much) about memory limits. . Second, I could use a Scikit-learn pipeline to validate and tune hyperparameters using cross-validation, instead of a separate validation set. Also, this pipeline could be serialized as a single large object, rather than as separate pieces (encoder, vectorizer, feature selector, and classifier). As a final note for this particular train of thought, Joblib could potentially provide more efficient serialization than the Pickle module, allowing a more complex pipeline to be deployed on the same server. . Third, a model could&#39;ve been trained to classify the input post first into a broad category. Then, some sort of model could be used to to classify into a specific subreddit within that broad category. I&#39;m not sure about the feasibility of the second part of this idea, but thought it could be an interesting one to explore. . Lastly, different classes and calibers of models could have been tested for use in the various steps in the pipeline. In this case, I&#39;m referring primarily to using deep learning/neural networks. For example, word vectors could be generated with word embedding models such as Word2Vec. Or the process could be recreated with a library like PyTorch, and a framework like FastText. . I plan to explore at least some of these in separate blog posts. . As always, thank you for reading! I&#39;ll see you in the next one. .",
            "url": "https://tobias-fyi.github.io/fastfyi/workshop/post-here",
            "relUrl": "/workshop/post-here",
            "date": " • Jan 12, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://tobias-fyi.github.io/fastfyi/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://tobias-fyi.github.io/fastfyi/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}