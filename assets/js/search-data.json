{
  
    
        "post0": {
            "title": "Savor Data",
            "content": ". Introduction . Take advantage of your own big data. . Savor is a project based on an idea that I first had in 2016. At the time I was working as a consultant for an enterprise resource planning (ERP) software company. I worked intimately with manufacturers to integrate our system into their business, with the goal of optimizing their manufacturing processes. I became fascinated by the idea of tracking and planning various aspects of my life in a similar manner. I began to imagine what it would be like to have a similar type of system that I could use to optimize my life. . That&#39;s the core idea:building a system to organize my life as if it were an interconnected series of manufacturing processes. Saying it may initially seem somewhat impersonal; I believe it&#39;s the opposite: the goal is to use data and software to understand myself better. That&#39;s where the tagline comes from — I&#39;d like to take advantage of my own big data to make my life better in whatever ways I see fit at any given time. . Companies like Google and Facebook have been taking advantage of my big data for years, with the primary goal of making money. In the process of segmenting and profiling me, they&#39;ve grown to know a lot about me. I&#39;d like to have a similar data-driven profile of my life, for my own purposes. Namely, to learn more about myself and my life; to be able to optimize it. . I guess it&#39;s at this point that I can see people rolling their eyes and thinking this is just another productivity app — words like &quot;optimize&quot; don&#39;t help things. However, I want to get across the fact that because I have total control over this system, I get to choose exactly how it gets used and precisely what is optimized. While sometimes this optimization would come in the form of making myself more productive, it&#39;s equally likely that I&#39;ll want to optimize the length of time and quality of connection I have with family and friends. . Imagine that: a system that can help you find time to spend with family and friends, and find mindsets and/or conversation topics that will most likely increase the quality of those connections. . I think that sounds like the opposite of impersonal — getting intimate with oneself on levels and to a degree potentially not possible before. . Real-time Journal . I have many ways of describing the vision I have for the app, but one of the key features which communicates the value is the &quot;real-time journal&quot;. . My goal with this is to streamline the data capture part of the process such that capturing the data of everyday life — everyday experience — in real-time is not only possible, but easy and intuitive. Much of the resolution of memory is lost soon after events happen; best to document that experience as soon as possible with the important details that one might easily forget. . The added benefit to this is that with a real-time journal such as this, one starts to accumulate large amounts of data about one&#39;s own life. With the right tools, this data can lead to a deeper understanding of many aspects of life. . I&#39;ve been capturing this kind of data for the past ~2.5 years, and have accumulated over 26,000 records detailing just about every minute of every day. . Of course at the beginning the process was clunky, the data models crude. Over time, I continued to work on it until it became second nature; a background task that just happens. I improved the data models, which of course caused some compatibility issues between the new and old data. . I&#39;m still working on getting all 26k records in a single dataset. For now — i.e. in this notebook — I&#39;m going to use the data I&#39;ve gathered using the latest iteration of the data model, implemented at the beginning of December 2019. Therefore, this notebook will be using 12,297 records constituting ~10 months of data. . Data Model . I&#39;ll be going into more detail in proceeding notebooks and analyses, but it will be useful to this one to give a brief overview of how the journal data is structured. . The latest data model of my journal has 3 distinct layers, in descending order of granularity: . Project Log | Engagement Log | Moment Log | At the top level, I try to work on a single &quot;project&quot; for a certain period of time. This helps me stay focused on what I wanted to do/work on during that time. Another way to think about it is that this level defines overarching parts of the day, usually somewhere between 5 and 10, depending on what the day looks like. . Within each &quot;project block&quot; I switch between specific activities, such as between coding, writing, and reading/research. That is the second level, with each individual activity, or &quot;engagement&quot;, assigned to that higher level block in a many-to-one relationship. The second level is where the vast majority of the action is; where I document most of my experiences. . The third level is for very short activities I do that aren&#39;t necessarily related to the main activity. For example, I could be working on the notebook to explore this data but take short little breaks to get water, use the restroom, or clear my head. In the previous iteration of the data model I didn&#39;t account for these little activities — every activity was &quot;created equal&quot;, so to speak. Thus, in order to account for that time, I&#39;d have to split up and duplicate the main engagement record, interspersing the short breaks into them that way. Simply put, that caused too much overhead. . The goal with this project is to reduce the time and effort required to keep a real-time journal to the point where it doesn&#39;t interrupt what I&#39;m actually doing. . Notebook in context . I wanted to explain about the project and how the data is set up to give some context. The last bit of context for now is how this notebook fits into the project. . This notebook will focus primarily on exploring the engage_log table. . The primary features I&#39;ll be exploring in this notebook are: time_in time_out . duration | mental | physical | mental_note | physical_note | . There are many others that I could and will use, but I&#39;ll save those for another notebook. . I&#39;m starting with this table because it is the experiential layer in which most of my experience is documented. The other layers are mostly there to support the engage_log, whether to provide, along with other structural benefits, a group index (project_log), or to append additional details (moment_log). . With all the basics out of the way, let&#39;s get into it! . Imports and Config . from os import environ from pprint import pprint # Data manipulation import pandas as pd import numpy as np import janitor # Visualization import matplotlib.pyplot as plt import seaborn as sns # Pandas config pd.options.display.max_rows = 100 pd.options.display.max_columns = 100 . Accessing the data . I&#39;m currently working on a pipeline between Airtable (where the the data exists currently) and a local Postgres instance. Once that is complete, I&#39;ll load the data directly from there. . Until then, however, I&#39;m going to load the data from CSVs I downloaded from the Airtable browser-based GUI. . data_path = &quot;../../assets/data_/20-09-07-engage_log.csv&quot; engage_1 = pd.read_csv(data_path) . . Initial Wrangling . For the purposes of this notebook, the data is already relatively clean. There are just a few things to address before I can get into the exploration: . Prune the dataset to the features being used in this notebook . As I mentioned above, I&#39;m limiting the analysis in this notebook to a handful of features. Granted, they are some of the more important ones. But I don&#39;t want to get carried away this early in the analysis. The paradox of choice is a real thing; there is plenty to dig into with what I&#39;m using, and I will really only be able to scratch the surface of these features in this notebook. . Null values . The only null I&#39;ll be dropping is the one at the very end of the dataset. This is the record that hasn&#39;t ended yet, as of the moment of writing. The rest are just datapoints that I didn&#39;t gather, and can be filled with empty strings rather than dropped or imputed. . Fix data types . There are only a couple of issues with data types that need to be addressed. First, datetime columns will have to be converted to the datetime datatype. Second, the way Airtable exports the data, duration is exported as a string that&#39;s really only useful to the eyes. That should be an integer. . Pruning . print(&quot;Initial shape:&quot;, engage_1.shape) engage_keep_cols = [ &quot;time_in&quot;, &quot;time_out&quot;, &quot;duration&quot;, &quot;mental&quot;, &quot;physical&quot;, &quot;mental_note&quot;, &quot;physical_note&quot;, &quot;subloc&quot;, &quot;project_location&quot;, ] # Copy columns into new dataframe engage_2 = engage_1[engage_keep_cols].copy() print(&quot;After column pruning:&quot;, engage_2.shape) engage_2.head(3) . Initial shape: (12297, 23) After column pruning: (12297, 9) . time_in time_out duration mental physical mental_note physical_note subloc project_location . 0 2019-12-03 06:00 | 2019-12-03 06:19 | 19:00 | Podcast | Exercise | Full Stack Radio - Evan Yue Vue 3.0 + new e... | Cardio - elliptical | Elliptical | 24hr-Bel | . 1 2019-12-03 06:19 | 2019-12-03 06:37 | 18:00 | Podcast | Exercise | Full Stack Radio with Evan Yue Vue 3.0 - fi... | Cardio - stairs | Stairmaster | 24hr-Bel | . 2 2019-12-03 06:37 | 2019-12-03 07:02 | 25:00 | Podcast | Exercise | Django Chat Caching - something to read up ... | Weights - hip abduction in / out (machine) - k... | Machines | 24hr-Bel | . Nulls . engage_3 = engage_2.dropna(axis=0, subset=[&quot;time_out&quot;]) # Fill remaining nulls with empty string engage_3 = engage_3.fillna(value=&quot;&quot;) # Confirm it worked engage_3.isnull().sum() . time_in 0 time_out 0 duration 0 mental 0 physical 0 mental_note 0 physical_note 0 subloc 0 project_location 0 dtype: int64 . # from savor_code.pipelines import convert_datetime_cols def convert_datetime_cols(data: pd.DataFrame, dt_cols: list) -&gt; pd.DataFrame: &quot;&quot;&quot;If datetime columns exist in dataframe, convert them to datetime. :param data (pd.DataFrame) : DataFrame with datetime cols to be converted. :param dt_cols (list) : List of potential datetime cols. :return (pd.DataFrame) : DataFrame with datetime cols converted. &quot;&quot;&quot; data = data.copy() # Don&#39;t change original dataframe for col in dt_cols: if col in data.columns: # Make sure column exists data[col] = pd.to_datetime(data[col]) return data date_cols = [ &quot;time_in&quot;, &quot;time_out&quot;, ] engage_4 = convert_datetime_cols(engage_3, date_cols) engage_4.dtypes . time_in datetime64[ns] time_out datetime64[ns] duration object mental object physical object mental_note object physical_note object subloc object project_location object dtype: object . Convert duration to minutes . The duration feature was imported as a string formatted like so: [hh:]mm:ss. To convert this into minutes, I&#39;ll split on the colon and extract the hours and minutes, multiplying the hours by 60 and adding them to the minutes. I can leave out the seconds, as I did not capture the timestamps at that level of detail. . Unfortunately, if the hour is not present in the record, it simply doesn&#39;t include that segment. Therefore, I had to write a custom function to both split and calculate the minutes. . def split_and_calculate_mins(column): &quot;&quot;&quot;Splits up `duration` based on colon, accounting for missing hours. Expects format: [hh:]mm:ss.&quot;&quot;&quot; # Split up cell into component parts segments = [int(s) for s in column.split(&quot;:&quot;)] # Check length - if more than 2, means hour is present if len(segments) &gt; 2: # Calculate mins from hours and sum return (segments[0] * 60) + segments[1] elif len(segments) == 2: # Case with mins:secs # Simply return the minutes return segments[0] else: # Catch edge case when no duration return 0 . engage_4[&quot;duration&quot;] = engage_4[&quot;duration&quot;].apply(split_and_calculate_mins) engage_4.head() . time_in time_out duration mental physical mental_note physical_note subloc project_location . 0 2019-12-03 06:00:00 | 2019-12-03 06:19:00 | 19 | Podcast | Exercise | Full Stack Radio - Evan Yue Vue 3.0 + new e... | Cardio - elliptical | Elliptical | 24hr-Bel | . 1 2019-12-03 06:19:00 | 2019-12-03 06:37:00 | 18 | Podcast | Exercise | Full Stack Radio with Evan Yue Vue 3.0 - fi... | Cardio - stairs | Stairmaster | 24hr-Bel | . 2 2019-12-03 06:37:00 | 2019-12-03 07:02:00 | 25 | Podcast | Exercise | Django Chat Caching - something to read up ... | Weights - hip abduction in / out (machine) - k... | Machines | 24hr-Bel | . 3 2019-12-03 07:02:00 | 2019-12-03 07:08:00 | 6 | Podcast | Walk | Not so standard deviations misc discussions... | Walked to locker room then to car | Outside | 24hr-Bel | . 4 2019-12-03 07:08:00 | 2019-12-03 07:20:00 | 12 | Podcast | Drive | SE Daily TIBCO | | Trinity | 24hr-Bel | . engage_4.dtypes . time_in datetime64[ns] time_out datetime64[ns] duration int64 mental object physical object mental_note object physical_note object subloc object project_location object dtype: object . . Exploration and Visualization . There are so, so many interesting questions to ask and avenues to explore with this data. I&#39;d been brainstorming casually over the years on the topic of how to tackle the exploratory analysis and visualization. . Here are a few ideas to get me started: . How do I spend my time? Time spent on each activity | What patterns does this follow on a daily/weekly/monthly time horizon? | . | How much do I write about my experience? Word and character counts: total, total/avg per month over time | . | Sentiment analysis over time Does my mood oscillate according to any discernable pattern? | Does my mood correlate with spending time on particular activities? | . | . There&#39;s a lot more that can and will be done with this data. Best to leave it at that for now. In fact, I&#39;ll leave the sentiment analysis for a later notebook — the first two are enough to get started. . How do I spend my time? . The mental and physical features include the mental or physical activities that I&#39;m engaged in at any given moment. These are an obvious place to start exploring this question. . First, I find and visualize the top 10 mental and physical activities, by total time spent over the entire dataset. Then I break that up into monthly and weekly totals to get an idea of how my schedule has changed over the past 10 months. . act_1 = engage_4[[&quot;time_in&quot;, &quot;time_out&quot;, &quot;duration&quot;, &quot;mental&quot;, &quot;physical&quot;]].copy() act_1.head() . time_in time_out duration mental physical . 0 2019-12-03 06:00:00 | 2019-12-03 06:19:00 | 19 | Podcast | Exercise | . 1 2019-12-03 06:19:00 | 2019-12-03 06:37:00 | 18 | Podcast | Exercise | . 2 2019-12-03 06:37:00 | 2019-12-03 07:02:00 | 25 | Podcast | Exercise | . 3 2019-12-03 07:02:00 | 2019-12-03 07:08:00 | 6 | Podcast | Walk | . 4 2019-12-03 07:08:00 | 2019-12-03 07:20:00 | 12 | Podcast | Drive | . mental_set = {act for acts in act_1[&quot;mental&quot;] for act in acts.split(&quot;,&quot;)} physical_set = {act for acts in act_1[&quot;physical&quot;] for act in acts.split(&quot;,&quot;)} # How many distinct activities? print(len(mental_set)) print(len(physical_set)) . 51 65 . Total time spent on activities . from collections import Counter # Counters for total time and total number of occurrences mental_sums = Counter() physical_sums = Counter() mental_count = Counter() physical_count = Counter() # Iterate through all rows + lists within rows for row in act_1.itertuples(index=False): # Split on comma, add time to each item for w in row[3].split(&quot;,&quot;): mental_count[w] += 1 mental_sums[w] += row[2] for f in row[4].split(&quot;,&quot;): physical_count[f] += + 1 physical_sums[f] += row[2] . print(&quot;Time spent on top 10 activities, in minutes&quot;) print(&quot;Mental:&quot;) for mental in mental_sums.most_common(10): print(f&quot; {mental}&quot;) print(&quot; nPhysical:&quot;) for physical in physical_sums.most_common(10): print(f&quot; {physical}&quot;) . Time spent on top 10 activities, in minutes Mental: (&#39;Sleep&#39;, 129453) (&#39;Converse&#39;, 72596) (&#39;Code&#39;, 49092) (&#39;Watch&#39;, 28160) (&#39;Podcast&#39;, 21496) (&#39;Audiobook&#39;, 21309) (&#39;Information&#39;, 21146) (&#39;Learn&#39;, 19914) (&#39;Think&#39;, 19283) (&#39;Write&#39;, 15077) Physical: (&#39;Rest&#39;, 152607) (&#39;Sit&#39;, 86098) (&#39;Stand&#39;, 70127) (&#39;Drive&#39;, 13296) (&#39;Eat&#39;, 11113) (&#39;Cook&#39;, 8448) (&#39;Dress&#39;, 7028) (&#39;Dishes&#39;, 6124) (&#39;Exercise&#39;, 6086) (&#39;Dental hygiene&#39;, 5998) . mental_bar_1 = (pd.DataFrame.from_dict(dict(mental_sums.most_common(10)), orient=&quot;index&quot;, columns=[&quot;count&quot;]) .reset_index() .rename_column(&quot;index&quot;, &quot;mental_activity&quot;) ) # Visualize the top mental activities plt.figure(figsize=(12, 6)) sns.barplot(&quot;mental_activity&quot;, &quot;count&quot;, data=mental_bar_1, palette=&quot;deep&quot;); plt.title(&quot;Top 10 mental activities based on total time spent, in minutes&quot;); . physical_bar_1 = (pd.DataFrame.from_dict(dict(physical_sums.most_common(10)), orient=&quot;index&quot;, columns=[&quot;count&quot;]) .reset_index() .rename_column(&quot;index&quot;, &quot;physical_activity&quot;) ) # Visualize the top physical activities plt.figure(figsize=(12, 6)) sns.barplot(&quot;physical_activity&quot;, &quot;count&quot;, data=physical_bar_1, palette=&quot;deep&quot;); plt.title(&quot;Top 10 physical activities based on total time spent, in minutes&quot;); . Monthly totals . To get a monthly sum of time spent on the top 10 activities, I encode each into its own feature — almost one-hot-encoding style, with the difference of using the duration value instead of 1 or 0. With the data in this form, it is simple to group by month then sum to get monthly totals for each activity. . act_mental = act_1.copy() # Copy to not mess with original dataframe # Iterate through top 10 for mental in mental_sums.most_common(10): # Create new function to apply with each iteration def create_activity_feature_with_duration(df): &quot;&quot;&quot;If activity is assigned, copy duration value into that new col.&quot;&quot;&quot; if mental[0] in df[&quot;mental&quot;]: return df[&quot;duration&quot;] act_mental = act_mental.join_apply(create_activity_feature_with_duration, mental[0].lower()) # Example of duration value act_mental[[&quot;time_in&quot;, &quot;time_out&quot;, &quot;duration&quot;, &quot;podcast&quot;]].head() . time_in time_out duration podcast . 0 2019-12-03 06:00:00 | 2019-12-03 06:19:00 | 19 | 19.0 | . 1 2019-12-03 06:19:00 | 2019-12-03 06:37:00 | 18 | 18.0 | . 2 2019-12-03 06:37:00 | 2019-12-03 07:02:00 | 25 | 25.0 | . 3 2019-12-03 07:02:00 | 2019-12-03 07:08:00 | 6 | 6.0 | . 4 2019-12-03 07:08:00 | 2019-12-03 07:20:00 | 12 | 12.0 | . act_physical = act_1.copy() # Copy to not mess with last dataframe # Not adding to the same dataframe, as the visualizations will be separate # Iterate through top 10 for physical in physical_sums.most_common(10): # Create new function to apply with each iteration def create_activity_feature_with_duration(df): &quot;&quot;&quot;If activity is assigned, copy duration value into that new col.&quot;&quot;&quot; if physical[0] in df[&quot;physical&quot;]: return df[&quot;duration&quot;] act_physical = act_physical.join_apply(create_activity_feature_with_duration, physical[0].lower()) act_physical[[&quot;time_in&quot;, &quot;time_out&quot;, &quot;duration&quot;, &quot;drive&quot;, &quot;exercise&quot;]].head() . time_in time_out duration drive exercise . 0 2019-12-03 06:00:00 | 2019-12-03 06:19:00 | 19 | NaN | 19.0 | . 1 2019-12-03 06:19:00 | 2019-12-03 06:37:00 | 18 | NaN | 18.0 | . 2 2019-12-03 06:37:00 | 2019-12-03 07:02:00 | 25 | NaN | 25.0 | . 3 2019-12-03 07:02:00 | 2019-12-03 07:08:00 | 6 | NaN | NaN | . 4 2019-12-03 07:08:00 | 2019-12-03 07:20:00 | 12 | 12.0 | NaN | . Now that I have the top 10 activities as their own features with their duration values, I&#39;ll need to group by month. . There are multiple ways to accomplish this. One way would be to extract the month component of the time_in feature and use that as the grouping column. Or, I could set time_in as a DatetimeIndex index and use time-aware grouping methods. . I&#39;m going to do the latter. . mental_top_activities = [m[0].lower() for m in mental_sums.most_common(10)] physical_top_activities = [m[0].lower() for m in physical_sums.most_common(10)] . act_mental_2 = act_mental.set_index(pd.DatetimeIndex(act_mental[&quot;time_in&quot;])) act_physical_2 = act_physical.set_index(pd.DatetimeIndex(act_physical[&quot;time_in&quot;])) . mental_by_month = act_mental_2.groupby(pd.Grouper(freq=&quot;M&quot;)) physical_by_month = act_physical_2.groupby(pd.Grouper(freq=&quot;M&quot;)) # Use lists to get sums from groupby # Remove last month, as it would throw things off mental_sum_by_month = mental_by_month[mental_top_activities].sum().drop(pd.to_datetime(&quot;2020-09-30&quot;), axis=0) physical_sum_by_month = physical_by_month[physical_top_activities].sum().drop(pd.to_datetime(&quot;2020-09-30&quot;), axis=0) . mental_sum_by_month . . sleep converse code watch podcast audiobook information learn think write . time_in . 2019-12-31 12838.0 | 7752.0 | 4878.0 | 3727.0 | 2118.0 | 1994.0 | 678.0 | 4175.0 | 2160.0 | 84.0 | . 2020-01-31 14236.0 | 8193.0 | 3947.0 | 2643.0 | 1697.0 | 1403.0 | 1474.0 | 430.0 | 2530.0 | 674.0 | . 2020-02-29 12164.0 | 6227.0 | 10743.0 | 1581.0 | 2164.0 | 2359.0 | 6010.0 | 794.0 | 1276.0 | 384.0 | . 2020-03-31 13793.0 | 5364.0 | 8493.0 | 5044.0 | 1897.0 | 2991.0 | 606.0 | 2802.0 | 2636.0 | 606.0 | . 2020-04-30 14778.0 | 5718.0 | 8614.0 | 7279.0 | 2001.0 | 2021.0 | 882.0 | 5910.0 | 1786.0 | 412.0 | . 2020-05-31 15791.0 | 9451.0 | 3394.0 | 2428.0 | 2814.0 | 2156.0 | 1000.0 | 2658.0 | 2117.0 | 2913.0 | . 2020-06-30 14278.0 | 8905.0 | 2619.0 | 1787.0 | 1519.0 | 1874.0 | 2081.0 | 720.0 | 2152.0 | 3608.0 | . 2020-07-31 14090.0 | 11991.0 | 2877.0 | 2421.0 | 2092.0 | 2417.0 | 3932.0 | 1097.0 | 2091.0 | 2863.0 | . 2020-08-31 15062.0 | 6997.0 | 2186.0 | 992.0 | 3852.0 | 3044.0 | 3714.0 | 490.0 | 2071.0 | 2757.0 | . mental_sum_by_month.plot(figsize=(12, 6)) plt.title(&quot;Top 10 mental activities based on minutes spent, by month&quot;); . Now we&#39;re talking! This has some interesting information in it. . For example, the increase in time spent coding (and relative decrease in sleep) from around February through March corresponds to when I was working on Trash Panda (Jan - Mar) then studying computer science at Lambda School (Mar - May). Then as my job search began in earnest, the amount of time I spent networking (conversing) increased, while coding time unfortunately decreased. . physical_sum_by_month . . rest sit stand drive eat cook dress dishes exercise dental hygiene . time_in . 2019-12-31 14961.0 | 10120.0 | 6474.0 | 1747.0 | 1486.0 | 805.0 | 736.0 | 524.0 | 607.0 | 833.0 | . 2020-01-31 16424.0 | 6944.0 | 9166.0 | 549.0 | 1125.0 | 949.0 | 748.0 | 423.0 | 1150.0 | 490.0 | . 2020-02-29 14181.0 | 9330.0 | 11372.0 | 1337.0 | 917.0 | 1052.0 | 386.0 | 674.0 | 37.0 | 975.0 | . 2020-03-31 16282.0 | 11608.0 | 9726.0 | 420.0 | 1125.0 | 1286.0 | 1185.0 | 835.0 | 449.0 | 1470.0 | . 2020-04-30 17553.0 | 11257.0 | 9801.0 | 153.0 | 1121.0 | 1430.0 | 651.0 | 1013.0 | 766.0 | 443.0 | . 2020-05-31 17562.0 | 10693.0 | 3906.0 | 1954.0 | 1482.0 | 1241.0 | 1859.0 | 846.0 | 849.0 | 416.0 | . 2020-06-30 17479.0 | 8644.0 | 4450.0 | 1691.0 | 1132.0 | 419.0 | 478.0 | 541.0 | 715.0 | 451.0 | . 2020-07-31 18024.0 | 11246.0 | 3996.0 | 2626.0 | 1391.0 | 476.0 | 497.0 | 451.0 | 355.0 | 405.0 | . 2020-08-31 17459.0 | 4958.0 | 9409.0 | 1615.0 | 1115.0 | 741.0 | 382.0 | 729.0 | 1128.0 | 424.0 | . physical_sum_by_month.plot(figsize=(12, 6)) plt.title(&quot;Top 10 physical activities based on minutes spent, by month&quot;); . One obvious and interesting aspect of my physical activities over time is the interplay between sitting and standing (and driving, to some extent). I say driving, because the period of time beginning around May is when I began to take road trips from Colorado to California to visit family. As I was not at my desk, I tended to not stand while working. . Weekly totals . I found the monthly totals to be an interesting overview of the data. But I figured weekly totals would be even more interesting. The process for manipulating the data is pretty much the same, just grouped by week instead of by month. . mental_by_week = act_mental_2.groupby(pd.Grouper(freq=&quot;W&quot;)) physical_by_week = act_physical_2.groupby(pd.Grouper(freq=&quot;W&quot;)) # Use lists to get sums from groupby # Remove last month, as it would throw things off mental_sum_by_week = mental_by_week[mental_top_activities].sum().drop(pd.to_datetime(&quot;2020-09-13&quot;), axis=0) physical_sum_by_week = physical_by_week[physical_top_activities].sum().drop(pd.to_datetime(&quot;2020-09-13&quot;), axis=0) . mental_sum_by_week.plot(figsize=(12, 6)) plt.title(&quot;Top 10 mental activities based on minutes spent, by week&quot;); . The same basic patterns show up here as in the monthly groups, but with more detail (obviously). Although more detail is generally good, it does make the visualization a lot more hectic. It&#39;s a lot to look at. . The detail gives a better idea of when exactly my time spent conversing increased as drastically as it did this summer. Those two peaks correspond to when I travelled out to California to spend time with my family. . One part I see sticking out more in this than when looking at it monthly is the increase in time spent writing that starts in the middle of May. That is when I was finishing up a whole bunch of articles for my portfolio. Then, in June I committed to spending at least 10 extra minutes journaling at the end of the day. . physical_sum_by_week.plot(figsize=(12, 6)) plt.title(&quot;Top 10 physical activities based on minutes spent, by week&quot;); . The sitting/standing dynamic here is even more drastic and interesting. Even though they have a generally negative relationship, there is are times when both are increasing, like from January to March. I wonder where that time came from? . How much I write about my experience . Seeing as this is a journal, I thought it would be interesting to look at how much I actually write in it. There are definitely times when I choose to spend more time on the actual writing part than others. I imagine that these patterns will be noticeable. . Here are the visualizations I&#39;m going to generate: . Total word count | Total character count | Word count over time, per month and week | Character count over time, per month and week | . content_1 = engage_4.drop(columns=[&quot;time_out&quot;, &quot;subloc&quot;, &quot;project_location&quot;]) content_1.head() . time_in duration mental physical mental_note physical_note . 0 2019-12-03 06:00:00 | 19 | Podcast | Exercise | Full Stack Radio - Evan Yue Vue 3.0 + new e... | Cardio - elliptical | . 1 2019-12-03 06:19:00 | 18 | Podcast | Exercise | Full Stack Radio with Evan Yue Vue 3.0 - fi... | Cardio - stairs | . 2 2019-12-03 06:37:00 | 25 | Podcast | Exercise | Django Chat Caching - something to read up ... | Weights - hip abduction in / out (machine) - k... | . 3 2019-12-03 07:02:00 | 6 | Podcast | Walk | Not so standard deviations misc discussions... | Walked to locker room then to car | . 4 2019-12-03 07:08:00 | 12 | Podcast | Drive | SE Daily TIBCO | | . To get the actual word and character counts, I use the pandas .apply() method along with simple lambda functions. . content_2 = content_1.copy() # Copy to isolate cell&#39;s effect on df for feat in [&quot;mental_note&quot;, &quot;physical_note&quot;]: content_2[f&quot;{feat}_word_count&quot;] = content_2[feat].apply(lambda x: len(x.split())) content_2[f&quot;{feat}_char_count&quot;] = content_2[feat].apply(lambda x: len(x)) . count_cols = [ &quot;mental_note_word_count&quot;, &quot;physical_note_word_count&quot;, &quot;mental_note_char_count&quot;, &quot;physical_note_char_count&quot;, ] content_3_counts_only = content_2.set_index(pd.DatetimeIndex(content_2[&quot;time_in&quot;]))[count_cols].copy() content_3_counts_only.head() . mental_note_word_count physical_note_word_count mental_note_char_count physical_note_char_count . time_in . 2019-12-03 06:00:00 18 | 3 | 75 | 20 | . 2019-12-03 06:19:00 16 | 3 | 78 | 16 | . 2019-12-03 06:37:00 42 | 15 | 197 | 77 | . 2019-12-03 07:02:00 9 | 7 | 59 | 33 | . 2019-12-03 07:08:00 4 | 0 | 17 | 0 | . Once the counts are calculated, the process of grouping and visualizing is pretty much the same before. . counts_by_week = content_3_counts_only.groupby(pd.Grouper(freq=&quot;W&quot;)) # Use lists to get sums from groupby # Remove last month, as it would throw things off counts_sum_by_week = counts_by_week[count_cols].sum().drop(pd.to_datetime(&quot;2020-09-13&quot;), axis=0) . counts_sum_by_week[[&quot;mental_note_word_count&quot;, &quot;mental_note_char_count&quot;]].plot(figsize=(12, 6)); plt.title(&quot;Mental note - word and character counts, by week&quot;); . Considering that I put all of my coding journals into this journal as well, it makes sense that the time I spent working on Trash Panda also had me putting a lot more into the mental notes. . counts_sum_by_week[[&quot;physical_note_word_count&quot;, &quot;physical_note_char_count&quot;]].plot(figsize=(12, 6)); plt.title(&quot;Physical note - word and character counts, by week&quot;); . An interesting aspect of the physical word and character counts is that I started getting back into my daily exercising routine, including my return to learning how to trick. It is interesting that the physical notes also increased so much during Trash Panda as well — I guess I was just writing more in general. . Character/word ratio . One last thing I&#39;m going to look at is the ratio between character and word counts, which gives an idea of the size of the words I&#39;m using. . counts_sum_by_week_2 = counts_sum_by_week.copy() counts_sum_by_week_2[&quot;mental_char_word_ratio&quot;] = counts_sum_by_week_2[&quot;mental_note_char_count&quot;] / counts_sum_by_week_2[&quot;mental_note_word_count&quot;] counts_sum_by_week_2[&quot;physical_char_word_ratio&quot;] = counts_sum_by_week_2[&quot;physical_note_char_count&quot;] / counts_sum_by_week_2[&quot;physical_note_word_count&quot;] . counts_sum_by_week_2[[&quot;mental_char_word_ratio&quot;, &quot;physical_char_word_ratio&quot;]].plot(figsize=(12, 6)); plt.title(&quot;Physical and mental notes - characters per word, by week&quot;); . I imagine that the reason the number of characters per word increased around April is that I spent that time studing into computer science, which likely has longer words than what I use normally. Along with that was a general increase in my learning, as I went deep into that problem-solving mindset. . . Final Thoughts . These explorations are only the very beginning of a scratch on the surface of the information contained in the data. But they are a good first step! . I believe that the data will truly start to shine once the actual text contained within the records are added to the analysis, as I will hopefully be able to quantify some deeper aspects of my experience. . As always, thank you for reading, and I&#39;ll see you in the next one! .",
            "url": "https://tobias-fyi.github.io/fastfyi/blog/savor-data-analysis-part-1",
            "relUrl": "/blog/savor-data-analysis-part-1",
            "date": " • Sep 7, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Automated Image Background Removal with Python",
            "content": ". Introduction . TL;DR . The goal of this tutorial is to describe one method of automating the process of cutting out objects (things, people, pets, etc.) from images and combining them to make a collage of sorts. . First, I go through creating binary masks for one or more objects in an image by using a class of computer vision algorithms called image segmentation. Binary mask(s) in hand(s), I go through one method (technically two, actually) of using said binary mask(s) to extract or remove part(s) of an image. Next, I do some basic image transformations (rotate, crop, and scale) on the resulting cutout. Finally, I paste the cutout on top of another image to make a collage. . Rather than drawing binary masks by hand or using proprietary software like Photoshop to manipulate and transform images, I&#39;ll show you how to automate the process using completely free, open-source tools. Namely, we&#39;ll be using Python along with a few open-source libraries: . NumPy | OpenCV - opencv-python | PyTorch - Detectron2 | Pillow | . . The Problem . Selecting and separating parts of an image can be a tedious, time-consuming process. Anyone who&#39;s done a fair amount of tinkering with image manipulation using a program like Photoshop knows the struggle. . Although modern tools make the process easier, wouldn&#39;t it be nice if there was a way to automate the process? . Creating &quot;Paws&quot; . As an example, say I&#39;d like to cut out my cat Hobbes from a photo in order to &quot;Photoshop&quot; him into a different image. Here&#39;s the photo of Hobbes I&#39;ll be using. . . I think his position is perfect for creating &quot;Hawbbes&quot; (Jaws + Hobbes)...meh I&#39;ll call it &quot;Paws&quot;. By cutting him out and rotating him a bit, he could be pasted onto an underwater shot of someone swimming and he could live his dream of being a fierce sharkitty. . Here&#39;s an image I found on Unsplash that would work as the background onto which Hobbes, once cut out of the image above, can be pasted. . . Basically, in order to cut Hobbes out of the photo above, I&#39;ll have to make all the pixels in the image transparent except for the ones representing Hobbes. Then I&#39;ll crop, rotate, scale, and superimpose the resulting image on top of the underwater shot such that Hobbes roughly takes up the bottom half. . . . Solution . Image Masking . To accomplish this manually, I could spend anywhere from a few minutes to a few hours outlining Hobbes in the original image to create a mask — masking the image. The time investment depends on how easily-separable the subject is from the rest of the image, how accurate I want the cut to be, and what tools are available to me. . Regarding that last point, the magicians at Adobe have done some rather impressive black magic with Photoshop, giving users very quick and very effective methods for selecting parts of an image. However, the goal of this post is to accomplish this programmatically, without the use of any closed-source software. . A mask is basically a method of distinguishing/selecting/separating pixels. If you&#39;ve never heard the term used this way before, one way to think about it is with masking tape and paint. Typically, one would put masking tape—i.e. create a &quot;mask&quot;—around areas on a wall that should not be painted. This is essentially what a mask is doing in any photo manipulation software: indicating what areas of an image to affect or transform (or which areas not to do so). . Here&#39;s the image of Hobbes with the image segmentation-generated masks overlayed on top of it (which we&#39;ll be creating later) showing, obviously, where Hobbes is in the image. It doesn&#39;t really matter that the model thinks he&#39;s a dog — we won&#39;t be using the predicted class, only the mask. And the mask is still good enough for our purposes. . . A binary mask is a method of masking which uses a two-tone color scheme, to indicate the areas of an image to be affected and not affected. By overlaying a binary mask on top of the original image, the boundaries between the two colors can be used to affect the different areas of the image differently, whether that is making pixels transparent (removing them) or applying some sort of effect or transformation. . The white area in the image below shows the same coordinates as the orange one above, converted into a binary mask. While I&#39;ve only spent any significant time with Photoshop, I&#39;d imagine any decent image manipulation software can work with binary masks similarly to how we&#39;ll be working with them. . . Computer vision . In order to generate binary masks based on the content of the image, the algorithm must be somewhat intelligent. That is, it must be able to process the image in such a way that it can recognize where the foreground is and draw a polygon around it with some degree of accuracy. . Luckily, there are a number of deep learning models that will do just that. The field is called Computer Vision, and the class of algorithm used in this article is known as image segmentation. . Don&#39;t worry if you don&#39;t have any experience with this type of thing, or even if you don&#39;t necessarily want to get experience with it. Modern machine learning tooling makes it incredibly quick and easy to get a model up and predicting with pre-trained weights. Though if you want to understand what&#39;s going on, it will likely help to know a bit of Python programming. . One caveat: the pre-trained models will usually work well with classes of objects that were in their training data. The model weights used in this post were trained on the COCO dataset, which contains 80 object classes. Depending on what the object in the foreground is that you are trying to extract, you may or may not need to extend the model with a custom dataset and training session. That is a topic for another post. . Detectron2 . The deep learning framework used here is PyTorch, developed by Facebook AI Research (FAIR). More specifically, we&#39;ll use a computer vision framework, also developed by FAIR, called Detectron2. . Although the primary framework used in this article is Detectron2, this process should be translatable to other image segmentation models as well. In fact, I&#39;ll be adding an addendum to this post in which I&#39;ll go over using Matterport&#39;s TensorFlow-based implementation of Mask R-CNN to accomplish the exact same thing. . Heck, while I&#39;m at it, I might as well do it with fastai as well. . End Result . I know you&#39;ve been dying to see the end result of the whole process. . Without any further ado, I present to you, Paws! . . [[Caption :: All of that was done with code. Pretty neat, eh?]] With that, let&#39;s get into how this masterpiece was created. . . . Setup . Install Detectron2 and other dependencies . As mentioned in the introduction, the framework we&#39;ll be using for image segmentation is called Detectron2. The following cells install and set up Detectron2 in a Google Colab environment (pulled from the official Detectron2 getting started notebook). If you don&#39;t want to use Colab for whatever reason, either play around with installation and setup or refer to the installation instructions. . The other top-level dependencies needed for this tutorial: . NumPy | opencv-python | Pillow | . The nice thing about Colab is all of these come pre-installed. Oh yeah, you also get free access to a GPU. Thanks, Googs! . Again, simply click the &quot;Open in Colab&quot; badge at the top of this page, then hit File &gt; Save a copy in Drive, which does exactly what it says: saves a copy of the notebook to your Google Drive. In addition, you can open an ephemeral copy of the notebook without saving it first by hitting File &gt; Open in playground mode. . Once you have everything installed, we can start with some imports and configuration. . # === Some imports and setup === # # Setup Detectron2 logger import detectron2 from detectron2.utils.logger import setup_logger setup_logger() # Common libraries import numpy as np import os, json, cv2, random # Only needed when running in Colab from google.colab.patches import cv2_imshow # Detectron2 utilities from detectron2 import model_zoo from detectron2.engine import DefaultPredictor from detectron2.config import get_cfg from detectron2.utils.visualizer import Visualizer from detectron2.data import MetadataCatalog, DatasetCatalog . . . . Running a pre-trained Detectron2 model . Most, if not all, open-source deep learning frameworks have a set of pre-trained weights available to use. The creators of the frameworks will conduct a series of training sessions on the most commonly-used datasets in order to benchmark the performance of their algorithms. Luckily for everyone else, they typically provide the results of this training in the form of weights, which can be loaded into the model and be used for inference immediately. . For many tasks, including recognizing and outlining an image of a cat, pre-trained weights will work fine. The model weights used in this post were trained on the popular COCO dataset, which contains 80 object classes, including cats. If, for example, we wanted to do the same thing with whales or one specific person, we&#39;d have to do some custom training. . I will be publishing a companion blog post to this one about training Detectron2 on a custom dataset. Once that is published, I&#39;ll link to it here. If there&#39;s no link yet, I haven&#39;t published it yet. . If you&#39;re curious about custom training now, the Detectron2 &quot;Getting Started&quot; Colab notebook also goes through one way of doing so. . Image loading and image arrays . The first thing we need in order to use the model is an image on which it can be used. . We first download the image to the local filesystem using wget, then load it into memory using cv2 (opencv-python). . !wget https://raw.githubusercontent.com/tobias-fyi/assetstash/master/visual/images/img_seg_bin_mask/01_hobbes.jpg im = cv2.imread(&quot;./01_hobbes.jpg&quot;) . --2020-07-20 19:35:16-- https://raw.githubusercontent.com/tobias-fyi/assetstash/master/visual/images/img_seg_bin_mask/01_hobbes.jpg Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ... Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 344670 (337K) [image/jpeg] Saving to: ‘01_hobbes.jpg’ 01_hobbes.jpg 100%[===================&gt;] 336.59K --.-KB/s in 0.05s 2020-07-20 19:35:16 (7.01 MB/s) - ‘01_hobbes.jpg’ saved [344670/344670] . If you think about what a digital image actually is, it makes sense to represent it as a matrix — each row corresponds to a row of pixels, and each column a column of pixels in the image. Technically, images would be considered a 3-dimensional array, because they have width, height, and depth (number of channels). . Depending on if the image has three channels (typically RGB: red, green, blue) or four (typically RGBA: same plus an alpha channel), the values at each row-column index (or cell, like in a spreadsheet, in case that helps you visualize it) indicate the intensities of each of the color channels (and transparency, in the case of 4-channel images) for each pixel. . Thus, after the image is loaded, it really is just an array of numbers and can be utilized and manipulated just like any other array. For example, in order to rotate the image, a linear transformation can be applied to the image matrix to &quot;rotate&quot; the pixel values within the matrix. . Here is an example of a single row in the array representing the image of Hobbes is shown. . # === Look at the image, in array form === # print(&quot;Image dimensions:&quot;, im.shape) print(&quot; nImage array - first row of 3-value sub-arrays:&quot;) im[0] . . Image dimensions: (800, 600, 3) Image array - first row of 3-value sub-arrays: . array([[172, 192, 209], [188, 208, 225], [119, 137, 154], ..., [137, 151, 149], [139, 153, 151], [142, 156, 154]], dtype=uint8) . # === Look at the image, rendered === # cv2_imshow(im) . . . Inference with Detectron2 . After the image is loaded, we&#39;re ready to use Detectron2 to run inference on the image and find the mask of Hobbes. Running inference means generating predictions from the model. In the case of image segmentation, the model is making a prediction for each pixel, providing its best guess at what class of object each one belongs to, if any. . We create a Detectron2 config and instantiate a DefaultPredictor, which is then used to run inference. . Just a heads-up: the first time this runs, it will automatically attempt to start downloading the pre-trained weights — a ~180mb pickle file. That&#39;s a lot of pickles... . In addition to downloading and configuring the weights, the threshold is set for the minimum predicted probability. In other words, the model will only output a prediction if it is certain enough — the probability assigned to the prediction is above the threshold. . # Add project-specific config (e.g., TensorMask) here if you&#39;re # not running a model in Detectron2&#39;s core library cfg = get_cfg() cfg.merge_from_file(model_zoo.get_config_file(&quot;COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml&quot;)) cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5 # set threshold for this model # Find a model from detectron2&#39;s model zoo. You can use the https://dl.fbaipublicfiles... url as well cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(&quot;COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml&quot;) predictor = DefaultPredictor(cfg) outputs = predictor(im) . model_final_f10217.pkl: 178MB [00:05, 29.7MB/s] . By default, the output of the model contains a number of results, including the predicted classes, coordinates for the bounding boxes (object detection), and mask arrays (image segmentation), along with various others, such as pose estimation (for people). More information on the types of predictions made by Detectron2 can be found in the documentation. . We are really only interested in the one mask outlining Mr. Hobbes here, though will also need to extract the IDs for the predicted classes in order to select the correct mask. If the image only has one type of object, then this part isn&#39;t really necessary. But when there are many different classes in a single image, it&#39;s important to be certain which object we are extracting. . First, let&#39;s take a look at how the model did. . v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=0.8) out = v.draw_instance_predictions(outputs[&quot;instances&quot;].to(&quot;cpu&quot;)) cv2_imshow(out.get_image()[:, :, ::-1]) . . Extracting the mask . The &quot;dog&quot; is the first id in the .pred_classes list. Therefore, the mask that we want is the first one in the .pred_masks tensor (array). . Those colored areas are the &quot;masks&quot;, which can be extracted from the output of the model and used to manipulate the image in neat ways. First, we&#39;ll need to get the array holding the mask. . In this case, as can be seen below, each mask is a 2-dimensional array of Boolean values, each one representing a pixel. If a pixel has a &quot;True&quot; value, that means it is inside the mask, and vice-versa. . print(outputs[&quot;instances&quot;].pred_classes) print(outputs[&quot;instances&quot;].pred_masks) . tensor([16, 57, 57], device=&#39;cuda:0&#39;) tensor([[[False, False, False, ..., False, False, False], [False, False, False, ..., False, False, False], [False, False, False, ..., False, False, False], ..., [False, False, False, ..., False, False, False], [False, False, False, ..., False, False, False], [False, False, False, ..., False, False, False]], [[False, False, False, ..., False, False, False], [False, False, False, ..., False, False, False], [False, False, False, ..., False, False, False], ..., [False, False, False, ..., False, False, False], [False, False, False, ..., False, False, False], [False, False, False, ..., False, False, False]], [[False, False, False, ..., False, False, False], [False, False, False, ..., False, False, False], [False, False, False, ..., False, False, False], ..., [False, False, False, ..., False, False, False], [False, False, False, ..., False, False, False], [False, False, False, ..., False, False, False]]], device=&#39;cuda:0&#39;) . # List of all classes in training dataset (COCO) # predictor.metadata.as_dict()[&quot;thing_classes&quot;] # The class we are interested in predictor.metadata.as_dict()[&quot;thing_classes&quot;][16] . &#39;dog&#39; . # Find the index of the class we are interested in # First, convert to numpy array to allow direct indexing class_ids = np.array(outputs[&quot;instances&quot;].pred_classes.cpu()) class_index = np.where(class_ids == 16) # Find index where class ID is 16 # Use that index to index the array of masks and boxes mask_tensor = outputs[&quot;instances&quot;].pred_masks[class_index] print(mask_tensor.shape) mask_tensor . torch.Size([1, 800, 600]) . tensor([[[False, False, False, ..., False, False, False], [False, False, False, ..., False, False, False], [False, False, False, ..., False, False, False], ..., [False, False, False, ..., False, False, False], [False, False, False, ..., False, False, False], [False, False, False, ..., False, False, False]]], device=&#39;cuda:0&#39;) . hobbes_mask = mask_tensor.cpu() print(&quot;Before:&quot;, type(hobbes_mask)) print(hobbes_mask.shape) hobbes_mask = np.array(hobbes_mask[0]) print(&quot;After:&quot;, type(hobbes_mask)) print(hobbes_mask.shape) . Before: &lt;class &#39;torch.Tensor&#39;&gt; torch.Size([1, 800, 600]) After: &lt;class &#39;numpy.ndarray&#39;&gt; (800, 600) . hobbes_mask . array([[False, False, False, ..., False, False, False], [False, False, False, ..., False, False, False], [False, False, False, ..., False, False, False], ..., [False, False, False, ..., False, False, False], [False, False, False, ..., False, False, False], [False, False, False, ..., False, False, False]]) . . Converting to a binary mask . Now that we&#39;ve run inference on the image and retrieved the mask array, it&#39;s time to turn that array into a binary mask. While I won&#39;t be using this particular binary mask directly, it can be downloaded as a png and/or edited and used to various ends. . # The &quot;True&quot; pixels will be converted to white and copied onto the black background background = np.zeros(hobbes_mask.shape) background.shape . (800, 600) . bin_mask = np.where(hobbes_mask, 255, background).astype(np.uint8) print(bin_mask.shape) bin_mask . (800, 600) . array([[0, 0, 0, ..., 0, 0, 0], [0, 0, 0, ..., 0, 0, 0], [0, 0, 0, ..., 0, 0, 0], ..., [0, 0, 0, ..., 0, 0, 0], [0, 0, 0, ..., 0, 0, 0], [0, 0, 0, ..., 0, 0, 0]], dtype=uint8) . cv2_imshow(bin_mask) . . Using the binary mask to cut out Hobbes . In order to use numpy operations between the mask and image, the dimensions of the mask must match the image. The image array has three values for each pixel, indicating the values of red, green, and blue (RGB) that the pixel should render. Therefore, the mask must also have three values for each pixel. To do this, I used a NumPy method called np.stack to basically &quot;stack&quot; three of the masks on top of one another. . Once the dimensions match, another NumPy method, np.where, can be used to copy or extract only the pixels contained within the area of the mask. I created a blank background onto which those pixels are copied. . # Split into RGB (technically BGR in OpenCV) channels b, g, r = cv2.split(im.astype(&quot;uint8&quot;)) # Create alpha channel array of ones # Then multiply by 255 to get the max transparency value a = np.ones(hobbes_mask.shape, dtype=&quot;uint8&quot;) * 255 print(b.shape, g.shape, r.shape, a.shape) . (800, 600) (800, 600) (800, 600) (800, 600) . # We want the image to be fully opaque at this point a . array([[255, 255, 255, ..., 255, 255, 255], [255, 255, 255, ..., 255, 255, 255], [255, 255, 255, ..., 255, 255, 255], ..., [255, 255, 255, ..., 255, 255, 255], [255, 255, 255, ..., 255, 255, 255], [255, 255, 255, ..., 255, 255, 255]], dtype=uint8) . # Rejoin with alpha channel that&#39;s always 1, or non-transparent rgba = [b, g, r, a] # Both of the lines below accomplish the same thing im_4ch = cv2.merge(rgba, 4) # im_4ch = np.stack([b, g, r, a], axis=2) print(im_4ch.shape) cv2_imshow(im_4ch) . (800, 600, 4) . # Create 4-channel blank background bg = np.zeros(im_4ch.shape) print(&quot;BG shape:&quot;, bg.shape) # Create 4-channel mask mask = np.stack([hobbes_mask, hobbes_mask, hobbes_mask, hobbes_mask], axis=2) print(&quot;Mask shape:&quot;, mask.shape) # Copy color pixels from the original color image where mask is set foreground = np.where(mask, im_4ch, bg).astype(np.uint8) # Check out the result cv2_imshow(foreground) . BG shape: (800, 600, 4) Mask shape: (800, 600, 4) . . The Roundabout Method . This is that &quot;second&quot; method I talked about in the introduction. . This is how I added a fourth channel to the image after the fact, once the colored pixels had been copied onto a black background. While this method works, I&#39;m sure you can think of one primary issue with it. . It took me too long to realize this, but by using a black background and the method below, which converts all black pixels to transparent, any pixels brought over from the original image that also happened to be black were converted to transparent. . That&#39;s why I decided to refactor into the method above. . However, I felt like I should leave it in anyways, as it still has some potentially useful code in it. For example, in the case when the image cannot be converted to four channels beforehand. . bg = np.zeros(im.shape) bg.shape . (800, 600, 3) . mask = np.stack([hobbes_mask, hobbes_mask, hobbes_mask], axis=2) . foreground = np.where(mask, im, bg).astype(np.uint8) . # i.e. add the alpha channel and convert black pixels to alpha tmp = cv2.cvtColor(foreground.astype(&quot;uint8&quot;), cv2.COLOR_BGR2GRAY) _, alpha = cv2.threshold(tmp, 0, 255, cv2.THRESH_BINARY) b, g, r = cv2.split(foreground.astype(&quot;uint8&quot;)) rgba = [b, g, r, alpha] dst2 = cv2.merge(rgba, 4) # Look at the result, if needed # cv2_imshow(dst2) . . . Image manipulation with Python . Now, this image can be saved (as a PNG to preserve the alpha channel/transparency) and simply overlayed onto another image. Or, even better, the image can be used directly (as it is now, in the form of an array), scaled, rotated, moved, then pasted overtop of the other image. . At first I was going to use Photoshop to overlay Hobbes and make him look like a super dangerous sharkitty. But then I remembered the goal of this post, and decided to do it programmatically with Python. . The primary library I&#39;ll be using to manipulate images is Pillow. . from PIL import Image # Use plt to display images import matplotlib.pyplot as plt %matplotlib inline . !wget https://raw.githubusercontent.com/tobias-fyi/assetstash/master/visual/images/img_seg_bin_mask/05_jaws_kinda.jpg -q -O 05_jaws_kinda.jpg . jaws_img = Image.open(&quot;05_jaws_kinda.jpg&quot;) # Dimensions of background image (600, 900) will be useful later print(jaws_img.size) plt.imshow(jaws_img) . (600, 900) . &lt;matplotlib.image.AxesImage at 0x7f97f3cd0748&gt; . . Rotation . I decided to rotate the image matrix directly using OpenCV, prior to loading it into Pillow. There is a .rotate method on Pillow&#39;s Image class as well, which could accomplish the same thing. Either way works — I just wanted to learn how to do it with OpenCV. . # Found here: https://stackoverflow.com/a/9042907/10589271 # There is another (potentially easier) way to do it with Pillow, using Image.rotate() def rotate_image(image, angle): image_center = tuple(np.array(image.shape[1::-1]) / 2) rot_mat = cv2.getRotationMatrix2D(image_center, angle, 1.0) result = cv2.warpAffine(image, rot_mat, image.shape[1::-1], flags=cv2.INTER_LINEAR) return result . fg_rotated = rotate_image(foreground, 45) cv2_imshow(fg_rotated) . Load into Pillow . Once the image is rotated, it needs to be cropped and scaled appropriately. I decided to use Pillow for these transformations. For some reason that I have not looked into yet, OpenCV images are in BGR (blue, green, red) format instead of the virtually universal RGB format. Thus, in order to load the image from cv2 into Pillow without reversing the blues and reds, the color first must be converted to RGB. . Once converted, it can simply be loaded into Pillow as a PIL.Image object, which contains a suite of useful methods for transformation and more. . # Convert color from BGRA to RGBA fg_rotated_fixed = cv2.cvtColor(fg_rotated, cv2.COLOR_BGRA2RGBA) # Load into PIL.Image from array in memory hobbes_rotated = Image.fromarray(fg_rotated_fixed) plt.imshow(hobbes_rotated) . &lt;matplotlib.image.AxesImage at 0x7f706946a470&gt; . Crop . I manually defined the coordinates of the box used to crop the image by eyeballing it (shout out to Matplotlib for including tick marks on rendered images). A more automated method for doing this would be to extract the bounding box coordinates from the model output, and use that to crop the image before doing any other transformations. I will add this to a later iteration of this tutorial (if you&#39;re reading this, it likely hasn&#39;t been implemented yet). . box = (0, 80, 480, 500) crop = hobbes_rotated.crop(box2) print(crop.size) plt.imshow(crop) . (480, 420) . &lt;matplotlib.image.AxesImage at 0x7f70693ac208&gt; . Resize . Next, the cropped and rotated image must be resized in order to fill up the entire width of the background onto which it will be pasted. The reason for this is simply because the dimensions of the &quot;paste&quot; box must exactly match that of the &quot;copy&quot; box. It must be explicit — i.e. it&#39;s not like Photoshop where any of the image falling outside the boundaries of the &quot;canvas&quot; is cropped or simply left out. . width = jaws_img.size[0] scale = width / crop.size[0] # Calculate scale to match width height = int(scale * crop.size[1]) # Scale up height accordingly new_size = (width, height) # Resize! resized = crop.resize(new_size) print(resized.size) plt.imshow(resized) . (600, 525) . &lt;matplotlib.image.AxesImage at 0x7f7069ca30b8&gt; . Save! . Finally, with the rotated, cropped, and resized image of Hobbes all ready to go, we can paste it onto the background and save the result! . paws = jaws_img.copy() # Paste box dimensions have to exactly match the image being pasted paste_box = (0, paws.size[1] - resized.size[1], paws.size[0], paws.size[1]) paws.paste(resized, paste_box, mask=resized) plt.imshow(paws) . &lt;matplotlib.image.AxesImage at 0x7f706c1ae550&gt; . paws.save(&quot;06_paws.jpg&quot;) . . . . Final Thoughts . What a masterpiece! . Potential improvements . As with just about any process, there are always aspects that could be changed in an effort to improve efficiency or ease-of-use. I will keep adding to this list of potential improvements as I think or hear of them: . Fully automated the crop by using the bounding box from the model output | . Further Work . There are a couple of additional adjacent projects to this that I&#39;d like to work on at some point in the future, both to help others use the method outlined above, and to give me practice with various other aspects of the development lifecycle. . The two projects I have in mind are essentially the same thing, accomplished different ways. First, I&#39;d like to integrate this method into a Python package and put it on PyPI so it can be installed with Pip and easily customized/used from the command line or in other scripts. Second, I want to build a simple web app and API that would allow anyone to upload an image, choose a class of object to extract (or remove), run the model, then download the image with the background (and/or other objects) removed. . As I work on these, I&#39;ll put links here. .",
            "url": "https://tobias-fyi.github.io/fastfyi/blog/remove-bg-python",
            "relUrl": "/blog/remove-bg-python",
            "date": " • Jul 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Trash Panda",
            "content": ". Introduction . The Problem . You have an object in your hand that you intend to throw away. When you think about it as you&#39;re walking to the bins, you realize you actually don&#39;t know whether this type of object is recyclable or not. Maybe it is made of multiple different materials, or of an uncommon or unrecognizable material. . You&#39;re in the middle of an important project, and it&#39;s crunch time—no extra time available to spend researching. You end up throwing it in the recycling because it...well, it seems like something that would be recyclable. With the decision made and action taken, you return to your important project, forgetting all about what just transpired. . I&#39;d bet that most who are reading this have had an experience like this. . The priceless time and energy spent researching how to properly dispose of every single item can add up. However, the US is in something of a recycling crisis at the moment, partially due to the low quality of our recyclable waste—it tends to be very intermixed with non-recyclables. . Ever since China&#39;s National Sword legislation in 2017, which vastly reduced the amount of foreign recycling—particularly low-quality—the country would accept, recycling companies in the US have been forced to grapple with this quality issue. The cost of recycling increases when more trash is intermingled with it, as more sorting has to occur prior to processing. Whether it is more expensive machines or higher cost of labor, sorting costs money. . While the domestic recycling infrastructure will (hopefully) grow to meet the increasing demand, the best way to solve a problem is to address the source of the issue, not the symptoms. One key reason for the low quality recycling is simply a lack of easily accessible information. Even with the power of modern search engines at our fingertips, finding relevant recycling information can take a long time, as what exactly constitutes recycling changes depending on area and company. . The simple fact is that most people don&#39;t want to spend the additional time it takes (at least up front) to have good recycling habits. So why not simply remove that additional time from the equation? . . The Solution . The goal was to build an app that helps to foster better recycling habits by reducing the the effort needed to find accurate and relevant information on how to properly dispose of any given item of waste. To make this possible, we needed to reduce the friction so much so that looking up how to dispose of something that a user is holding in their hand is just as quick and easy as debating for a few moments on what bin it goes in. . Put another way, our goal was to reduce the cognitive tax of getting relevant recycling information so much that disposing of every item of waste properly, regardless of what it is, becomes effortless. . Our stakeholder envisioned that the user would simply snap a photo of something they are about to toss. Then, the app&#39;s computer vision (object detection) functionality would recognize the object and automatically pull up the relevant information on how it should be disposed of according to the user&#39;s location and available services. The user would know immediately if the item should be thrown in the trash, recycle, or compost, or if it is recyclable only at an offsite facility. For the latter case, the user would be able to view a list of nearby facilities that accept the item or material. . The result of this vision is a progressive web app (PWA) called Trash Panda, which does just that. You can try out the app on your mobile device now by following the link below. . The Trash Panda app (meant for mobile) . A note on PWAs . For those who aren&#39;t familiar, a PWA is basically a web app that can both be used via the browser and downloaded to the home screen of a mobile device. Google has been moving to fully support PWAs, meaning Trash Panda is available on the Play Store right now. Of course the benefit of a PWA is you don&#39;t actually have to download it at all if you don&#39;t want to. You can use it directly from the browser. . Apple is pretty far behind in their support of PWAs. As a result, the behavior on an iOS device is not ideal. For those on iOS, be sure to use Safari. And when taking a picture of an item, you have to exit out of the video window before pressing the normal shutter button. . You&#39;ll figure it out—we believe in you! . The Team (and My Role On It) . For eight weeks near the beginning of 2020, I worked with a remote interdisciplinary team to bring the vision of Trash Panda to life. . . Trash Panda is by far the most ambitious machine learning endeavor I had yet embarked on. Indeed, it was the largest software project I&#39;d worked on in just about every respect: time, team, ambition, breadth of required knowledge. As such, it provided to me many valuable, foundational experiences that I&#39;ll surely keep with me throughout my entire career. . I seriously lucked out on the team for this project. Every single one of them was hard-working, thoughtful, friendly—a pleasure to work with. The team included myself and three other machine learning engineers, four web developers, and two UX designers (links to all of their respective sites in the Final Thoughts section below). Our stakeholder Trevor Clack, who came up with the idea for the app and pitched it to Labs, also worked on the project as a machine learning engineer. . Trevor&#39;s Trash Panda blog post We all pushed ourselves throughout each and every day of the eight weeks to make Trevor&#39;s vision come to life, learning entirely new technologies, frameworks, skills, and processes along the way. . For example, the web developers taught themselves how to use GraphQL, along with a variety of related/dependent technology. On the machine learning side of things, none of us had significant applied experience with computer vision (CV) going into the project. We&#39;d spent a few days studying and working with it in the Deep Learning unit of our Lambda School curriculum. But that was more to expose us to it, rather than covering the entire process in-depth. We had only the shallowest of surface scratches compared to what was ultimately needed to meet the vision set out for us. . As the machine learning engineers on the team, we were responsible for the entire process of getting an object detection system built, trained, deployed, and integrated with the app. Basically, we were starting from scratch, both in the sense of a greenfield project and of us being inexperienced with CV. . Of course, CV is still machine learning—many steps in the process are similar to any other supervised machine learning project. But working with images comes with its own suite of unique challenges that we had to learn how to overcome. . We split up the work as evenly as possible, given our initially limited knowledge of the details, with some steps being split up between some or all of us, and other steps having a sole owner. . The first step for which I was solely responsible included building a system to automatically remove the background from images (or extract the foreground, depending on how you look at it). Essentially, when tasked with figuring out a way to automate the process of removing the background from images so they could be auto-labeled via a script written by Trevor, I built a secondary pipeline with a pre-trained image segmentation model. More details can be found in the Automated Background Removal section below. . Furthermore, I was responsible for building and deploying the object detection API. I built the API using Flask, containerized it with Docker, and deployed it to AWS Elastic Beanstalk. I go into a little more detail in the Deployment section below, though I will be digging into the code and process of the API much more in a separate blog post. . All members of the machine learning team contributed to the gathering and labeling of the dataset. To this end, each of us ended up gathering and labeling somewhere in the range of 20,000 images, for a total of over 82,000. . . The Data . As is the case with most, if not all, machine learning projects, we spent the vast majority of the time gathering and labeling our dataset. Before we could get into gathering the actual data, however, we needed to define what data we needed to gather and how we were going to gather it. . Classifications . As also seems to be the case with most, if not all, projects in general, we were almost constantly grappling with scope management. In an ideal world, our model would be able to recognize any object that anyone would ever want to throw away. But in reality is this is practically impossible, particularly within the 8 weeks we had to work on Trash Panda. I say &quot;practically&quot; because I&#39;m sure if a company dedicated enough resources to the problem, eventually it could be solved, at least to some degree. . Fortunately, we were granted an API key from Earth911 (shoutout to them for helping out!) to utilize their recycling center search database. At the time we were working with it, the database held information on around 300 items—how they should be recycled based on location, and facilities that accept them if they are not curbside recyclable. They added a number of items when we were already most of the way done with the project, and have likely added more since then. . We had our starting point for the list of items our system should be able to recognize. However, the documentation for the neural network architecture we&#39;d decided to use suggested that to create a robust model, it should be trained with at least 1,000 instances (in this case, images) of each of the classes we wanted it to detect. . Gathering 300,000 images was also quite a bit out of the scope of the project at that point. So, the DS team spent many hours reducing the size of that list to something a little more manageable and realistic. . The main method of doing so was to group the items based primarily on visual similarity. We knew it was also out of the scope of our time with the project to train a model that could tell the difference between #2 plastic bottles and #3 plastic bottles, or motor oil bottles and brake fluid bottles. . . Given enough time and resources, who knows? Maybe we could train a model that accurately recognizes 300+ items and distinguishes between similar-looking items. But we had to keep our scope realistic to be sure that we actually finished something useful in the time we had. . We also considered the items that 1) users would be throwing away on a somewhat regular basis, and 2) users would usually either be unsure of how to dispose of properly or would dispose of properly. More accuracy on the important and/or common items would be more valuable to users. Some items were not grouped. . By the end of this process, we managed to cluster and prune the original list of about 300 items and materials down to 73. . Image Data Pipelines . We figured that in order to get through gathering and labeling 70,000+ images with only four people, within our timeframe, and without any budget whatsoever, we had to get creative and automate as much of the process as possible. . As explained below, the image gathering step required significant manual labor. However, we had a plan in place for automating most of the rest of the process. Below is a general outline of the image processing and labeling pipeline we built. . Rename the images to their md5sum hash to avoid duplicates and ensure unique filenames | Resize the images to save storage (and processing power later on) | Discern between transparent background and non-transparent background | If image is non-transparent, remove the background | Automatically draw bounding boxes around the object (the foreground) | If image is transparent, add a background | The beauty of automation is once the initial infrastructure is built and working, the volume can be scaled up indefinitely. The auto-labeling functionality was not perfect by any means. But it still felt great watching this pipeline rip through large batches of images. . Gather . The first part of the overall pipeline was gathering the images—around 1,000 for each of the 73 classes. This was a small pipeline in its own right, which unfortunately involved a fair bit of manual work. . Timothy built the piece of the image gathering pipeline that allowed us to at least automate some of it—the downloading part. Bing ended up being the most fruitful source of images for us. Before starting we expected to use Google Images, but pivoted when it turned out that Bing&#39;s API was much more friendly to scraping and/or programmatically downloading. . Timothy&#39;s blog post about the Trash Panda project can be found here: Games by Tim - Trash Panda. . We used his script, which in turn used a Python package called Bulk-Bing-Image-downloader, to gather the majority of images. (I say majority because we also used some images from Google&#39;s Open Images Dataset and COCO.) . The gathering process was a mixture of downloading and sifting. As we were pulling images straight from the internet, irrelevant images inevitably found their way into the search queries somehow. For each class, we went through a iterative (and somewhat tedious) loop until we had around 1,000 images of that class. The steps were simple, yet time-consuming: . Gather a batch of images (usually several hundred at a time) from Bing using a script written by Timothy | Skim through them, removing irrelevant and/or useless images | The latter was what made this pipeline difficult to fully automate, as we couldn&#39;t think of any good way of automatically vetting the images. (Though I did write a script to help me sift through images as quickly as possible, which will be the topic of a future blog post.) We had to be sure the images we were downloading in bulk actually depicted the class of item in question. . As they say, &quot;garbage in, garbage out.&quot; . And in case you weren&#39;t aware, the internet is full of garbage. . Annotate . To train an object detection model, each image in the training dataset must be annotated with rectangular bounding boxes (or, more accurately, the coordinates that define the bounding box) surrounding each of the objects belonging to a class that we want the model to recognize. These are used as the label, or target, for the model—i.e. what the model will be trying to predict. . Trevor came up with an idea to automate the labeling part of the process—arguably the most time-intensive part. Basically, the idea was to use images that feature items over transparent backgrounds. All of the major search engines allow an advanced search for transparent images. If the item is the only object in the image, it is relatively simple and straightforward to write a script that draws a bounding box around it. . If you&#39;d like some more detail on this, Trevor wrote a blog post about it. . automated bbox . Automated Background Removal . There was one big issue with this auto-labeling process. Finding a thousand unique images of a single class of object is already something of a task. And depending on the object, finding that many without backgrounds is virtually impossible. . For the images that had backgrounds, we would either have to manually label them, or find a way to automate the process and build it into the pipeline. Because the script to label images without backgrounds was already written and working, we decided to find a way of automatically removing the background from images. . This is the part of the pipeline that I built. . I&#39;ll give a brief overview here of how I built a system for automatically removing backgrounds from images. If you&#39;re curious about the details, I wrote a separate blog post on the topic: . Automated Image Background Removal with Python I tested out a few different methods of image background removal, with varying degrees of success. The highest quality results came from creating a batch task in Adobe Photoshop, as whatever algorithm powers its &quot;Select Foreground&quot; functionality is very consistent and accurate. However, we could not use this method because it could not be added to the pipeline—I was the only one with a license, meaning the speed of my computer and internet could cause a major bottleneck. . Another method I tested out was the OpenCV implementation of an algorithm called GrabCut. Long story short, I wasn&#39;t able to get the quality we needed from it, as can be seen below. . . The main issue is that the algorithm is &quot;interactive&quot;. That is, it uses the coordinates of a rectangle surrounding the object in the image for which the foreground should be extracted. For best results, the coordinates are generated manually for each image. The tighter that rectangle surrounds the foreground, the better the outline will be. . The above image is my attempt at simply using the entire image as the rectangle. As can be seen below, the result was much better when I tightened the rectangle around the can. I tried for many hours to find an automatable solution to this, but could not get results that were quite good enough. I decided to move onto other strategies. . . Ultimately, I ended up building a short image processing pipeline that utilized a pre-trained image segmentation model (similar to object detection) to find the object(s) in the image. I initially built the pipeline with a library called Detectron2, based on PyTorch. However, after running into some issues with it, I decided to reimplement the pipeline using Mask R-CNN, based on TensorFlow. . . Part of the output of the image segmentation model is a series of coordinates that describe an outline of the object(s) in the image. I used that as a binary mask to define the area of the image that should be kept, making the rest of it transparent. . . Unfortunately, I did not have much time to spend on improving the performance of the image segmentation model, and as a result there was still a fair amount of manual labeling to be done after the pipeline. I could (and should) have trained the image segmentation model using a small subset of images from each class. This would&#39;ve made the output mask much more accurate and reduced the time spent fixing the labels afterwards. . As it was, using only the pretrained weights, there were some object classes that it performed very well on, while for others it did not. . Running the Pipeline . As with building the pipeline, we split up the 73 classes evenly amongst the four of us (around 18 classes each) and got to work gathering and labeling the images. . If we&#39;d had a couple more weeks to spend improving the pipeline, this process likely would not have taken so long or been so tedious. As it was, we spent the better part of 4 weeks gathering and labeling the dataset. . I believe the pipeline did save us enough time to make it worth the extra time it took to build. Plus, I got to learn and apply some cool new tech! . . The Model . Architecture . The neural network architecture we used to train the main object detection system used in the app is called YOLOv3: You Only Look Once, version 3. . . YOLOv3 is a state-of-the-art single-shot object detection system originally written in C. One of the main reasons we used this one in particular was its speed—a benefit of single-shot algorithms. With the YOLO algorithms, object detection can be run in real time. For example, it could be run on a live security camera feed, detecting when someone enters a property. . Training . The vast majority of the work we put into this model was building a high-quality dataset. We used transfer learning to give the model an initial training kickstart. We were able to benefit from previous training done by the algorithm&#39;s developers by loading the weights from convolutional layers that were trained on ImageNet. . Our model was trained on a GPU-enabled AWS Sagemaker instance. After about 60 hours, our model reached a total average precision of 54.71%. . As expected, the model performs much better on certain classes of objects. The more easily recognizable classes (tires, printers, disks, digital cameras, plastic bottles) had average precisions in the 80-90% range. . On the other hand, the lower precision object classes were usually those that took on a wider variety of shapes, textures, and colors. For example, musical instruments, food waste, office supplies. It makes sense that, given a similar amount of training data, classes like this would be more difficult to distinguish. . Here is the breakdown for the 13,000 weights - mAP (mean average precision): . detections_count = 82369, unique_truth_count = 10934 class_id = 0, name = aerosol_cans, ap = 54.87% (TP = 54, FP = 45) class_id = 1, name = aluminium_foil, ap = 42.11% (TP = 32, FP = 22) class_id = 2, name = ammunition, ap = 55.38% (TP = 61, FP = 49) class_id = 3, name = auto_parts, ap = 41.70% (TP = 31, FP = 21) class_id = 4, name = batteries, ap = 62.19% (TP = 92, FP = 44) class_id = 5, name = bicycles, ap = 79.86% (TP = 86, FP = 22) class_id = 6, name = cables, ap = 64.81% (TP = 76, FP = 40) class_id = 7, name = cardboard, ap = 52.99% (TP = 50, FP = 41) class_id = 8, name = cartridge, ap = 70.16% (TP = 68, FP = 25) class_id = 9, name = cassette, ap = 53.45% (TP = 13, FP = 10) class_id = 10, name = cd_cases, ap = 80.11% (TP = 30, FP = 3) class_id = 11, name = cigarettes, ap = 29.43% (TP = 38, FP = 56) class_id = 12, name = cooking_oil, ap = 69.23% (TP = 61, FP = 25) class_id = 13, name = cookware, ap = 70.76% (TP = 81, FP = 83) class_id = 14, name = corks, ap = 57.99% (TP = 55, FP = 32) class_id = 15, name = crayons, ap = 52.43% (TP = 44, FP = 25) class_id = 16, name = desktop_computers, ap = 75.17% (TP = 34, FP = 12) class_id = 17, name = digital_cameras, ap = 93.40% (TP = 120, FP = 15) class_id = 18, name = disks, ap = 90.14% (TP = 90, FP = 25) class_id = 19, name = doors, ap = 0.00% (TP = 0, FP = 0) class_id = 20, name = electronic_waste, ap = 73.97% (TP = 50, FP = 16) class_id = 21, name = eyeglasses, ap = 21.75% (TP = 24, FP = 37) class_id = 22, name = fabrics, ap = 52.17% (TP = 69, FP = 52) class_id = 23, name = fire_extinguishers, ap = 30.32% (TP = 22, FP = 30) class_id = 24, name = floppy_disks, ap = 78.26% (TP = 83, FP = 53) class_id = 25, name = food_waste, ap = 19.16% (TP = 28, FP = 22) class_id = 26, name = furniture, ap = 3.45% (TP = 0, FP = 0) class_id = 27, name = game_consoles, ap = 58.90% (TP = 45, FP = 19) class_id = 28, name = gift_bags, ap = 65.97% (TP = 72, FP = 48) class_id = 29, name = glass, ap = 72.21% (TP = 149, FP = 49) class_id = 30, name = glass_container, ap = 0.00% (TP = 0, FP = 0) class_id = 31, name = green_waste, ap = 57.38% (TP = 61, FP = 55) class_id = 32, name = hardware, ap = 24.78% (TP = 28, FP = 62) class_id = 33, name = hazardous_fluid, ap = 79.06% (TP = 85, FP = 10) class_id = 34, name = heaters, ap = 71.18% (TP = 54, FP = 50) class_id = 35, name = home_electronics, ap = 32.91% (TP = 83, FP = 92) class_id = 36, name = laptop_computers, ap = 41.66% (TP = 95, FP = 55) class_id = 37, name = large_appliance, ap = 5.91% (TP = 7, FP = 38) class_id = 38, name = lightbulb, ap = 28.36% (TP = 61, FP = 28) class_id = 39, name = medication_containers, ap = 59.85% (TP = 88, FP = 46) class_id = 40, name = medications, ap = 55.37% (TP = 68, FP = 39) class_id = 41, name = metal_cans, ap = 52.21% (TP = 82, FP = 24) class_id = 42, name = mixed_paper, ap = 37.32% (TP = 64, FP = 43) class_id = 43, name = mobile_device, ap = 63.24% (TP = 151, FP = 97) class_id = 44, name = monitors, ap = 39.15% (TP = 75, FP = 77) class_id = 45, name = musical_instruments, ap = 20.67% (TP = 44, FP = 42) class_id = 46, name = nail_polish, ap = 84.06% (TP = 105, FP = 30) class_id = 47, name = office_supplies, ap = 7.01% (TP = 30, FP = 52) class_id = 48, name = paint, ap = 47.56% (TP = 54, FP = 25) class_id = 49, name = paper_cups, ap = 76.20% (TP = 85, FP = 32) class_id = 50, name = pet_waste, ap = 31.60% (TP = 5, FP = 1) class_id = 51, name = pizza_boxes, ap = 61.07% (TP = 69, FP = 38) class_id = 52, name = plastic_bags, ap = 31.45% (TP = 39, FP = 36) class_id = 53, name = plastic_bottles, ap = 77.49% (TP = 303, FP = 92) class_id = 54, name = plastic_caps, ap = 22.99% (TP = 5, FP = 1) class_id = 55, name = plastic_cards, ap = 57.51% (TP = 45, FP = 14) class_id = 56, name = plastic_clamshells, ap = 74.12% (TP = 36, FP = 16) class_id = 57, name = plastic_containers, ap = 71.93% (TP = 104, FP = 47) class_id = 58, name = power_tools, ap = 75.21% (TP = 77, FP = 58) class_id = 59, name = printers, ap = 88.06% (TP = 106, FP = 62) class_id = 60, name = propane_tanks, ap = 62.89% (TP = 36, FP = 15) class_id = 61, name = scrap_metal, ap = 0.00% (TP = 0, FP = 0) class_id = 62, name = shoes, ap = 83.53% (TP = 64, FP = 12) class_id = 63, name = small_appliances, ap = 80.61% (TP = 93, FP = 34) class_id = 64, name = smoke_detectors, ap = 79.73% (TP = 65, FP = 10) class_id = 65, name = sporting_goods, ap = 3.34% (TP = 0, FP = 0) class_id = 66, name = tires, ap = 83.31% (TP = 77, FP = 11) class_id = 67, name = tools, ap = 69.69% (TP = 113, FP = 37) class_id = 68, name = toothbrushes, ap = 2.92% (TP = 1, FP = 1) class_id = 69, name = toothpaste_tubes, ap = 59.78% (TP = 35, FP = 9) class_id = 70, name = toy, ap = 49.70% (TP = 47, FP = 24) class_id = 71, name = vehicles, ap = 4.51% (TP = 1, FP = 4) class_id = 72, name = water_filters, ap = 47.91% (TP = 36, FP = 21) class_id = 73, name = wood, ap = 76.15% (TP = 105, FP = 51) class_id = 74, name = wrapper, ap = 72.45% (TP = 67, FP = 18) for conf_thresh = 0.25, precision = 0.65, recall = 0.41, F1-score = 0.50 for conf_thresh = 0.25, TP = 4507, FP = 2430, FN = 6427, average IoU = 51.08 % IoU threshold = 50 %, used Area-Under-Curve for each unique Recall mean average precision (mAP@0.50) = 0.523220, or 52.32 % Total Detection Time: 564 Seconds Set -points flag: `-points 101` for MS COCO `-points 11` for PascalVOC 2007 (uncomment `difficult` in voc.data) `-points 0` (AUC) for ImageNet, PascalVOC 2010-2012, your custom dataset . . Deployment . In order to utilize our hard-earned trained weights for inference in the app, I built an object detection API with Flask and used Docker to deploy it to AWS Elastic Beanstalk. . The trained weights were loaded into a live network using OpenCV, which can then be used for inference (prediction; detecting objects). Once a user takes a photo in the app, it is encoded and sent to the detection API. The API decodes the image and passes it into the live network running with the trained weights. The network runs inference on the image and sends back the class of item with the highest predicted probability. . Again, I go into much greater detail on the API in a separate blog post (link to come, once it&#39;s published). . . Final Thoughts . Potential Improvements . Although we had eight weeks for this project, we were starting with a completely green field. Therefore, at least the first two weeks were dedicated to discussions and planning. . The result of scope management is features and ideas left unimplemented. At the beginning, the entire team brainstormed a lot about potential ideas for the app. And throughout the project, the machine learning team was always thinking and discussing potential improvements. Here is a brief summary of some of them. . The first and most obvious one is to simply expand the model by gathering and labeling an even wider variety and even greater numbers of images. By breaking out the grouped items into more specific classes and gathering more images, the model could be trained to distinguish between some of the visually similar (though materially different) objects. . Furthermore, there are some low-hanging fruit that would likely make the model more robust. One of those that we did not get to explore due to lack of time is image augmentation. In a nutshell, image augmentation is a method of artificially growing an image dataset by &quot;creating&quot; new images from the existing ones by transforming or otherwise manipulating them. . These augmentations could include changing the color, crop, skew, rotation, and even combining multiple images. The labels are also transformed to match the location and size of the transformed objects in each image. Not only does this process help create more training data, but it can also help the model extract more of the underlying features of the objects, allowing it to be more robust to a greater variety of lighting conditions and camera angles. . Another way to increase the size and variety of the dataset would be to add a feature that gathers the images (and labels) taken by the user. After every prediction that is served, the user could be presented with a box containing two buttons, one indicating that the prediction was correct, the other, incorrect. The correct ones could go straight to the server to wait until the next model training round. And if the user indicates that the predicted class was incorrect, they could be presented with an interface to draw a box around the object and choose the correct class. Then that image and label would be sent to the server to be used in training. . The last idea was one that the entire team discussed at one point, though is a feature that would likely only be possible with some kind of financial backing. The idea is to implement a system that recognizes when many of a similar type of object are detected—those need to be recycled at the same facility, for example—and can pay drivers to pick all of those items up and take them to the proper facilities. In other words, be the &quot;UBER for recycling&quot;. . Team Links . I said it before and I&#39;ll say it again: the team I worked with on this project was top-notch. Here they are in all of their glory, with all of their portfolios/blogs/GitHubs (those that have them) in case you want to check out their other work. . Machine learning . Tobias Reaper | Trevor Clack | Vera Mendes | Timothy Hsu | . Web development . Mark Halls | Mark Artishuk | Colin Bazzano | Carlo Lucido | . User Experience . Kendra McKernan | Lynn Baxter | . Other Links . I linked to this video above but figured I&#39;d include it here as well. It&#39;s the video that Trevor created to pitch the idea (originally called &#39;Recycle This?&#39;) to the Lambda Labs coordinators. Also included is another video he created to demo the final product. I was one of the presenters of our app for the final demos to both our cohort and to the entire school. Links to those videos are below. Here is a video (thanks to Trevor) showing the progress of the interface at various stages throughout the project. . As always, thanks for reading, and I&#39;ll see you in the next one! .",
            "url": "https://tobias-fyi.github.io/fastfyi/workshop/trash-panda",
            "relUrl": "/workshop/trash-panda",
            "date": " • Mar 8, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Post Here",
            "content": ". Introduction . The Problem . Reddit is an expansive site. Anyone who has spent any significant amount of time on it knows what I mean. There is a subreddit for seemingly every topic anyone could ever want to discuss or even think about (and many that most do not want think about). . Reddit is a powerful site; a tool for connecting and sharing information with like- or unlike-minded individuals around the world. When used well, it can be a very useful resource. . On the other hand, the deluge of information that&#39;s constantly piling into the pages of can be overwhelming and lead to wasted time. As with any tool, it can be used for good or for not-so-good. . A common problem that Redditors experience, particularly those who are relatively new to the site, is where to post content. Given that there are subreddits for just about everything, with wildly varying degrees of specificity it can be quite overwhelming trying to find the best place for each post. . Just to illustrate the point, some subreddits get weirdly specific. I won&#39;t go into the really weird or NSFW, but here are some good examples of what I mean by specific: . r/Borderporn | r/BreadStapledtoTrees | r/birdswitharms | r/totallynotrobots | . ...need I go on? (If you&#39;re curious and/or want to be entertained indefinitely, here is a thread with these and much, much more.) . Most of the time when a post is deemed irrelevant to a particular subreddit, it will simply be removed by moderators or a bot. However, depending on the subreddit and how welcoming they are to newbies, sometimes it can lead to very unfriendly responses and/or bans. . So how does one go about deciding where to post or pose a question? . Post Here aims to take the guesswork out of this process. . . The Solution . The goal with the Post Here app, as mentioned, is to provide a tool that makes it quick and easy to find the most appropriate subreddits for any given post. A user would simply provide the title and text of the their prospective post and the app would provide the user with a list of subreddit recommendations. . Recommendations are produced by a model attempts to predict which subreddit a given post would belong to. The model was built using Scikit-learn, and was trained on a large dataset of reddit posts. In order to serve the recommendations to the web app, an API was built using Flask and deployed to Heroku. . The live version of the app is linked below. . Post Here:The Subreddit Suggester . My Role . I worked on the Post Here app with a remote, interdisciplinary team of data scientists, machine learning engineers, and web developers. I was one of two machine learning engineers on the team, responsible for the entire process of building and training the machine learning models. The two data scientists on the team were primarily responsible for building and deploying the API. . The main challenge we ran into, which directed much of the iterative process, was scope management. . At this point in my machine learning journey, this was one of the larger datasets that I&#39;d taken on. Uncompressed, the dataset we used was over 800mb of mostly natural language text. The dataset and the time constraint—we had less than four full days of work to finish the project—were the primary causes of the challenges we ended up facing. . With such a dataset, one important concept we had to keep in mind was the curse of dimensionality, which is basically a title for the various problems and phenomena that occur when dealing with extremely highly dimensional datasets. When processed, a natural language dataset of this size would likely fall prey to this curse and may prove somewhat unwieldy without large amounts of processing power. . I ended up researching and applying various methods of addressing this problem in order to fit the processing/modeling pipeline on the free Heroku Dyno, with a memory limit of 500mb, while preserving adequate performance. Many of our deployments failed because the pipeline, when loaded into memory on the server, exceeded that limit. . One important tradeoff we had to wrangle with was how much, and in what ways we could limit the dataset—i.e. how many classes to try and predict, and how many observations per class to include when training. The original dataset contains data for 1,000 subreddits. It was not within the scope of a a four-day project to build a classification model of a caliber that could accurately classify 1,000 classes. . In the beginning, we did try to build a basic model trained on all 1,000 classes. But with the time and processing power I had, it proved to be untenable. In the end, we settled for a model that classified text into 305 subreddits with a test precision-at-k of .75, .88, and .92 for &#39;k&#39; of 1, 3, and 5, respectively. . Imports and Configuration . import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns import os import janitor . from sklearn.model_selection import train_test_split from sklearn.preprocessing import LabelEncoder from sklearn.ensemble import RandomForestClassifier from sklearn.naive_bayes import MultinomialNB from sklearn.feature_selection import chi2, SelectKBest # === NLP Imports === # from sklearn.feature_extraction.text import TfidfVectorizer . # Configure pandas display settings pd.options.display.max_colwidth = 100 # Set random seed seed = 92 . . The Data . The dataset we ended up using to train the recommendation system is called the Reddit Self-Post Classification Task dataset, available on Kaggle thanks to Evolution AI. The full dataset clocks in at over 800mb, containing 1,013,000 rows: 1,000 posts each from 1,013 subreddits. . For more details on the dataset, including a nice interactive plot of all of the subreddits, refer to Evolution AI&#39;s blog post. . Wrangling and Exploration . First, I needed to reduce the size of the dataset. I defined a subset of 12 categories which I thought were most relevant to the task at hand, and used that list to do the initial pruning. Those 12 categories left me with 305 unique subreddits and 305,000 rows. The list I used was as follows: . health | profession | electronics | hobby | writing/stories | advice/question | social_group | stem | parenting | books | finance/money | travel | . Next, I took a random sample of those 305,000 rows. The result was a dataset with 91,500 rows, now consisting of between 250 and 340 rows per subreddit. If I tried to use all of the features (tokens, or words) that resulted from this corpus, even in its reduced state, it would still result in a serialized vocabulary and/or model too large for our free Heroku Dyno. However, the features used in the final model can be chosen based on how useful they are for the classification. . According to the dataset preview on Kaggle, there are quite a large number of missing values in each of the features—12%, 25%, and 39% of the subreddit, title, and selftext columns, respectively. However, I did not find any sign of those null values in the dataset nor mention of them in the dataset&#39;s companion blog post or article. I chocked it up to an error in the Kaggle preview. . Finally, I went about doing some basic preprocessing to get the data ready for vectorization. As described in the description page on Kaggle, newline and tab characters were replaced with their HTML equivalents, &lt;lb&gt; and &lt;tab&gt;. I removed those and other HTML entities using a simple regular expression. I also concatenated title and selftext into a single text feature in order to process them together. . rspct = pd.read_csv(&quot;assets/data/rspct.tsv&quot;, sep=&quot; t&quot;) print(rspct.shape) rspct.head(3) . (1013000, 4) . id subreddit title selftext . 0 6d8knd | talesfromtechsupport | Remember your command line switches... | Hi there, &lt;lb&gt;The usual. Long time lerker, first time poster, be kind etc. Sorry if this isn&#39;t the right place...&lt;lb&gt;&lt;lb&gt;Alright. Here&#39;s the story. I&#39;m an independent developer who produces my ow... | . 1 58mbft | teenmom | So what was Matt &quot;addicted&quot; to? | Did he ever say what his addiction was or is he still chugging beers while talking about how sober he is?&lt;lb&gt;&lt;lb&gt;Edited to add: As an addict myself, anyone I know whose been an addict doesn&#39;t drin... | . 2 8f73s7 | Harley | No Club Colors | Funny story. I went to college in Las Vegas. This was before I knew anything about motorcycling whatsoever. Me and some college buddies would always go out on the strip to the dance clubs. We alwa... | . Nulls . Kaggle says that 12%, 25%, and 39% of the subreddit, title, and selftext columns are null, respectively. If that is indeed the case, they did not get read into the dataframe correctly. However, it could be an error on Kaggle&#39;s part, seeing as there is no mention of these anywhere else in the description or blog post or article, nor sign of them during my explorations. . rspct.isnull().sum() . id 0 subreddit 0 title 0 selftext 0 dtype: int64 . Preprocessing . To prune the list of subreddits, I&#39;ll load in the subreddit_info.csv file, join, then choose a certain number of categories (category_1) to filter on. . info = pd.read_csv(&quot;assets/data/subreddit_info.csv&quot;, usecols=[&quot;subreddit&quot;, &quot;category_1&quot;, &quot;category_2&quot;]) print(info.shape) info.head() . (3394, 3) . subreddit category_1 category_2 . 0 whatsthatbook | advice/question | book | . 1 CasualConversation | advice/question | broad | . 2 Clairvoyantreadings | advice/question | broad | . 3 DecidingToBeBetter | advice/question | broad | . 4 HelpMeFind | advice/question | broad | . rspct = pd.merge(rspct, info, on=&quot;subreddit&quot;).drop(columns=[&quot;id&quot;]) print(rspct.shape) rspct.head() . (1013000, 5) . subreddit title selftext category_1 category_2 . 0 talesfromtechsupport | Remember your command line switches... | Hi there, &lt;lb&gt;The usual. Long time lerker, first time poster, be kind etc. Sorry if this isn&#39;t the right place...&lt;lb&gt;&lt;lb&gt;Alright. Here&#39;s the story. I&#39;m an independent developer who produces my ow... | writing/stories | tech support | . 1 talesfromtechsupport | I work IT for a certain clothing company and they use iPod Touchs for scanning some items | [ME]- Thank you fro calling Store support, this is David. How may I help you?&lt;lb&gt;&lt;lb&gt;[Store]- Yeah, my iPod is frozen&lt;lb&gt;&lt;lb&gt;[ME]- Okay, can I have you hold down the power and the home button at t... | writing/stories | tech support | . 2 talesfromtechsupport | It... It says right there on the screen...? | Hi guys! &lt;lb&gt;&lt;lb&gt;&amp;amp;nbsp;&lt;lb&gt;&lt;lb&gt;LTL, FTP - all that jazz. Starting you off with a short one.&lt;lb&gt;&lt;lb&gt;&amp;amp;nbsp;&lt;lb&gt;&lt;lb&gt;I&#39;m the senior supporter at a smaller tech company with clients all over t... | writing/stories | tech support | . 3 talesfromtechsupport | The computers not working. FIX IT NOW! | Hey there TFTS! This is my second time posting. I don&#39;t work for any tech support company, but I do have friends, family and teachers at school that have no idea how stuff works.&lt;lb&gt;&lt;lb&gt;This tale ... | writing/stories | tech support | . 4 talesfromtechsupport | A Storm of Unreasonableness | Usual LTL, FTP. I have shared this story on a different site, but after reading TFTS for sometime I figured it&#39;d belong here as well. &lt;lb&gt;&lt;lb&gt;This is from when I worked at a 3rd party call center ... | writing/stories | tech support | . rspct.isnull().sum() # That&#39;s a good sign . subreddit 0 title 0 selftext 0 category_1 0 category_2 0 dtype: int64 . rspct[&quot;category_1&quot;].value_counts() . video_game 100000 tv_show 68000 health 58000 profession 56000 software 52000 electronics 51000 music 43000 sports 40000 sex/relationships 31000 hobby 30000 geo 29000 crypto 29000 company/website 28000 other 27000 anime/manga 26000 drugs 23000 writing/stories 22000 programming 21000 arts 21000 autos 20000 advice/question 18000 education 17000 animals 17000 politics/viewpoint 16000 social_group 16000 card_game 15000 food/drink 15000 stem 14000 hardware/tools 14000 parenting 13000 religion/supernatural 13000 books 12000 appearance 11000 finance/money 10000 board_game 9000 meta 9000 movies 7000 rpg 7000 travel 5000 Name: category_1, dtype: int64 . keep_cats = [ &quot;health&quot;, &quot;profession&quot;, &quot;electronics&quot;, &quot;hobby&quot;, &quot;writing/stories&quot;, &quot;advice/question&quot;, &quot;social_group&quot;, &quot;stem&quot;, &quot;parenting&quot;, &quot;books&quot;, &quot;finance/money&quot;, &quot;travel&quot;, ] # Prune dataset to above categories # Overwriting to save memory rspct = rspct[rspct[&quot;category_1&quot;].isin(keep_cats)] print(rspct.shape) print(&quot;Unique subreddits:&quot;, len(rspct[&quot;subreddit&quot;].unique())) rspct.head(2) . (305000, 5) Unique subreddits: 305 . subreddit title selftext category_1 category_2 . 0 talesfromtechsupport | Remember your command line switches... | Hi there, &lt;lb&gt;The usual. Long time lerker, first time poster, be kind etc. Sorry if this isn&#39;t the right place...&lt;lb&gt;&lt;lb&gt;Alright. Here&#39;s the story. I&#39;m an independent developer who produces my ow... | writing/stories | tech support | . 1 talesfromtechsupport | I work IT for a certain clothing company and they use iPod Touchs for scanning some items | [ME]- Thank you fro calling Store support, this is David. How may I help you?&lt;lb&gt;&lt;lb&gt;[Store]- Yeah, my iPod is frozen&lt;lb&gt;&lt;lb&gt;[ME]- Okay, can I have you hold down the power and the home button at t... | writing/stories | tech support | . rspct = rspct.sample(frac=.3, random_state=seed) print(rspct.shape) rspct.head() . (91500, 5) . subreddit title selftext category_1 category_2 . 594781 stepparents | Ex Wants Toddler Son (2M) to Meet Her AP/SO - x-post from /r/divorce | Quick background: My soon-to-be ex-wife (26F) and I (27M) have been separated for about 5 months now. She has been in a serious relationship her AP (23M) whom she met and cheated on me with 6 mont... | parenting | step parenting | . 617757 bigseo | Do we raise our pricing? | I took a management role at an agency. We&#39;re way, way under $500/mo for SEO pricing - and I&#39;m embarrassed to say that we&#39;re hurting for business. Seems to me that it&#39;s a struggle to get clients to... | profession | seo | . 642368 chemistry | Mac vs. PC? | Hello, all! I am currently a senior in high school and in the fall I will be going to SUNY Geneseo, majoring in chemistry and minoring in mathematics. &lt;lb&gt;&lt;lb&gt;Geneseo requires it’s students to get... | stem | chemistry | . 325221 migraine | Beer as an aural abortive? | Hiya folks,&lt;lb&gt;&lt;lb&gt;I&#39;ve been a migraine sufferer pretty much my whole life. For me intense auras, numbness, confusion, the inability to speak or see is BY FAR the worst aspect of the ordeal. When ... | health | migraine | . 524939 MouseReview | Recommend office mouse | I was hoping you folks could help me out. Here&#39;s my situation and requirements:&lt;lb&gt;&lt;lb&gt;* I don&#39;t play games at all&lt;lb&gt;* Budget $30.00 or less&lt;lb&gt;* Shape as close to old Microsoft Intellimouse Opti... | electronics | computer mouse | . # Concatenate title and selftext rspct[&quot;text&quot;] = rspct[&quot;title&quot;] + &quot; &quot; + rspct[&quot;selftext&quot;] # Drop categories rspct = rspct.drop(columns=[&quot;category_1&quot;, &quot;category_2&quot;, &quot;title&quot;, &quot;selftext&quot;]) . # NOTE: takes a couple minutes to run rspct[&quot;text&quot;] = rspct[&quot;text&quot;].str.replace(&quot;(&lt;lb&gt;)*|(&lt;tab&gt;)*|(&amp;amp;)*|(nbsp;)*&quot;, &quot;&quot;) rspct.head() . subreddit text . 594781 stepparents | Ex Wants Toddler Son (2M) to Meet Her AP/SO - x-post from /r/divorce Quick background: My soon-to-be ex-wife (26F) and I (27M) have been separated for about 5 months now. She has been in a serious... | . 617757 bigseo | Do we raise our pricing? I took a management role at an agency. We&#39;re way, way under $500/mo for SEO pricing - and I&#39;m embarrassed to say that we&#39;re hurting for business. Seems to me that it&#39;s a s... | . 642368 chemistry | Mac vs. PC? Hello, all! I am currently a senior in high school and in the fall I will be going to SUNY Geneseo, majoring in chemistry and minoring in mathematics. Geneseo requires it’s students to... | . 325221 migraine | Beer as an aural abortive? Hiya folks,I&#39;ve been a migraine sufferer pretty much my whole life. For me intense auras, numbness, confusion, the inability to speak or see is BY FAR the worst aspect o... | . 524939 MouseReview | Recommend office mouse I was hoping you folks could help me out. Here&#39;s my situation and requirements:* I don&#39;t play games at all* Budget $30.00 or less* Shape as close to old Microsoft Intellimou... | . subreddits = rspct[&quot;subreddit&quot;].unique() print(len(subreddits)) subreddits[:50] . 305 . array([&#39;stepparents&#39;, &#39;bigseo&#39;, &#39;chemistry&#39;, &#39;migraine&#39;, &#39;MouseReview&#39;, &#39;Malazan&#39;, &#39;Standup&#39;, &#39;preppers&#39;, &#39;Invisalign&#39;, &#39;whatsthisplant&#39;, &#39;CrohnsDisease&#39;, &#39;KingkillerChronicle&#39;, &#39;OccupationalTherapy&#39;, &#39;churning&#39;, &#39;Libraries&#39;, &#39;acting&#39;, &#39;eczema&#39;, &#39;Allergies&#39;, &#39;bigboobproblems&#39;, &#39;AskAnthropology&#39;, &#39;psychotherapy&#39;, &#39;WayfarersPub&#39;, &#39;synthesizers&#39;, &#39;StopGaming&#39;, &#39;stopsmoking&#39;, &#39;eroticauthors&#39;, &#39;amazonecho&#39;, &#39;TalesFromThePizzaGuy&#39;, &#39;rheumatoid&#39;, &#39;homestead&#39;, &#39;VoiceActing&#39;, &#39;FinancialCareers&#39;, &#39;Sleepparalysis&#39;, &#39;ProtectAndServe&#39;, &#39;short&#39;, &#39;Fibromyalgia&#39;, &#39;teaching&#39;, &#39;PlasticSurgery&#39;, &#39;insomnia&#39;, &#39;PLC&#39;, &#39;rapecounseling&#39;, &#39;peacecorps&#39;, &#39;paintball&#39;, &#39;autism&#39;, &#39;Nanny&#39;, &#39;Plumbing&#39;, &#39;Epilepsy&#39;, &#39;asmr&#39;, &#39;fatpeoplestories&#39;, &#39;Magic&#39;], dtype=object) . rspct[&quot;subreddit&quot;].value_counts() . Dreams 340 Gifts 337 HFY 333 Cubers 333 cassetteculture 333 ... foreignservice 265 WritingPrompts 263 immigration 263 TryingForABaby 262 Physics 250 Name: subreddit, Length: 305, dtype: int64 . . Modeling . Label Encoding . # This process naively transforms each class of the target into a number le = LabelEncoder() # Instantiate a new encoder instance le.fit(y_train) # Fit it on training label data # Transform both using the trained instance y_train = le.transform(y_train) y_val = le.transform(y_val) y_test = le.transform(y_test) y_train[:8] . array([ 92, 140, 65, 90, 278, 65, 272, 212]) . Vectorization . A vectorizer is used to extract numerical features (information) from a corpus of natural language text. I used a bag-of-words method of vectorization, which for the most part, disregards grammar. . The output of this vectorizer is a document-term matrix, with the documents (observations, or rows) on one axes and the terms (words, bigrams) on the other. This matrix can be thought of as a sort of vocabulary, or text-number translator. . At first, the &quot;vocabulary&quot; derived from the corpus using the vectorizer was the largest object when serialized. Luckily, there are many options and parameters available to reduce its size, most of which are simply different methods for reducing the number of features (terms) it contains. . One option is to put a hard limit of 100,000 on the number of features in the vocabulary. This is a simple, naive limit on the generated features, and thus, the resulting vocabulary size. . I decided to remove stopwords before vectorization in hopes that this would reduce the size of the vector vocabulary. To my initial surprise, removing the stop words (using NLTK&#39;s list) actually increased the size of the serialized vocab from 59mb to 76mb. . After some consideration, I found this to be a reasonable result. I figured that many of the stop words are short (&quot;I&quot;, &quot;me&quot;, &quot;my&quot;, etc.), and their removal caused the average length of words (and therefore bigrams as well) in the vocab to increase. While this may not account for the entirety of the difference, this provides some intuition as to why there is a difference. . Although the vocab without stop words was larger, I ended up using it anyways because it provided an extra ~0.01 in the precision-at-k score of the final model. . lengths = [] three_or_below = [] for word in stop_words: lengths.append(len(word)) if len(word) &lt;= 4: three_or_below.append(len(word)) print(f&quot;There are {len(stop_words)} stop words in the list.&quot;) print(f&quot;{len(three_or_below)} are 4 chars long or shorter.&quot;) print(f&quot;Average length is: {np.mean(lengths)}.&quot;) . There are 179 stop words in the list. 109 are 4 chars long or shorter. Average length is: 4.229050279329609. . tfidf = TfidfVectorizer( max_features=100000, min_df=10, ngram_range=(1,2), stop_words=stop_words, # Use nltk&#39;s stop words ) # Fit the vectorizer on the feature column to create vocab (doc-term matrix) vocab = tfidf.fit(X_train) # Get sparse document-term matrices X_train_sparse = vocab.transform(X_train) X_val_sparse = vocab.transform(X_val) X_test_sparse = vocab.transform(X_test) X_train_sparse.shape, X_val_sparse.shape, X_test_sparse.shape . ((65880, 63588), (7320, 63588), (18300, 63588)) . Feature Selection . As mentioned previously, the size of the corpus means the dimensionality of the featureset after vectorization will be very high. I passed in 100,000 as the maximum number of features to the vectorizer, limiting the initial size of the vocab. However, the features would have to be reduced more before training the model, as it is generally not good practice to have a larger number of features (100,000) than observations (91,500). . To reduce it down from that 100,000, I used a process called select k best, which does exactly what it sounds like: selects a certain number of the best features. The key aspect of this process is how to measure the value of the features; how to find which ones are the &quot;best&quot;. The scoring function I used in this case is called ch2 (chi-squared). . This function calculates chi-squared statistics between each feature and the target, measuring the dependence, or correlation, between them. The intuition here is that features which are more correlated with the target are more likely to be useful to the model. . I played around with some different values for the maximum number of features to be selected. Ultimately, I was once again limited by the size of the free Heroku Dyno, and settled on 20,000. This allowed the deployment to go smoothly while retaining enough information for the model to have adequate performance. . selector = SelectKBest(chi2, k=20000) selector.fit(X_train_sparse, y_train) X_train_select = selector.transform(X_train_sparse) X_val_select = selector.transform(X_val_sparse) X_test_select = selector.transform(X_test_sparse) X_train_select.shape, X_val_select.shape, X_test_select.shape . ((65880, 20000), (7320, 20000), (18300, 20000)) . Model validation . In this case, the model has a target that it is attempting to predict—a supervised problem. Therefore, the performance can be measured on validation and test sets. . To test out the recommendations I copied some posts and put them through the prediction pipeline to see what kinds of subreddits were getting recommended. For the most part, the predictions were decent. . The cases where the recommendations were a little less than ideal happened when I pulled example posts from subreddits that were not in the training data. The model generally did a good job recommending similar subreddits. . Baseline . For the baseline model, I decided to go with a basic random forest. This choice was somewhat arbitrary, though I was curious to see how a random forest would do with such a high target cardinality (number of classes/categories). . The baseline precision-at-k metrics for the random forest on the validation set were .54, .63, and .65, for k of 1, 3, and 5, respectively. . def precision_at_k(y_true, y_pred, k=5): y_true = np.array(y_true) y_pred = np.array(y_pred) y_pred = np.argsort(y_pred, axis=1) y_pred = y_pred[:, ::-1][:, :k] arr = [y in s for y, s in zip(y_true, y_pred)] return np.mean(arr) . rfc = RandomForestClassifier(max_depth=32, n_jobs=-1, n_estimators=200) rfc.fit(X_train_select, y_train) . RandomForestClassifier(max_depth=32, n_estimators=200, n_jobs=-1) . y_pred_proba_rfc = rfc.predict_proba(X_val_select) # For each prediction, find the index with the highest probability y_pred_rfc = np.argmax(y_pred_proba_rfc, axis=1) y_pred_rfc[:10] . array([296, 139, 177, 78, 12, 177, 161, 216, 40, 31]) . print(&quot;Validation scores:&quot;) print(&quot; precision@1 =&quot;, np.mean(y_val == y_pred_rfc)) print(&quot; precision@3 =&quot;, precision_at_k(y_val, y_pred_proba_rfc, 3)) print(&quot; precision@5 =&quot;, precision_at_k(y_val, y_pred_proba_rfc, 5)) . Validation scores: precision@1 = 0.5368852459016393 precision@3 = 0.6282786885245901 precision@5 = 0.6502732240437158 . Multinomial Naive Bayes . Multinomial naive Bayes is a probabilistic learning method for multinomially distributed data, and one of two classic naive Bayes algorithms used for text classification. I decided to iterate with this algorithm because it is meant for text classification tasks. . The precision-at-k metrics for the final Multinomial naive Bayes model on the validation set were .76, .88, and .9188, for k of 1, 3, and 5, respectively. Performance on the test set was nearly identical: .75, .88, and .9159. . nb = MultinomialNB(alpha=0.1) nb.fit(X_train_select, y_train) . MultinomialNB(alpha=0.1) . Evaluate on validation set . y_pred_proba_val = nb.predict_proba(X_val_select) # For each prediction, find index with highest probability y_pred_val = np.argmax(y_pred_proba_val, axis=1) y_pred_val[:10] . array([274, 139, 57, 78, 12, 17, 151, 216, 40, 171]) . print(&quot;Validation scores:&quot;) print(&quot; precision@1 =&quot;, np.mean(y_val == y_pred_val)) print(&quot; precision@3 =&quot;, precision_at_k(y_val, y_pred_proba_val, 3)) print(&quot; precision@5 =&quot;, precision_at_k(y_val, y_pred_proba_val, 5)) . Validation scores: precision@1 = 0.7599726775956284 precision@3 = 0.8834699453551913 precision@5 = 0.9188524590163935 . Evaluate on test set . y_pred_proba_test = nb.predict_proba(X_test_select) # For each prediction, find index with highest probability y_pred_test = np.argmax(y_pred_proba_test, axis=1) y_pred_test[:10] . array([ 97, 199, 116, 249, 43, 203, 263, 275, 96, 27]) . print(&quot;Test scores:&quot;) print(&quot; precision@1 =&quot;, np.mean(y_test == y_pred_test)) print(&quot; precision@3 =&quot;, precision_at_k(y_test, y_pred_proba_test, 3)) print(&quot; precision@5 =&quot;, precision_at_k(y_test, y_pred_proba_test, 5)) . Test scores: precision@1 = 0.7498360655737705 precision@3 = 0.8834426229508197 precision@5 = 0.9159562841530055 . Recommendations . The API should return a list of recommendations, not a single prediction. To accomplish this, I wrote a function that returns the top 5 most likely subreddits and their respective probabilities. . # The main functionality of the predict API endpoint def predict(title: str, submission_text: str, return_count: int = 5): &quot;&quot;&quot;Serve subreddit predictions. Parameters - title : string Title of post. submission_text : string Selftext that needs a home. return_count : integer The desired number of recommendations. Returns - Python dictionary formatted as follows: [{&#39;subreddit&#39;: &#39;PLC&#39;, &#39;proba&#39;: 0.014454}, ... {&#39;subreddit&#39;: &#39;Rowing&#39;, &#39;proba&#39;: 0.005206}] &quot;&quot;&quot; # Concatenate title and post text fulltext = str(title) + str(submission_text) # Vectorize the post -&gt; sparse doc-term matrix post_sparse = vocab.transform([fulltext]) # Feature selection post_select = selector.transform(post_sparse) # Generate predicted probabilities from trained model proba = nb.predict_proba(post_select) # Wrangle into correct format proba_dict = (pd .DataFrame(proba, columns=[le.classes_]) # Classes as column names .T # Transpose so column names become index .reset_index() # Pull out index into a column .rename(columns={&quot;level_0&quot;: &quot;name&quot;, 0: &quot;proba&quot;}) # Rename for aesthetics .sort_values(by=&quot;proba&quot;, ascending=False) # Sort by probability .iloc[:return_count] # n-top predictions to serve .to_dict(orient=&quot;records&quot;) ) proba_json = {&quot;predictions&quot;: proba_dict} return proba_json . title_science = &quot;&quot;&quot;Is there an evolutionary benefit to eating spicy food that lead to consumption across numerous cultures throughout history? Or do humans just like the sensation?&quot;&quot;&quot; post_science = &quot;&quot;&quot;I love spicy food and have done ever since I tried it. By spicy I mean HOT, like chilli peppers (we say spicy in England, I don&#39;t mean to state the obvious I&#39;m just not sure if that&#39;s a global term and I&#39;ve assumed too much before). I love a vast array of spicy foods from all around the world. I was just wondering if there was some evolutionary basis as to why spicy food managed to become some widely consumed historically. Though there seem to It way well be that we just like a tingly mouth, the simple things in life.&quot;&quot;&quot; science_recs = predict(title_science, post_science) science_recs . {&#39;predictions&#39;: [{&#39;name&#39;: &#39;GERD&#39;, &#39;proba&#39;: 0.009900622287634142}, {&#39;name&#39;: &#39;Allergies&#39;, &#39;proba&#39;: 0.009287774623361566}, {&#39;name&#39;: &#39;ibs&#39;, &#39;proba&#39;: 0.009150308633162811}, {&#39;name&#39;: &#39;AskAnthropology&#39;, &#39;proba&#39;: 0.009028660140513678}, {&#39;name&#39;: &#39;fatpeoplestories&#39;, &#39;proba&#39;: 0.00851982441049019}]} . title_pc = &quot;&quot;&quot;Looking for help with a build&quot;&quot;&quot; post_pc = &quot;&quot;&quot;I posted my wants for my build about 2 months ago. Ordered them and when I went to build it I was soooooo lost. It took 3 days to put things together because I was afraid I would break something when I finally got the parts together it wouldn’t start, I was so defeated. With virtually replacing everything yesterday it finally booted and I couldn’t be more excited!&quot;&quot;&quot; post_pc_recs = predict(title_pc, post_pc, 10) post_pc_recs . {&#39;predictions&#39;: [{&#39;name&#39;: &#39;lego&#39;, &#39;proba&#39;: 0.008418484170536294}, {&#39;name&#39;: &#39;rccars&#39;, &#39;proba&#39;: 0.008112076951648648}, {&#39;name&#39;: &#39;MechanicalKeyboards&#39;, &#39;proba&#39;: 0.0078335440606017}, {&#39;name&#39;: &#39;fightsticks&#39;, &#39;proba&#39;: 0.007633958584830632}, {&#39;name&#39;: &#39;Luthier&#39;, &#39;proba&#39;: 0.00716176615193545}, {&#39;name&#39;: &#39;modeltrains&#39;, &#39;proba&#39;: 0.007088134228361153}, {&#39;name&#39;: &#39;cade&#39;, &#39;proba&#39;: 0.007058109839673285}, {&#39;name&#39;: &#39;vandwellers&#39;, &#39;proba&#39;: 0.006700262151491209}, {&#39;name&#39;: &#39;cosplay&#39;, &#39;proba&#39;: 0.006536648725434882}, {&#39;name&#39;: &#39;homestead&#39;, &#39;proba&#39;: 0.006166832450007183}]} . post_title = &quot;&quot;&quot;What to do about java vs javascript&quot;&quot;&quot; post = &quot;&quot;&quot;I am a new grad looking for a job and currently in the process with a company for a junior backend engineer role. I was under the impression that the position was Javascript but instead it is actually Java. My general programming and &quot;leet code&quot; skills are pretty good, but my understanding of Java is pretty shallow. How can I use the next three days to best improve my general Java knowledge? Most resources on the web seem to be targeting complete beginners. Maybe a book I can skim through in the next few days? Edit: A lot of people are saying &quot;the company is a sinking ship don&#39;t even go to the interview&quot;. I just want to add that the position was always for a &quot;junior backend engineer&quot;. This company uses multiple languages and the recruiter just told me the incorrect language for the specific team I&#39;m interviewing for. I&#39;m sure they&#39;re mainly interested in seeing my understanding of good backend principles and software design, it&#39;s not a senior lead Java position.&quot;&quot;&quot; # === Test out the function === # post_pred = predict(post_title, post) # Default is 5 results post_pred . {&#39;predictions&#39;: [{&#39;name&#39;: &#39;cscareerquestions&#39;, &#39;proba&#39;: 0.516989539243874}, {&#39;name&#39;: &#39;devops&#39;, &#39;proba&#39;: 0.031462691062989795}, {&#39;name&#39;: &#39;interviews&#39;, &#39;proba&#39;: 0.02846504725703069}, {&#39;name&#39;: &#39;datascience&#39;, &#39;proba&#39;: 0.024227300545057697}, {&#39;name&#39;: &#39;bioinformatics&#39;, &#39;proba&#39;: 0.017516176338177075}]} . title_book = &quot;Looking for books with great plot twists&quot; # This one comes from r/suggestmeabook post2 = &quot;&quot;&quot;I&#39;ve been dreaming about writing my own stort story for a while but I want to give it an unexpected ending. I&#39;ve read lots of books, but none of them had the plot twist I want. I want to read books with the best plot twists, so that I can analyze what makes a good plot twist and write my own story based on that points. I don&#39;t like romance novels and I mostly enjoy sci-fi or historical books but anything beside romance novels would work for me, it doesn&#39;t have to be my type of novel. I&#39;m open to experience after all. I need your help guys. Thanks in advance.&quot;&quot;&quot; # === This time with 10 results === # post2_pred = predict(title_book, post2, 10) post2_pred . {&#39;predictions&#39;: [{&#39;name&#39;: &#39;suggestmeabook&#39;, &#39;proba&#39;: 0.4070015062748489}, {&#39;name&#39;: &#39;writing&#39;, &#39;proba&#39;: 0.14985778378113648}, {&#39;name&#39;: &#39;eroticauthors&#39;, &#39;proba&#39;: 0.07159411817054702}, {&#39;name&#39;: &#39;whatsthatbook&#39;, &#39;proba&#39;: 0.06062653422250441}, {&#39;name&#39;: &#39;ComicBookCollabs&#39;, &#39;proba&#39;: 0.027277418056905547}, {&#39;name&#39;: &#39;Malazan&#39;, &#39;proba&#39;: 0.019514923212723943}, {&#39;name&#39;: &#39;TheDarkTower&#39;, &#39;proba&#39;: 0.017162701613834493}, {&#39;name&#39;: &#39;DestructiveReaders&#39;, &#39;proba&#39;: 0.0151031907793204}, {&#39;name&#39;: &#39;WoT&#39;, &#39;proba&#39;: 0.011165890302931272}, {&#39;name&#39;: &#39;readyplayerone&#39;, &#39;proba&#39;: 0.007566597361383115}]} . Model deployment . As mentioned, the model, vocab, and feature selector were all serialized using Python&#39;s pickle module. In the Flask app, the pickled objects are loaded and ready for use, just like that. . I will go over the details of how the Flask app was set up in a separate blog post. . . Final Thoughts . For me, the most important and valuable aspects of this project were mainly surrounding the challenge of scope management. I constantly had to ask myself, &quot;What is the best version of this I can create given our limitations?&quot; . At first, I thought it would be feasible to predict all of the 1,000+ subreddits in the data, and wasted hours of valuable time attempting to do so. While I had tested various strategies of reducing the complexity of the model, the performance was rather terrible when it was trained on 100 or less examples of each of the complete list of subreddits. . The data scientist who I primarily worked with (we had one data scientist in addition to him and one other machine learning engineer, both of whom did not contribute significantly to the project) kept telling me that I should try reducing the number of classes first, allowing for more examples of each class and fewer classes for the model to predict. . Ultimately, this is the strategy that worked best, and I wasted valuable time by not listening to him the first few times he recommended that strategy. Good teamwork requires the members being humble and listening, something that I have taken to heart since the conclusion of this project. . Scope Management, Revisited . As time was very short while building this initial recommendation API, there are many things that we wished we could have done but simply did not have the time. Here are a few of the more obvious improvements that could be made. . The first, and most obvious one, is to simply deploy to a more powerful server, such as one hosted on AWS Elastic Beanstalk or EC2. This way, we could use the entire dataset to train an optimal model without worrying (as much) about memory limits. . Second, I could use a Scikit-learn pipeline to validate and tune hyperparameters using cross-validation, instead of a separate validation set. Also, this pipeline could be serialized as a single large object, rather than as separate pieces (encoder, vectorizer, feature selector, and classifier). As a final note for this particular train of thought, Joblib could potentially provide more efficient serialization than the Pickle module, allowing a more complex pipeline to be deployed on the same server. . Third, a model could&#39;ve been trained to classify the input post first into a broad category. Then, some sort of model could be used to to classify into a specific subreddit within that broad category. I&#39;m not sure about the feasibility of the second part of this idea, but thought it could be an interesting one to explore. . Lastly, different classes and calibers of models could have been tested for use in the various steps in the pipeline. In this case, I&#39;m referring primarily to using deep learning/neural networks. For example, word vectors could be generated with word embedding models such as Word2Vec. Or the process could be recreated with a library like PyTorch, and a framework like FastText. . I plan to explore at least some of these in separate blog posts. . As always, thank you for reading! I&#39;ll see you in the next one. .",
            "url": "https://tobias-fyi.github.io/fastfyi/workshop/post-here",
            "relUrl": "/workshop/post-here",
            "date": " • Jan 12, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "print(fiction)",
            "content": ". Introduction . print(fiction) is a solo project I worked on to explore the data on and around fictional stories. . I used Scrapy to scrape metadata for over 20,000 books from GoodReads and used it to train a series of machine learning classifiers. The final version of the model classified books as either fiction or nonfiction with ~80% accuracy. . The dataset is freely available for download on GitHub. . I built an interactive dashboard using Plotly Dash that can be used to tinker with the model parameters and view the resulting prediction in real time. . You can find the current live version of the app here: . print(fiction) . This project is part of an on-going series of exploratory articles and projects called Sci-Fi IRL, through which I am exploring the relationship between science-fiction and the real world. It is my belief that the stories we read, write, and believe in, particularly about our future, have an effect on how that future ultimately turns out. . Our human minds are geared toward thinking about what could go wrong. It follows that the majority of stories in popular and niche culture are written about how things could go wrong, usually catastrophically so—it &quot;tells a good story&quot;. . In the case of science-fiction, stories tend to be dystopian in nature, showing what could go wrong if our technology advances along certain trajectories. . But does this affect our outlook on what these technologies can do for us? . While it is always good to consider the possible ramifications of technological advances, I believe that too many dystopian stories are causing humans, as a civilization, to fall short of our potential. If instead of describing dystopia, the majority of science-fiction was utopian—exploring the possible ways that things could go right for us—I believe it would, in a very real sense, point us a little bit more in that direction. . If that&#39;s a bit too lofty for you, another way to think about this is to imagine what your life could be like 100 years from now (i.e. if you&#39;d been born 60 years from now). Depending on how things go, you could be scraping by with a group of other radiation-poisoned humans as the world recovers from nuclear holocaust. Or, you could be out exploring the galaxy in a luxury space yacht, with a potential lifespan in the centuries or millennia. . Which is more interesting to you? Which future would you rather live in? . This is the area I&#39;m exploring with this series. I want to find the data and conduct the analyses that begins to show how our collective narrative (aliased by popular science-fiction) can bring about changes in our technological progress. . Of course this area is too large to explore in a single project, which is why I am doing it as a series. The first article in the series explored, at a very basic level, how technical terminology disperses through popular culture. You can find that article here: Tech Term Velocity. . In this project, print(fiction) the broad question I wanted to explore was this: . What separates fact from fiction? . ...which is really just a cliché way of saying I wanted to explore the differences between nonfiction and fiction stories. My favorite method of consuming science-fiction is through books. Therefore, I chose to look at the differences between fiction and nonfiction books. . Without diving into the actual content of books (that&#39;s a project for a later time when I have more experience with natural language processing), my goal was to look for patterns in the book metadata that could distinguish the two genres. . One last quick note before getting into it. . I&#39;m not going to walk through every single step (though I&#39;ll do my best to paint the whole picture) or show the actual code in this article. If you&#39;re curious about the nitty-gritty details and code I wrote for any of the steps, the Jupyter notebooks found at the link below walk through all of it. . Imports and Configuration . import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from scipy.stats import randint, uniform . %matplotlib inline pd.options.display.max_rows = 200 pd.options.display.max_columns = 200 . # Preprocessing import category_encoders as ce from sklearn.preprocessing import StandardScaler from sklearn.experimental import enable_iterative_imputer from sklearn.impute import SimpleImputer, IterativeImputer # Model validation from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV from sklearn.pipeline import make_pipeline, Pipeline from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, classification_report # Models from sklearn.linear_model import LogisticRegression from sklearn.ensemble import RandomForestClassifier from sklearn.neighbors import KNeighborsClassifier # Interpretations import eli5 from eli5.sklearn import PermutationImportance . . The Data . I searched around a bit for data that could help me answer this question. There were a few datasets on Kaggle that contained metadata scraped from GoodReads. I liked the idea of using metadata, but none of these datasets were quite what I wanted. Therefore, I decided to gather my own dataset and gain some more experience with web scraping along the way. . Scraping . I did not build the Scrapy scraper completely from scratch, though I probably would have if I hadn&#39;t run across a small project on GitHub that was built for this exact purpose. I figured the scraper itself wasn&#39;t as important to this particular project as the data it produced, so I decided to use it instead of building my own. . However, I did get my feet pretty wet with Scrapy in order to understand what it was doing and why it didn&#39;t work when I first tested it out. I forked it and made some minor modifications to the code to fix a bug or two and make it better fit my use-case. Overall, finding the project was a definite time-saver, and I still got to learn about how to build a robust web spider with Scrapy. . My fork of the repository can be found here: tobias-fyi/GoodReads. . Of course it&#39;s not realistic to simply send off the spider to crawl every single book on GoodReads. I decided to look at the popular reading lists that are curated on the site, as they do a good job aggregating books chosen by many tens of thousands of users. . The list I ended up choosing to use as the basis for my scrape is called Books That Everyone Should Read At Least Once, which has a little over 20,000 titles, voted on by almost 100,000 users. . This worked out nicely, and after about 16 hours of crawling and scraping, I had my dataset containing the metadata for about 20,000 books. . The raw output from the scraper took the form of a series of JSON files. I wrote a few helper functions that concatenated these files into a single Pandas DataFrame, with some basic initial preprocessing steps built in, such as dropping duplicate records and cleaning up column names. . Convert data to tabular format . The output of the scraper was a set of JSON files. In order to use it in the project, I&#39;ll need to convert to Pandas DataFrame. . datapath = &quot;/Users/Tobias/workshop/vela/ds/interview_prep/practice/print-fiction/assets/json_data&quot; . bookfiles = [ # List of book json files to be included in the books dataframe &quot;book_must_read_01_20.jl&quot;, &quot;book_must_read_21_200.jl&quot;, &quot;book_must_read_201_216.jl&quot;, ] # Create list of filepaths from book file names bookpaths = [os.path.join(datapath, filename) for filename in bookfiles] bookpaths . [&#39;/Users/Tobias/workshop/vela/ds/interview_prep/practice/print-fiction/assets/json_data/book_must_read_01_20.jl&#39;, &#39;/Users/Tobias/workshop/vela/ds/interview_prep/practice/print-fiction/assets/json_data/book_must_read_21_200.jl&#39;, &#39;/Users/Tobias/workshop/vela/ds/interview_prep/practice/print-fiction/assets/json_data/book_must_read_201_216.jl&#39;] . Functions to combine files into single DataFrame and do some preprocessing . def json_cat(json_files): &quot;&quot;&quot;Reads and concatenates a list of .jl (json lines) files into a single dataframe. &quot;&quot;&quot; # Read the books json files into a list of dataframes dfs = [pd.read_json(filepath, lines=True) for filepath in json_files] # Concatenate the list of dataframes df = pd.concat(dfs, sort=False) return df . def encode_book_genres(df): &quot;&quot;&quot;Deconcatenates top 30 book genres into separate features, OneHotEncoding style. &quot;&quot;&quot; # Set comprehension - creates a set of all genres listed in dataset all_genres = {genre for row_genres in df[&quot;genres&quot;] for genre in row_genres} # Create a new feature for every genre for genre in all_genres: has_genre = lambda g: genre in g df[genre] = df.genres.apply(has_genre) # Create list of top 30 most common genres most_common_genres = df[list(all_genres)].sum().sort_values(ascending=False).head(30) most_common_genres = most_common_genres.index.tolist() # Drop all but the top 30 genres from the dataframe unwanted_genres = list(all_genres - set(most_common_genres)) df = df.drop(columns=unwanted_genres) # Drop the original &quot;genres&quot; feature df = df.drop(columns=[&quot;genres&quot;]) return df . def book_pub_date(df): &quot;&quot;&quot;Deconcatenates book publish_date to three separate features for year, month, and day. Drops the original publish_date feature. &quot;&quot;&quot; # === The Pandas method === # # Convert the &quot;publish_date&quot; column to datetime df[&quot;publish_date&quot;] = pd.to_datetime(df[&quot;publish_date&quot;], errors=&quot;coerce&quot;, infer_datetime_format=True) # Break out &quot;publish_date&quot; into dt components df[&quot;publish_year&quot;] = df[&quot;publish_date&quot;].dt.year df[&quot;publish_month&quot;] = df[&quot;publish_date&quot;].dt.month df[&quot;publish_day&quot;] = df[&quot;publish_date&quot;].dt.day df = df.drop(columns=[&quot;publish_date&quot;]) # Drop the OG publish_date return df . def book_cat(paths_list, output_filename): &quot;&quot;&quot;Reads and concatenates a list of book_*.jl (json lines) files.&quot;&quot;&quot; # === Concatenate the list of dataframes === # df = json_cat(paths_list) # === Initial wrangling === # # I will address these three steps later on # df = df.dropna(subset=[&quot;genres&quot;]) # Drop rows with null &quot;genres&quot; # df = encode_book_genres(df) # Break out genres into top 30 # df = book_pub_date(df) # Break out publish_date into components df = df.drop_duplicates(subset=[&quot;url&quot;]) # Drop duplicate records # Format column names with pyjanitor df = (df.clean_names()) # Break ratings_histogram (array) into component features df_hist = df[&quot;rating_histogram&quot;].apply(pd.Series) rating_cols = {} # Dict for mapping column names for col in df_hist.columns.tolist(): rating_cols[col] = f&quot;{col}_rating_count&quot; # Rename according to mapper created above df_hist = df_hist.rename(columns=rating_cols) # Concat new columns onto main dataframe df = pd.concat([df, df_hist], axis=1, join=&quot;outer&quot;) # Drop extra column df = df.drop(columns=[&quot;rating_histogram&quot;]) df.to_csv(output_filename, index=False) print(f&quot;Created dataframe and saved to current directory as &#39;{output_filename}&#39;&quot;) return df . Create and export books DataFrame . books = book_cat(bookpaths, &quot;must_read_books-01.csv&quot;) . Successfully created dataframe and saved to current directory as &#39;must_read_books-01.csv&#39; . print(books.shape) books.head() . (21514, 21) . url title author num_ratings num_reviews avg_rating num_pages language publish_date original_publish_year genres characters series places asin 0_rating_count 1_rating_count 2_rating_count 3_rating_count 4_rating_count 5_rating_count . 0 https://www.goodreads.com/book/show/323355.The... | The Book of Mormon: Another Testament of Jesus... | Anonymous | 71355.0 | 5704.0 | 4.37 | 531.0 | English | 2013-10-22 00:00:00 | 1830.0 | [Lds, Church, Christianity, Religion, Nonfiction] | NaN | NaN | NaN | NaN | NaN | 7520.0 | 2697.0 | 2521.0 | 1963.0 | 56654.0 | . 1 https://www.goodreads.com/book/show/28862.The_... | The Prince | Niccolò Machiavelli | 229715.0 | 7261.0 | 3.81 | 140.0 | English | 2003-06-01 00:00:00 | 1513.0 | [European Literature, Italian Literature, Hist... | [Theseus (mythology), Alexander the Great, Ces... | NaN | NaN | NaN | NaN | 5254.0 | 16827.0 | 61182.0 | 80221.0 | 66231.0 | . 2 https://www.goodreads.com/book/show/46654.The_... | The Foundation Trilogy | Isaac Asimov | 83933.0 | 1331.0 | 4.40 | 679.0 | English | 1974-01-01 00:00:00 | 1953.0 | [Science Fiction, Classics, Fiction] | [Hari Seldon, Salvor Hardin, Hober Mallow, Mul... | Foundation (Publication Order) #1-3 | NaN | NaN | NaN | 477.0 | 1521.0 | 9016.0 | 25447.0 | 47472.0 | . 3 https://www.goodreads.com/book/show/3980.From_... | From the Mixed-Up Files of Mrs. Basil E. Frank... | E.L. Konigsburg | 173617.0 | 6438.0 | 4.15 | 178.0 | English | 2003-06-02 00:00:00 | 1967.0 | [Childrens, Mystery, Middle Grade, Fiction, Yo... | [Mrs. Basil E. Frankweiler, Claudia Kincaid, J... | NaN | [New York City, New York, Connecticut] | NaN | NaN | 2742.0 | 6381.0 | 29358.0 | 58559.0 | 76577.0 | . 4 https://www.goodreads.com/book/show/18521.A_Ro... | A Room of One&#39;s Own | Virginia Woolf | 98164.0 | 5848.0 | 4.14 | 112.0 | English | 2000-01-01 00:00:00 | 1929.0 | [Essays, Feminism, Classics, Nonfiction, Writing] | NaN | NaN | NaN | NaN | NaN | 1357.0 | 3778.0 | 15993.0 | 35876.0 | 41160.0 | . books.isnull().sum() . url 0 title 1 author 1 num_ratings 1 num_reviews 1 avg_rating 1 num_pages 1175 language 2148 publish_date 436 original_publish_year 8675 genres 2941 characters 15691 series 14466 places 16276 asin 17561 0_rating_count 21514 1_rating_count 1295 2_rating_count 1295 3_rating_count 1295 4_rating_count 1295 5_rating_count 1295 dtype: int64 . # This version is directly from the scraping, minimal processing data_path = &quot;assets/must_read_books-01.csv&quot; books = pd.read_csv(data_path) print(books.shape) books.head(3) . (21514, 21) . url title author num_ratings num_reviews avg_rating num_pages language publish_date original_publish_year genres characters series places asin 0_rating_count 1_rating_count 2_rating_count 3_rating_count 4_rating_count 5_rating_count . 0 https://www.goodreads.com/book/show/323355.The... | The Book of Mormon: Another Testament of Jesus... | Anonymous | 71355.0 | 5704.0 | 4.37 | 531.0 | English | 2013-10-22 00:00:00 | 1830.0 | [&#39;Lds&#39;, &#39;Church&#39;, &#39;Christianity&#39;, &#39;Religion&#39;, ... | NaN | NaN | NaN | NaN | NaN | 7520.0 | 2697.0 | 2521.0 | 1963.0 | 56654.0 | . 1 https://www.goodreads.com/book/show/28862.The_... | The Prince | Niccolò Machiavelli | 229715.0 | 7261.0 | 3.81 | 140.0 | English | 2003-06-01 00:00:00 | 1513.0 | [&#39;European Literature&#39;, &#39;Italian Literature&#39;, ... | [&#39;Theseus (mythology)&#39;, &#39;Alexander the Great&#39;,... | NaN | NaN | NaN | NaN | 5254.0 | 16827.0 | 61182.0 | 80221.0 | 66231.0 | . 2 https://www.goodreads.com/book/show/46654.The_... | The Foundation Trilogy | Isaac Asimov | 83933.0 | 1331.0 | 4.40 | 679.0 | English | 1974-01-01 00:00:00 | 1953.0 | [&#39;Science Fiction&#39;, &#39;Classics&#39;, &#39;Fiction&#39;] | [&#39;Hari Seldon&#39;, &#39;Salvor Hardin&#39;, &#39;Hober Mallow... | Foundation (Publication Order) #1-3 | NaN | NaN | NaN | 477.0 | 1521.0 | 9016.0 | 25447.0 | 47472.0 | . Data wrangling and exploration . Dataset in hand, it was time to explore and wrangle! . As always, the first step was to take a look at what I have—basic info about the features, such as the types of data and null values. . books.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 21514 entries, 0 to 21513 Data columns (total 21 columns): # Column Non-Null Count Dtype -- -- 0 url 21514 non-null object 1 title 21513 non-null object 2 author 21513 non-null object 3 num_ratings 21513 non-null float64 4 num_reviews 21513 non-null float64 5 avg_rating 21513 non-null float64 6 num_pages 20339 non-null float64 7 language 19366 non-null object 8 publish_date 21078 non-null object 9 original_publish_year 12839 non-null float64 10 genres 18573 non-null object 11 characters 5823 non-null object 12 series 7048 non-null object 13 places 5238 non-null object 14 asin 3953 non-null object 15 0_rating_count 0 non-null float64 16 1_rating_count 20219 non-null float64 17 2_rating_count 20219 non-null float64 18 3_rating_count 20219 non-null float64 19 4_rating_count 20219 non-null float64 20 5_rating_count 20219 non-null float64 dtypes: float64(11), object(10) memory usage: 3.4+ MB . Feature management . Before digging deep into the data, there was some initial processing and pruning to be done to the features to make them more manageable later on. . Right off the bat I removed some features that likely wouldn&#39;t prove useful in the model: . url | asin | 0_rating_count | characters | places | . url and asin are obviously not going to be useful, just extra noise. As can be seen in the info table above, 0_rating_count was completely null because GoodReads doesn&#39;t allow books to get zero stars. . I based my decision to remove characters and places on my domain knowledge and on the quality of their data. In my experience, nonfiction books are much less likely to be set in a particular location or have characters. . On one hand, this could be valuable information for the model to know: if the book has a list of characters, it is more likely to be fiction. On the other hand, that information could be too useful—i.e. it could leak information about the target. . Both have a high proportion of null values—over 50%, as can be seen in the table above—and I cannot be sure whether the null values represent the fact that users simply haven&#39;t taken the time to add that data to those books, or if those books really do not have any significant characters or places. . drop_cols = [ &quot;url&quot;, &quot;asin&quot;, &quot;0_rating_count&quot;, &quot;characters&quot;, &quot;places&quot;, ] books = books.drop(columns=drop_cols) . . Note: Dropping the rows with null genres has to happen here in order to complete the data type conversion directly below this. . books = books.dropna(subset=[&quot;genres&quot;]) print(books.shape) books.head(2) . (18573, 18) . title author num_ratings num_reviews avg_rating num_pages language original_publish_year genres series 1_rating_count 2_rating_count 3_rating_count 4_rating_count 5_rating_count publish_year publish_month publish_day . 0 The Book of Mormon: Another Testament of Jesus... | Anonymous | 71355.0 | 5704.0 | 4.37 | 531.0 | English | 1830.0 | [&#39;Lds&#39;, &#39;Church&#39;, &#39;Christianity&#39;, &#39;Religion&#39;, ... | NaN | 7520.0 | 2697.0 | 2521.0 | 1963.0 | 56654.0 | 2013.0 | 10.0 | 22.0 | . 1 The Prince | Niccolò Machiavelli | 229715.0 | 7261.0 | 3.81 | 140.0 | English | 1513.0 | [&#39;European Literature&#39;, &#39;Italian Literature&#39;, ... | NaN | 5254.0 | 16827.0 | 61182.0 | 80221.0 | 66231.0 | 2003.0 | 6.0 | 1.0 | . Data types . I did some initial feature engineering early on to make the features more manageable for me and usable for the model. More feature engineering will take place later on. . First, the publish_date feature needed to be split up into its component parts (year, month, day), because Scikit-learn models can&#39;t directly use the datetime format. By splitting them up into integers, their meaning can be preserved and the model can process them. . Second, I had to deal with nested data. Most notably, the genres column was organized as an array of genres for each book. Because Pandas doesn&#39;t parse this kind of data by default, the column imported as the object (text) datatype. The best way I found to deal with it, without delving into natural language processing, was to break out each genre into its own column, one-hot encoding style. . This step was very important, as I used the genres to engineer the fiction target. . Note: these steps have to happen after the genre null values are removed. . def book_pub_date(df: pd.DataFrame): &quot;&quot;&quot;Deconcatenates book publish_date to three separate features for year, month, and day. Drops the original publish_date feature. &quot;&quot;&quot; # Convert the &quot;publish_date&quot; column to datetime df[&quot;publish_date&quot;] = pd.to_datetime(df[&quot;publish_date&quot;], errors=&quot;coerce&quot;, infer_datetime_format=True) # Break out &quot;publish_date&quot; into dt components df[&quot;publish_year&quot;] = df[&quot;publish_date&quot;].dt.year df[&quot;publish_month&quot;] = df[&quot;publish_date&quot;].dt.month df[&quot;publish_day&quot;] = df[&quot;publish_date&quot;].dt.day df = df.drop(columns=[&quot;publish_date&quot;]) # Drop the OG publish_date return df books = book_pub_date(books) books.head(2) . title author num_ratings num_reviews avg_rating num_pages language original_publish_year genres series 1_rating_count 2_rating_count 3_rating_count 4_rating_count 5_rating_count publish_year publish_month publish_day . 0 The Book of Mormon: Another Testament of Jesus... | Anonymous | 71355.0 | 5704.0 | 4.37 | 531.0 | English | 1830.0 | [&#39;Lds&#39;, &#39;Church&#39;, &#39;Christianity&#39;, &#39;Religion&#39;, ... | NaN | 7520.0 | 2697.0 | 2521.0 | 1963.0 | 56654.0 | 2013.0 | 10.0 | 22.0 | . 1 The Prince | Niccolò Machiavelli | 229715.0 | 7261.0 | 3.81 | 140.0 | English | 1513.0 | [&#39;European Literature&#39;, &#39;Italian Literature&#39;, ... | NaN | 5254.0 | 16827.0 | 61182.0 | 80221.0 | 66231.0 | 2003.0 | 6.0 | 1.0 | . Null values . A feature with a large proportion of null values is far less likely to be useful, as imputing (filling in) the missing data can add noise—or even artificial patterns—that would adversely affect the model. Therefore, a feature such as that can usually be considered extraneous and removed from the dataset. With a small enough proportion of null values, imputing or otherwise filling in the missing data is more likely to preserve the preexisting patterns in those features. . Depending on the distribution of null values throughout the dataset and the characteristics of the features, it might be better to remove rows that have a high proportion of nulls. . There is one specific feature for which I want to drop all rows that have a null: &#39;genres&#39;. Because I am going to be using this to engineer my target, I don&#39;t want to risk biasing the model by imputing the missing values. . After removing those, I was left with 18,573 rows. . The above table doesn&#39;t do a great job describing the null value situation. Below is a visualization (thanks to missingno) showing the distribution and proportion of null values per feature (after removing rows with null genres). . books.isnull().sum() . title 0 author 0 num_ratings 0 num_reviews 0 avg_rating 0 num_pages 676 language 1359 original_publish_year 6364 genres 0 series 11788 1_rating_count 83 2_rating_count 83 3_rating_count 83 4_rating_count 83 5_rating_count 83 publish_year 284 publish_month 284 publish_day 284 dtype: int64 . # import missingno as msno msno.matrix(books) plt.title(&quot;Null values by feature&quot;) # plt.tight_layout() # plt.savefig(&quot;null_values_by_feature.png&quot;, dpi=80) . Text(0.5, 1.0, &#39;Null values by feature&#39;) . I used Scikit-learn&#39;s IterativeImputer to impute the missing values for many of the features with null values. Basically, it models each feature as a function of the other features, using that model to &quot;predict&quot; what the missing value would have been if it wasn&#39;t null. . There are a couple of features that had to be dealt with differently, original_publish_year and series, as their null values actually held information. . For original_publish_year, I assumed that a null value indicated that the book had not been previously published. And similarly for series, I assumed null indicated a standalone book. I transformed both of these features into binary: 0 indicating the value was null, 1 indicating it was not. . def encode_book_genres(df: pd.DataFrame): &quot;&quot;&quot;Deconcatenates top 30 book genres into separate features, OneHotEncoding style. &quot;&quot;&quot; from ast import literal_eval # === Convert &#39;genres&#39; to python list === # df[&quot;genres&quot;] = df[&quot;genres&quot;].apply(literal_eval) # Create a set of all distinct genres listed in dataset all_genres = {genre for row_genres in df[&quot;genres&quot;] for genre in row_genres} # Create a new feature for every genre for genre in all_genres: has_genre = lambda g: genre in g df[genre] = df.genres.apply(has_genre) # Create list of top 30 most common genres # NOTE: I ended up only using &#39;fiction&#39;, the top result most_common_genres = df[list(all_genres)].sum().sort_values(ascending=False).head(1) most_common_genres = most_common_genres.index.tolist() # Drop all but the top 30 genres from the dataframe unwanted_genres = list(all_genres - set(most_common_genres)) df = df.drop(columns=unwanted_genres) # Drop the original &quot;genres&quot; feature df = df.drop(columns=[&quot;genres&quot;]) # Convert from Boolean to binary df = df.replace(to_replace={True: 1, False:0}) # Format column names with pyjanitor df = (df.clean_names()) return df books = encode_book_genres(books) books.head(2) . title author num_ratings num_reviews avg_rating num_pages language original_publish_year series 1_rating_count 2_rating_count 3_rating_count 4_rating_count 5_rating_count publish_year publish_month publish_day fiction . 0 The Book of Mormon: Another Testament of Jesus... | Anonymous | 71355.0 | 5704.0 | 4.37 | 531.0 | English | 1830.0 | NaN | 7520.0 | 2697.0 | 2521.0 | 1963.0 | 56654.0 | 2013.0 | 10.0 | 22.0 | 0 | . 1 The Prince | Niccolò Machiavelli | 229715.0 | 7261.0 | 3.81 | 140.0 | English | 1513.0 | NaN | 5254.0 | 16827.0 | 61182.0 | 80221.0 | 66231.0 | 2003.0 | 6.0 | 1.0 | 0 | . books[&quot;series&quot;] = books[&quot;series&quot;].notnull().replace(to_replace={True: 1, False:0}) books[&quot;series&quot;].value_counts() . 0 11788 1 6785 Name: series, dtype: int64 . books[&quot;republish&quot;] = books[&quot;original_publish_year&quot;].notnull().replace(to_replace={True: 1, False:0}) books = books.drop(columns=[&quot;original_publish_year&quot;]) books[&quot;republish&quot;].value_counts() . 1 12209 0 6364 Name: republish, dtype: int64 . books.dtypes . title object author object num_ratings float64 num_reviews float64 avg_rating float64 num_pages float64 language object series int64 1_rating_count float64 2_rating_count float64 3_rating_count float64 4_rating_count float64 5_rating_count float64 publish_year float64 publish_month float64 publish_day float64 fiction int64 republish int64 dtype: object . Duplicates or republished? . books.describe(exclude=&quot;number&quot;).T.sort_values(by=&quot;unique&quot;) . count unique top freq . language 17214 | 55 | English | 15909 | . author 18573 | 10019 | Stephen King | 79 | . title 18573 | 17816 | Selected Poems | 6 | . books[books[&quot;title&quot;] == &quot;When Breath Becomes Air&quot;] . title author num_ratings num_reviews avg_rating num_pages language series 1_rating_count 2_rating_count 3_rating_count 4_rating_count 5_rating_count publish_year publish_month publish_day fiction republish . 39 When Breath Becomes Air | Paul Kalanithi | 332950.0 | 28262.0 | 4.35 | 208.0 | English | 0 | 4685.0 | 8103.0 | 35451.0 | 101807.0 | 182904.0 | 2016.0 | 1.0 | 19.0 | 0 | 1 | . 4782 When Breath Becomes Air | Paul Kalanithi | 333005.0 | 28275.0 | 4.35 | 229.0 | English | 0 | 4685.0 | 8103.0 | 35454.0 | 101823.0 | 182940.0 | 2016.0 | 1.0 | 12.0 | 0 | 0 | . 10877 When Breath Becomes Air | Paul Kalanithi | 332942.0 | 28259.0 | 4.35 | 229.0 | English | 0 | 4685.0 | 8103.0 | 35451.0 | 101804.0 | 182899.0 | 2017.0 | 1.0 | 23.0 | 0 | 1 | . 12947 When Breath Becomes Air | Paul Kalanithi | 333024.0 | 28277.0 | 4.35 | 209.0 | NaN | 0 | 4686.0 | 8103.0 | 35455.0 | 101826.0 | 182954.0 | 2016.0 | 2.0 | 17.0 | 0 | 1 | . 13426 When Breath Becomes Air | Paul Kalanithi | 333025.0 | 28277.0 | 4.35 | 228.0 | English | 0 | 4686.0 | 8103.0 | 35455.0 | 101826.0 | 182955.0 | 2016.0 | 1.0 | 12.0 | 0 | 0 | . 19976 When Breath Becomes Air | Paul Kalanithi | 333042.0 | 28279.0 | 4.35 | 5.0 | English | 0 | 4686.0 | 8104.0 | 35457.0 | 101830.0 | 182965.0 | 2016.0 | 1.0 | 12.0 | 0 | 0 | . Looks like there are some duplicates. I&#39;m going to use title, author, and publish_year as the subset this time. . books = books.drop_duplicates(subset=[&quot;title&quot;, &quot;author&quot;, &quot;publish_year&quot;]) . books.shape . (18383, 18) . It looks like there were around 200 duplicates that slipped through the cracks. Not this time! . Distributions and outliers . As can be expected with media such as books, there were some large outliers with regards to popularity. While the vast majority of books had relatively few ratings and even fewer reviews, where is a relatively small group of authors that struck gold, with them and their books becoming household names. . I found it somewhat surprising that my dataset reflected this reality so closely, as I thought a list of &quot;must reads&quot; would be biased toward more popular books. That bias is likely present—the dataset is large enough to include many books that never became household names—though not to the degree I initially thought. . # Caption: Distribution of the number of ratings. plt.figure(figsize=(16, 8)) sns.distplot(books[&quot;num_reviews&quot;]) plt.title(&quot;Distribution of &#39;num_reviews&#39;&quot;); # plt.savefig(&quot;num_reviews_distplot.png&quot;, dpi=160) . The distributions of the number of ratings and reviews are both heavily skewed to the right. The box in the boxplot below indicates the interquartile range, and the line that sits just barely to the right indicates the top of the third quartile. Basically, anything above that line can be considered an outlier. . Another way to look at the dispersion of the data is to consider the mean versus the median, along with the standard deviation. In the case of num_ratings, the relative size of the mean (46,958, with a standard deviation of 212,398!) compared to the median (4,135) indicates that the mean is significantly influenced by outliers. . # Caption: Boxplot showing distribution of the number of ratings. plt.figure(figsize=(16, 8)) sns.boxplot(x=books[&quot;num_ratings&quot;]) plt.title(&quot;Distribution of &#39;num_ratings&#39;&quot;); # plt.savefig(&quot;num_ratings_boxplot.png&quot;, dpi=160) . Curious what those books are? . # Caption: Highest rated books. hiratings = books.nlargest(20, [&quot;num_ratings&quot;]).set_index(&quot;title&quot;)[&quot;num_ratings&quot;] plt.figure(figsize=(16, 8)) sns.barplot(hiratings, hiratings.index, palette=&quot;deep&quot;) plt.title(&quot;Books with the most ratings&quot;); # plt.tight_layout() # plt.savefig(&quot;books_most_ratings.png&quot;, dpi=160) . Dealing with the outliers . Notice anything about the books with the most ratings—i.e. the most popular? . Every one the top 12 most popular books is fiction. . Based on that observation, I figured the model would find these particular outliers to be useful. However, given the huge range of these features, they would have to be scaled for use in any linear models. To this end, I included an instance of Scikit-learn&#39;s StandardScaler in my pipeline, which standardizes numeric features, transforming the data such that each feature has a mean of zero and standard deviation of 1. Basically, that just brings all of the numeric data into the same range. . Scaling the data is very important for linear models or neural networks—not so much for decision tree-based models. Therefore, I only used the StandardScaler as needed. . # Caption: Boxplot showing the distribution of number of pages. plt.figure(figsize=(16, 8)) sns.boxplot(x=books[&quot;num_pages&quot;]) plt.title(&quot;Distribution of &#39;num_pages&#39;&quot;); # plt.savefig(&quot;num_pages_boxplot.png&quot;, dpi=160) . Another feature that had stark outliers was the number of pages. There were several books with over 5,000 pages when the majority had less than 500. As with the outliers discussed above, I figured that the number of pages was relevant to the model. . Upon further inspection, however, I found that most of the outliers in this case were not actually single books, but entire book series. Therefore, I decided to remove the farthest outliers—those over 2,000 pages in length. . At this point, I was left with 18,344 rows in my dataset. . # This could be one of the sliders on the dashboard cutoff = 2000 books_over_pages_cutoff = books[books[&quot;num_pages&quot;] &gt; cutoff] print(books_over_pages_cutoff.shape) books_over_pages_cutoff.head(2) . (39, 18) . title author num_ratings num_reviews avg_rating num_pages language series 1_rating_count 2_rating_count 3_rating_count 4_rating_count 5_rating_count publish_year publish_month publish_day fiction republish . 114 The Complete Anne of Green Gables Boxed Set | L.M. Montgomery | 99371.0 | 1491.0 | 4.43 | 2088.0 | English | 1 | 1611.0 | 2123.0 | 9689.0 | 24650.0 | 61298.0 | 1998.0 | 10.0 | 6.0 | 1 | 1 | . 274 In Search of Lost Time | Marcel Proust | 9249.0 | 515.0 | 4.34 | 4211.0 | English | 1 | 249.0 | 365.0 | 965.0 | 2045.0 | 5625.0 | 2003.0 | 6.0 | 3.0 | 1 | 1 | . books2 = books.drop(books_over_pages_cutoff.index, axis=0) print(books2.shape) books2.head(2) . (18344, 18) . title author num_ratings num_reviews avg_rating num_pages language series 1_rating_count 2_rating_count 3_rating_count 4_rating_count 5_rating_count publish_year publish_month publish_day fiction republish . 0 The Book of Mormon: Another Testament of Jesus... | Anonymous | 71355.0 | 5704.0 | 4.37 | 531.0 | English | 0 | 7520.0 | 2697.0 | 2521.0 | 1963.0 | 56654.0 | 2013.0 | 10.0 | 22.0 | 0 | 1 | . 1 The Prince | Niccolò Machiavelli | 229715.0 | 7261.0 | 3.81 | 140.0 | English | 0 | 5254.0 | 16827.0 | 61182.0 | 80221.0 | 66231.0 | 2003.0 | 6.0 | 1.0 | 0 | 1 | . books2[&quot;num_pages&quot;].describe() . count 17678.000000 mean 333.513576 std 196.836062 min 0.000000 25% 220.000000 50% 306.000000 75% 400.000000 max 1985.000000 Name: num_pages, dtype: float64 . Feature relationships . Another line of inquiry I wanted to explore before diving into the model was relationships between features and between the target and the features. The obvious first step was to create a matrix of scatterplots between what I believed to be the most important features and the target. As the target is binary, I thought it best to represent it as a third dimension in each of the scatterplots: color. . # Caption: Matrix of scatterplots showing relationships between certain features. sns.pairplot( books2, hue=&quot;fiction&quot;, vars=[&quot;num_reviews&quot;, &quot;avg_rating&quot;, &quot;num_pages&quot;], palette=&quot;deep&quot;, plot_kws=dict(alpha=0.8, s=20), height=4, ); . &lt;seaborn.axisgrid.PairGrid at 0x134837ad0&gt; . Although it&#39;s difficult to see much in the way of detail, this matrix is great for getting an overall idea of what&#39;s going on between the features in question. . One characteristic that immediately stuck out to me is the majority of outliers in num_ratings are fiction books. This helps to confirm my earlier hypothesis that these outliers will be valuable to the model. . Also, I noticed that avg_rating seems to be different between fiction and nonfiction books. The comparison of distributions (middle-middle) and the scatter plots comparing avg_rating to num_pages (middle-bottom and middle-right) seem to indicate that nonfiction books are rated slightly higher than fiction. . Seeing this difference in densities when grouped by the target made me want to explore other features in this manner. An interesting one I found was publish_year. As can be seen in plot below, it seems that the &quot;golden years&quot; of fiction were in the mid-2000s (at least according to this reading list), whereas the mid-2010s had more good nonfiction. . # Caption: Distribution of books by fictionality. sns.pairplot( books2[books2[&quot;publish_year&quot;] &gt; 1960], hue=&quot;fiction&quot;, vars=[&quot;publish_year&quot;], palette=&quot;deep&quot;, plot_kws=dict(alpha=0.8, s=20), height=8, ); . Although the exploration above is relatively basic, I can see that there seem to be some meaningful relationships in the data. Most important to my purposes is confirming that there are patterns that indicate some differentiation in the features when grouping them by the target. . With that in mind, I felt good enough to get into playing with some models. . . The Model . Baseline Models . As I discussed above, the most complex model is not necessarily the best model. It&#39;s generally a good idea to start with a simple baseline model and progressively add complexity in proceeding iterations. The chosen evaluation metric will (ideally) indicate when the added complexity is beneficial, and when it is not. . train, test = train_test_split(books, stratify=books[&quot;fiction&quot;], test_size=0.2, random_state=92) train, val = train_test_split(train, stratify=train[&quot;fiction&quot;], test_size=0.2, random_state=92) train.shape, val.shape, test.shape . ((11740, 18), (2935, 18), (3669, 18)) . target = &quot;fiction&quot; # Arrange y vector y_train = train[target] y_val = val[target] y_test = test[target] print(y_train.shape, y_val.shape, y_test.shape) # Arrange X matrices X_train = train.drop(columns=[target]) X_val = val.drop(columns=[target]) X_test = test.drop(columns=[target]) print(X_train.shape, X_val.shape, X_test.shape) . (11740,) (2935,) (3669,) (11740, 17) (2935, 17) (3669, 17) . Majority class . The simplest possible model is to simply predict the majority class every time, regardless of input. Assuming an evenly distributed binary target, that model should be right about half the time, or have a 50% accuracy. In my case, the majority class baseline is just north of that, around ~.52, or ~52%. . Now I know that no matter what model I end up choosing and what features I end up using, it must have an accuracy of more than .52. If I can&#39;t beat that... . print(y_train.value_counts()) sns.distplot(y_train); . 1 6145 0 5595 Name: fiction, dtype: int64 . maj = y_train.mode()[0] # Mode is 1 (fiction) # Simply predict 1 for every training example y_pred_maj = [maj] * len(y_train) # Baseline accuracy accuracy_score(y_train, y_pred_maj) . 0.5234241908006815 . Limited logistic regression . Taking it a step further, I find it useful to get a second baseline using an algorithm with a bit more complexity than that, but far less complexity than what is possible given the problem parameters. In this case I chose to train a limited logistic regression model using only a few of the features: num_reviews, avg_rating, and num_pages. This simple baseline model had an accuracy of ~.63 and an F1 score of ~.65. . I will go into some detail on what the F1 score is in the Model Validation section. For now, suffice to say that the discrepancy between it and the accuracy is due to the slight imbalance of the classes in the target. . base_features = [ &quot;num_reviews&quot;, &quot;avg_rating&quot;, &quot;num_pages&quot;, ] # Arrange X matrices X1_train = train[base_features] X1_val = val[base_features] X1_test = test[base_features] X1_train.shape, X1_val.shape, X1_test.shape . ((11740, 3), (2935, 3), (3669, 3)) . pipe1 = Pipeline([ (&quot;scaler&quot;, StandardScaler()), (&quot;imputer&quot;, SimpleImputer(strategy=&quot;median&quot;)), (&quot;logreg&quot;, LogisticRegression(random_state=92)), ]) # Train base pipeline pipe1.fit(X1_train, y_train) . Pipeline(steps=[(&#39;scaler&#39;, StandardScaler()), (&#39;imputer&#39;, SimpleImputer(strategy=&#39;median&#39;)), (&#39;logreg&#39;, LogisticRegression(random_state=92))]) . y_pred1 = pipe1.predict(X1_val) # Compute accuracy print(&quot;Baseline accuracy:&quot;, accuracy_score(y_val, y_pred1)) . Baseline accuracy: 0.6275979557069846 . confusion_matrix(y_val, y_pred1) . array([[ 821, 578], [ 515, 1021]]) . Default random forest . I actually trained one more baseline model to set the bar higher and get a more accurate and precise idea of how high the bar will go given the problem, the available features, and the amount of data in the dataset. . This last baseline was a random forest that I left with default hyperparameters, trained on the full set of features as they were at this point in the process. This more complex—maybe even overly complex—model did quite a bit better, with an accuracy of ~.73 and an F1 of ~.75. . That&#39;s the score to beat as I iterate on the model and add complexity. . def_drop_columns = [ &quot;title&quot;, &quot;author&quot;, &quot;language&quot;, ] X2_train = X_train.drop(columns=def_drop_columns) X2_val = X_val.drop(columns=def_drop_columns) X2_test = X_test.drop(columns=def_drop_columns) rf1_pipe = Pipeline([ (&quot;encoder&quot;, ce.OrdinalEncoder()), (&quot;imputer&quot;, SimpleImputer(strategy=&quot;median&quot;)), (&quot;rfc&quot;, RandomForestClassifier(random_state=92)), ]) # Train default random forest rf1_pipe.fit(X2_train, y_train) # Made predictions to get validation accuracy y_pred_rf1 = rf1_pipe.predict(X2_val) # Compute evaluation metrics print(&quot;Default random forest eval metrics:&quot;) print(&quot; Accuracy:&quot;, accuracy_score(y_val, y_pred_rf1)) print(&quot; F1 score:&quot;, f1_score(y_val, y_pred_rf1)) . Default random forest eval metrics: Accuracy: 0.7342419080068143 F1 score: 0.7477360931435963 . Model validation . As always, before jumping into building and training I needed to make some decisions about how I will measure success—or, more accurately, how the model measures success. Choosing appropriate evaluation metrics is crucial to get an accurate understanding of how well a model is fitting (or overfitting) the data. An overly complex model will likely overfit the training data, leading to a model that performs poorly on new inputs on which is was not trained. Decision trees are particularly prone to overfitting . Target distribution . The target I&#39;m building a model to predict is binary, 1 or 0; fiction or nonfiction. Therefore, the metrics I choose must be appropriate for a binary classification model. The most common way to measure the performance of a binary classifier is the accuracy score—i.e. the proportion of predictions correctly classified by the model. . A very important characteristic to look at when choosing a metric is the distribution of the target variable. In the case of binary classification, if the target is skewed one way or the other, accuracy is not a good method of evaluating performance. . Although the target in my dataset is very evenly distributed between fiction (~54%) and nonfiction (~46%), it still is skewed a little bit. This means that if I use accuracy, I can expect it to be a decent indicator of performance, but slightly less than ideal. The reason for this is that accuracy only tracks the number of the model&#39;s mistakes, irrespective of how the model made those mistakes. . Types of errors . In the case of binary classification, there are two ways a prediction can be wrong: false positive (type I error) or false negative (type II error). As the name suggests, false positive is when the model (falsely) predicts positive or 1 when the actual label is 0. False negative is the alternative to that, when the model predicts negative or 0 when the label is 1. . A better method of evaluating performance will take into account the number of each type of error in a model&#39;s predictions. Depending on the target and what the model will be used for, it may be better to use a metric that looks at one or the other. . A common example is a spam detection model. In this case, it is generally better to let some spam emails through the filter into the users&#39; inboxes (false negative) than to put potentially important emails into the spam folder (false positive). . Confusion matrix . A common and effective way to get deeper insight into a classification model&#39;s predictions is with a confusion matrix, which is a type of contingency table with the target&#39;s classes laid out along both axes, the rows representing the actual classes and the columns showing the predicted classes. . # Caption: Confusion matrix for default random forest model. from sklearn.utils.multiclass import unique_labels unique_labels(y_val) # Create unique labels def plot_confusion_matrix(y_true, y_pred): labels = unique_labels(y_true) columns = [f&#39;Predicted {label}&#39; for label in labels] index = [f&#39;Actual {label}&#39; for label in labels] table = pd.DataFrame(confusion_matrix(y_true, y_pred), columns=columns, index=index) return sns.heatmap(table, annot=True, fmt=&#39;d&#39;, cmap=&#39;viridis&#39;) # Plot the confusion matrix plt.figure(figsize=(10, 8)) plt.title(&quot;Confusion matrix: default random forest&quot;) font = {&#39;family&#39; : &#39;normal&#39;, &#39;weight&#39; : &#39;bold&#39;, &#39;size&#39; : 14} plt.rc(&#39;font&#39;, **font) plot_confusion_matrix(y_val, y_pred_rf1); . findfont: Font family [&#39;normal&#39;] not found. Falling back to DejaVu Sans. findfont: Font family [&#39;normal&#39;] not found. Falling back to DejaVu Sans. . Where each row and column intersect, it shows the number of predictions that fall into that category. In the case of binary classification, this results in a table containing four numbers: true positives, true negatives, false positives, and false negatives. A classifier that made no mistakes would have non-zero numbers only on the main diagonal (top left to bottom right). . Evaluation metrics . There are three main metrics that can be derived from the confusion matrix. The first is precision, which is the proportion of the total positives (true positive + false positive) that were correctly predicted to be positive (true positive). The second is recall, which is the proportion of the total predicted values (true positives + false negatives) that were correctly predicted to be positive (true positive). The last metric I&#39;ll be covering now is the F1 score: the weighted average of precision and recall. . I chose to use the F1 score as the primary evaluation metric for my model because I&#39;m more interested in reducing the number of mistakes in general, as opposed to preferring to optimize for one or the other. . true_pos = 1156 false_pos = 400 precision = true_pos / (true_pos + false_pos) print(f&quot;Precision: {precision}&quot;) . Precision: 0.7429305912596401 . true_pos = 1156 false_neg = 380 precision = true_pos / (true_pos + false_neg) print(f&quot;Precision: {precision}&quot;) . Precision: 0.7526041666666666 . from sklearn.metrics import classification_report print(classification_report(y_val, y_pred_rf1)) . precision recall f1-score support 0 0.72 0.71 0.72 1399 1 0.74 0.75 0.75 1536 accuracy 0.73 2935 macro avg 0.73 0.73 0.73 2935 weighted avg 0.73 0.73 0.73 2935 . Let&#39;s see what we can do to increase that score. . Feature Engineering . When I initially started this project, I went through the process of validating and training a model or two that tried to predict the average rating of books. This was by far the most common target chosen by those who started Kaggle kernels using other GoodReads datasets. Although this may have the most obvious business value if I was a data scientist working for a book publisher, to me this wasn&#39;t a particularly interesting target to try to predict. . Target practice . I realized this when I hit a wall with my progress in improving the rating-predictor model. One reason was that I did not see any obvious useful features that could be engineered. However, once I found my way to the idea of predicting the fictionality of the books, the target drove the direction I took with my feature engineering. It was a great learning experience for me in engineering features toward the specific target that the model is trying to predict. I called this process &quot;target practice&quot;. . Here are the feature ideas I came up with and engineered (all in short succession once the new target was chosen): . Title begins with &quot;The&quot; | Has subtitle: contains &quot;:&quot; | Title character count | Title word count | Title longest word | Author number of names | Author middle initial | Ratings (stars) ratio: 1 + 2 / 4 + 5 | . def engineer_features(data): &quot;&quot;&quot;Engineer a handful of new features.&quot;&quot;&quot; # Create new feature that is if the title begins with &quot;The&quot; data[&quot;the_title&quot;] = data[&quot;title&quot;].str.startswith(&quot;The&quot;) # New feature - has_subtitle data[&quot;has_subtitle&quot;] = data[&quot;title&quot;].str.contains(&quot;:&quot;) # New feature - title character length data[&quot;title_char_count&quot;] = data[&quot;title&quot;].apply(lambda x: len(x)) # New feature - title word count data[&quot;title_word_count&quot;] = data[&quot;title&quot;].apply(lambda x: len(x.split())) # New feature - title longest word data[&quot;title_longest_word&quot;] = data[&quot;title&quot;].apply(lambda x: len(max(x.split(), key=len))) # New feature - author number of names data[&quot;author_name_count&quot;] = data[&quot;author&quot;].apply(lambda x: len(x.split())) # New feature - author middle initial pat = r&quot; w* ( w. )+ w*&quot; data[&quot;author_middle_initial&quot;] = data[&quot;author&quot;].str.contains(pat, regex=True) # New feature - low/high rating ratio data[&quot;rating_ratio&quot;] = (data[&quot;1_rating_count&quot;] + data[&quot;2_rating_count&quot;]) / (data[&quot;4_rating_count&quot;] + data[&quot;5_rating_count&quot;]) # Replace Boolean with binary data = data.replace(to_replace={True: 1, False:0}) return data . Forest with new features . To get an initial idea of the effect that these new features had on the model, I retrained a new random forest with all the same hyperparameters (defaults). The result was a significant boost in the model&#39;s F1 score, from ~.75 to ~.79. That made me happy. . However, because I engineered all of the new features at once—i.e. I didn&#39;t retrain the model after every one—this does not give me insight into which ones were useful. In fact, at this point I hadn&#39;t looked at how useful any of the specific features were. . X3_train = engineer_features(X_train) X3_val = engineer_features(X_val) X3_test = engineer_features(X_test) rf2_pipe = Pipeline([ (&quot;encoder&quot;, ce.OrdinalEncoder()), (&quot;imputer&quot;, SimpleImputer(strategy=&quot;median&quot;)), (&quot;rfc&quot;, RandomForestClassifier(random_state=92)), ]) # Train default random forest rf2_pipe.fit(X3_train, y_train) # Made predictions to get validation accuracy y_pred_rf2 = rf2_pipe.predict(X3_val) # Compute evaluation metrics print(&quot;Default random forest eval metrics:&quot;) print(&quot; Accuracy:&quot;, accuracy_score(y_val, y_pred_rf2)) print(&quot; F1 score:&quot;, f1_score(y_val, y_pred_rf2)) . /Users/Tobias/.vega/vela-_qIiF1eP/lib/python3.7/site-packages/pandas/core/strings.py:1954: UserWarning: This pattern has match groups. To actually get the groups, use str.extract. return func(self, *args, **kwargs) . Default random forest eval metrics: Accuracy: 0.7819420783645656 F1 score: 0.7939471989697361 . . Permutation Importances . It is likely that some of the features do not help the model make correct predictions. Indeed, some may even be worse than that: they could add noise that makes the model perform worse. . To address this potential problem, I&#39;m going to find each feature&#39;s importance to the model using a method called permutation importance. Basically, this method will go through each of the features, one by one, replacing their data with random noise generated from the distribution of the original data. The performance of the model will be evaluated and compared with the score using all of the original data to find the effect that each feature has on the performance of the model. . The following table is the result of running permutation importance on both the new and old features. . # Use the same (fitted) steps from main pipeline transformers = Pipeline([ (&quot;encoder&quot;, rf2_pipe.named_steps[&quot;encoder&quot;]), (&quot;imputer&quot;, rf2_pipe.named_steps[&quot;imputer&quot;]), ]) # Encode and impute X3_train_transformed = transformers.transform(X3_train) X3_val_transformed = transformers.transform(X3_val) . permuter = PermutationImportance( rf2_pipe.named_steps[&quot;rfc&quot;], scoring=&#39;f1&#39;, n_iter=5, random_state=42 ) permuter.fit(X3_val_transformed, y_val) . PermutationImportance(estimator=RandomForestClassifier(random_state=92), random_state=42, scoring=&#39;f1&#39;) . feature_names = X3_val.columns.tolist() pd.Series(permuter.feature_importances_, feature_names).sort_values(ascending=False) eli5.show_weights( permuter, top=None, # Show permutation importances for all features feature_names=feature_names ) . Weight Feature . 0.0307 &plusmn; 0.0021 | has_subtitle | . 0.0264 &plusmn; 0.0116 | avg_rating | . 0.0197 &plusmn; 0.0066 | 4_rating_count | . 0.0193 &plusmn; 0.0031 | publish_year | . 0.0153 &plusmn; 0.0059 | num_ratings | . 0.0124 &plusmn; 0.0039 | series | . 0.0116 &plusmn; 0.0071 | 1_rating_count | . 0.0096 &plusmn; 0.0048 | num_pages | . 0.0078 &plusmn; 0.0099 | num_reviews | . 0.0073 &plusmn; 0.0091 | 3_rating_count | . 0.0062 &plusmn; 0.0075 | title_char_count | . 0.0060 &plusmn; 0.0043 | 5_rating_count | . 0.0031 &plusmn; 0.0048 | rating_ratio | . 0.0016 &plusmn; 0.0024 | language | . 0.0008 &plusmn; 0.0059 | the_title | . 0.0007 &plusmn; 0.0056 | 2_rating_count | . 0.0005 &plusmn; 0.0024 | title_longest_word | . 0.0004 &plusmn; 0.0025 | republish | . 0.0003 &plusmn; 0.0020 | author_name_count | . 0.0001 &plusmn; 0.0002 | author_middle_initial | . -0.0002 &plusmn; 0.0040 | publish_month | . -0.0006 &plusmn; 0.0041 | author | . -0.0007 &plusmn; 0.0030 | title_word_count | . -0.0008 &plusmn; 0.0009 | title | . -0.0027 &plusmn; 0.0030 | publish_day | . Along with being useful for feature selection, I find it interesting to see what features have the largest positive effect on the model&#39;s predictive power. From this table, I can see that the majority of the benefit I got from engineering the new features came from has_subtitle. This feature, according to the permutation importance table, is the most important predictor of the lot, and simply indicates whether the title of the book has a colon in it. My intuition was that having a subtitle is very common for nonfiction books, not so much for fiction. It seems that my intuition was generally good. . Feature pruning . Based on the above table, I should see a small increase in the model&#39;s performance by removing publish_month, author, title_word_count, title, and publish_day. . more_drop_cols = [ &quot;publish_month&quot;, &quot;author&quot;, &quot;title_word_count&quot;, &quot;title&quot;, &quot;publish_day&quot;, ] # New features are already engineered X4_train = X3_train.drop(columns=more_drop_cols) X4_val = X3_val.drop(columns=more_drop_cols) X4_test = X3_test.drop(columns=more_drop_cols) . rf3_pipe = Pipeline([ (&quot;encoder&quot;, ce.OrdinalEncoder()), (&quot;imputer&quot;, SimpleImputer(strategy=&quot;median&quot;)), (&quot;rfc&quot;, RandomForestClassifier(random_state=92)), ]) # Train default random forest rf3_pipe.fit(X4_train, y_train) # Made predictions to get validation accuracy y_pred_rf3 = rf3_pipe.predict(X4_val) # Compute evaluation metrics print(&quot;Default random forest:&quot;) print(&quot; Accuracy:&quot;, accuracy_score(y_val, y_pred_rf3)) print(&quot; F1 score:&quot;, f1_score(y_val, y_pred_rf3)) . Default random forest: Accuracy: 0.7867120954003407 F1 score: 0.7991014120667522 . Once I removed those features, I saw another bump of ~.01 in the model&#39;s F1 score. That put the model up to an F1 score just south of .80 on the validation set. . Now that I&#39;d engineered some new features and pruned them to only the useful ones, it was time to iterate and tune. . Iteration . As I mentioned above, a good general approach to finding and training the best model for a particular problem and dataset is to start simple and iterate. I already iterated to select the best features. Next up was to iterate on algorithms and their various hyperparameters. . Cross-validation . Up until this point, I had been using a consistent train-test-validation set split. However, now that I was going to be tuning hyperparameters, it made sense to start using cross-validation. I won&#39;t get into the details of what that is in this particular article. If you&#39;re curious, search for it on Duck Duck Go, or read about it in The Python Data Science Handbook. . # Start from original dataset, because data # will only be split into train and test books2 = engineer_features(books) . /Users/Tobias/.vega/vela-_qIiF1eP/lib/python3.7/site-packages/pandas/core/strings.py:1954: UserWarning: This pattern has match groups. To actually get the groups, use str.extract. return func(self, *args, **kwargs) . print(books2.shape) books2.head(2) . (18344, 25) . title author num_ratings num_reviews avg_rating num_pages language series 1_rating_count 2_rating_count 3_rating_count 4_rating_count 5_rating_count publish_year publish_month publish_day fiction republish the_title has_subtitle title_char_count title_word_count author_name_count author_middle_initial rating_ratio . 0 The Book of Mormon: Another Testament of Jesus... | Anonymous | 71355.0 | 5704.0 | 4.37 | 531.0 | English | 0 | 7520.0 | 2697.0 | 2521.0 | 1963.0 | 56654.0 | 2013.0 | 10.0 | 22.0 | 0 | 1 | 1 | 1 | 53 | 9 | 1 | 0 | 0.174301 | . 1 The Prince | Niccolò Machiavelli | 229715.0 | 7261.0 | 3.81 | 140.0 | English | 0 | 5254.0 | 16827.0 | 61182.0 | 80221.0 | 66231.0 | 2003.0 | 6.0 | 1.0 | 0 | 1 | 1 | 0 | 10 | 2 | 2 | 0 | 0.150773 | . train, test = train_test_split(books2, stratify=books2[&quot;fiction&quot;], test_size=0.2, random_state=92) train.shape, test.shape . ((14675, 25), (3669, 25)) . # No val this time bc using cross-validation target = &quot;fiction&quot; drop_cols = [ # Columns not useful to model &quot;title&quot;, &quot;author&quot;, &quot;language&quot;, &quot;publish_month&quot;, &quot;publish_day&quot;, ] # Arrange y vector y_train = train[target] y_test = test[target] print(y_train.shape, y_test.shape) # Arrange X matrices X_train = train.drop(columns=[target] + drop_cols) X_test = test.drop(columns=[target] + drop_cols) print(X_train.shape, X_test.shape) . (14675,) (3669,) (14675, 19) (3669, 19) . Hyperparameter tuning . Hyperparameters are the parameters of the model that are not directly learned during the training process—they must be adjusted manually. My preferred method of tuning them is with a randomized search. . Essentially, I provide a list of algorithm hyperparameters and a search window or distribution for each one. The algorithm runs through a specified number of iterations, each time randomly selecting values for each hyperparameter from their respective distribution and using those to train a new model. Each model is evaluated after each iteration using cross-validation. Then, once the search is over, the pipeline is refitted using the values that resulted in the model with the best cross-validation score. . As the search is random and limited to a reasonable number of iterations, some strategy is involved to find optimal search windows/distributions for each hyperparameter. The method I used for this is to start out searching a wide range of values and go through the process a few times, progressively narrowing the range around the values that come up as consistently optimal. . Validation . Before getting too deep into tuning the hyperparameters of a specific algorithm (it is computationally expensive to run exhaustive searches), I thought it best to test out a few different algorithms. I started out with the assumption that random forest or gradient-boosting would provide the best results, but it&#39;s generally good to test assumptions. . Though I did not test a very wide range of algorithms, I figured it would be worth it anyways to see if any of the more commonly-used ones showed any promise. . Somewhat to my surprise, the best models from tuning and cross-validation were basically on par with the default, non-cross-validated random forest. . The fact that the baseline was trained without using cross-validation could be one reason. In other words, the best model from the search might generalize better to new data; it could outperform the default one on the test dataset. . Or, it could be a result of the parameters and their ranges I searched—i.e. not giving a wide enough initial distribution. However, the wider the distribution, if the number of iterations is relatively low, the more &quot;noise&quot; might be present in the results of the search. I wanted to wait until I chose an algorithm to try increasing the number of search iterations significantly. . Here is the full list of algorithms I trained during the validation and iteration process, and their best F1 score: . Logistic regression: ~.76 | K-nearest neighbors: ~.69 | Random forest: ~.80 | Gradient-boosted decision tree: ~.80 | . Random forest . # Tune hyperparameters using cross-validation rf3_pipe = Pipeline([ (&quot;encoder&quot;, ce.OrdinalEncoder()), (&quot;imputer&quot;, IterativeImputer(random_state=92)), (&quot;rfc&quot;, RandomForestClassifier(random_state=92)), ]) rf3_params = { &quot;imputer__initial_strategy&quot;: [&quot;median&quot;, &quot;most_frequent&quot;], &quot;imputer__max_iter&quot;: randint(16, 40), &quot;imputer__tol&quot;: uniform(0.001, 0.05), &quot;imputer__n_nearest_features&quot;: randint(2, 10), &quot;imputer__imputation_order&quot;: [&quot;ascending&quot;, &quot;roman&quot;, &quot;random&quot;], &quot;rfc__n_estimators&quot;: randint(80, 300), &quot;rfc__max_depth&quot;: randint(6, 32), &quot;rfc__min_samples_split&quot;: uniform(0, 1), } # Define the search using parameter distros above rf3_search = RandomizedSearchCV( rf3_pipe, param_distributions=rf3_params, n_iter=5, cv=5, scoring=&#39;f1&#39;, verbose=10, return_train_score=True, n_jobs=-1, random_state=92, ) # Train default random forest rf3_search.fit(X_train, y_train) # Best combination of hyperparameters and their resulting f1 score print(&quot;Best hyperparameters&quot;, rf3_search.best_params_) print(&quot;F1 score&quot;, rf3_search.best_score_) . Fitting 5 folds for each of 5 candidates, totalling 25 fits . [Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers. [Parallel(n_jobs=-1)]: Done 5 tasks | elapsed: 10.2s [Parallel(n_jobs=-1)]: Done 10 tasks | elapsed: 12.0s [Parallel(n_jobs=-1)]: Done 17 tasks | elapsed: 15.3s [Parallel(n_jobs=-1)]: Done 21 out of 25 | elapsed: 16.7s remaining: 3.2s [Parallel(n_jobs=-1)]: Done 25 out of 25 | elapsed: 17.9s finished . Best hyperparameters {&#39;imputer__imputation_order&#39;: &#39;random&#39;, &#39;imputer__initial_strategy&#39;: &#39;median&#39;, &#39;imputer__max_iter&#39;: 19, &#39;imputer__n_nearest_features&#39;: 4, &#39;imputer__tol&#39;: 0.02223640657375538, &#39;rfc__max_depth&#39;: 21, &#39;rfc__min_samples_split&#39;: 0.00799211859966853, &#39;rfc__n_estimators&#39;: 205} F1 score 0.7874357962768823 . Nearest Neighbors . nn_pipe = Pipeline([ (&quot;encoder&quot;, ce.OrdinalEncoder()), (&quot;imputer&quot;, IterativeImputer(random_state=92)), (&quot;nn&quot;, KNeighborsClassifier()), ]) nn_params = { &quot;imputer__initial_strategy&quot;: [&quot;median&quot;, &quot;most_frequent&quot;], &quot;imputer__max_iter&quot;: randint(16, 40), &quot;imputer__tol&quot;: uniform(0.001, 0.05), &quot;imputer__n_nearest_features&quot;: randint(2, 10), &quot;imputer__imputation_order&quot;: [&quot;ascending&quot;, &quot;roman&quot;, &quot;random&quot;], &quot;nn__n_neighbors&quot;: randint(2, 20), &quot;nn__weights&quot;: [&quot;uniform&quot;, &quot;distance&quot;], &quot;nn__algorithm&quot;: [&quot;ball_tree&quot;, &quot;kd_tree&quot;], &quot;nn__leaf_size&quot;: randint(20, 50), } # Define the search using parameter distros above nn_search = RandomizedSearchCV( nn_pipe, param_distributions=nn_params, n_iter=5, cv=5, scoring=&quot;f1&quot;, verbose=10, return_train_score=True, n_jobs=-1, random_state=92, ) # Train default random forest nn_search.fit(X_train, y_train) # Best combination of hyperparameters and their resulting f1 score print(&quot;Best hyperparameters&quot;, nn_search.best_params_) print(&quot;F1 score&quot;, nn_search.best_score_) . Fitting 5 folds for each of 5 candidates, totalling 25 fits . [Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers. [Parallel(n_jobs=-1)]: Done 5 tasks | elapsed: 4.3s [Parallel(n_jobs=-1)]: Done 10 tasks | elapsed: 8.1s [Parallel(n_jobs=-1)]: Done 17 tasks | elapsed: 13.3s [Parallel(n_jobs=-1)]: Done 21 out of 25 | elapsed: 15.1s remaining: 2.9s [Parallel(n_jobs=-1)]: Done 25 out of 25 | elapsed: 16.6s finished . Best hyperparameters {&#39;imputer__imputation_order&#39;: &#39;ascending&#39;, &#39;imputer__initial_strategy&#39;: &#39;most_frequent&#39;, &#39;imputer__max_iter&#39;: 17, &#39;imputer__n_nearest_features&#39;: 8, &#39;imputer__tol&#39;: 0.040172486631023935, &#39;nn__algorithm&#39;: &#39;ball_tree&#39;, &#39;nn__leaf_size&#39;: 29, &#39;nn__n_neighbors&#39;: 15, &#39;nn__weights&#39;: &#39;uniform&#39;} F1 score 0.6971155846391321 . It seems that Random Forest is quite a bit better of an algorithm for this problem than k-nearest neighbors. Therefore, I won&#39;t be moving forward with nearest neighbors. . The last algorithm I&#39;ll try is a gradient-boosted decision tree classifier from XGBoost: XGBClassifier. . Gradient Boosting . Training a gradient-boosted decision tree using XGBoost. . Though I don&#39;t have a record of every single iteration of the below classifier search, the method I used to tune is to basically look at the values of each parameter, and moved the search range to more closely fit around those values. . I was surprised to find that my initial attempts at training the XGBClassifier had about the same performance as the default random forest with the newly-engineered features. . As I mentioned above, one hypothesis of what was causing the discrepancy (or lack thereof: I assumed gradient-boosting would increase the performance, which maybe wasn&#39;t a sound assumption), could be the simple fact that the randomized search doesn&#39;t cover every possibility. To test this, I increased the number of iterations and let &#39;er rip! . from xgboost import XGBClassifier xgb1_pipe = Pipeline([ (&quot;encoder&quot;, ce.OrdinalEncoder()), (&quot;imputer&quot;, IterativeImputer(random_state=92)), (&quot;xgb&quot;, XGBClassifier(random_state=92)), ]) xgb1_params = { &quot;imputer__initial_strategy&quot;: [&quot;median&quot;, &quot;most_frequent&quot;], &quot;imputer__max_iter&quot;: randint(16, 45), &quot;imputer__tol&quot;: uniform(0.02, 0.04), &quot;imputer__n_nearest_features&quot;: randint(2, 10), &quot;imputer__imputation_order&quot;: [&quot;ascending&quot;, &quot;roman&quot;, &quot;random&quot;], &quot;xgb__n_estimators&quot;: randint(80, 160), &quot;xgb__max_depth&quot;: randint(18, 48), &quot;xgb__learning_rate&quot;: uniform(0.05, .5), } # Define the search using parameter distros above xgb1_search = RandomizedSearchCV( xgb1_pipe, param_distributions=xgb1_params, n_iter=10, cv=4, scoring=&quot;f1&quot;, verbose=10, return_train_score=True, n_jobs=-1, random_state=92, ) # Train default random forest xgb1_search.fit(X_train, y_train) # Best combination of hyperparameters and their resulting f1 score print(&quot;F1 score&quot;, xgb1_search.best_score_) print(&quot;Best hyperparameters:&quot;) for param, val in xgb1_search.best_params_.items(): print( f&quot;{param}: {val}&quot;) . Fitting 4 folds for each of 10 candidates, totalling 40 fits . [Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers. [Parallel(n_jobs=-1)]: Done 1 tasks | elapsed: 20.2s [Parallel(n_jobs=-1)]: Done 4 tasks | elapsed: 39.4s [Parallel(n_jobs=-1)]: Done 9 tasks | elapsed: 1.5min [Parallel(n_jobs=-1)]: Done 14 tasks | elapsed: 2.0min [Parallel(n_jobs=-1)]: Done 21 tasks | elapsed: 2.9min [Parallel(n_jobs=-1)]: Done 28 tasks | elapsed: 3.6min [Parallel(n_jobs=-1)]: Done 37 tasks | elapsed: 5.2min [Parallel(n_jobs=-1)]: Done 40 out of 40 | elapsed: 5.6min finished . F1 score 0.7942245127829319 Best hyperparameters: imputer__imputation_order: ascending imputer__initial_strategy: median imputer__max_iter: 31 imputer__n_nearest_features: 3 imputer__tol: 0.03492326452711587 xgb__learning_rate: 0.10726406674881385 xgb__max_depth: 27 xgb__n_estimators: 100 . Final algorithm . Even with 40 total fits (4 cross-validation folds, 10 iterations) the gradient-boosted classifier did not really outperform the random forest by any significant margin. Given the additional complexity and computation required for an XGBoost (gradient-boosting) model, I decided to move forward with the random forest classifier. . To continue testing the hypothesis that my initial number of iterations was too low for the search to converge on a good combination of hyperparameters, I trained more random forests with higher numbers of search iterations. . To my continued surprise, even after many, many more iterations, the absolute best F1 score I got still hovered right around ~.80. . rf4_pipe = Pipeline([ (&quot;encoder&quot;, ce.OrdinalEncoder()), (&quot;imputer&quot;, IterativeImputer(random_state=92, n_nearest_features=3)), (&quot;rfc&quot;, RandomForestClassifier(random_state=92)), ]) rf4_params = { &quot;imputer__initial_strategy&quot;: [&quot;median&quot;, &quot;most_frequent&quot;], &quot;imputer__max_iter&quot;: randint(8, 20), &quot;imputer__tol&quot;: uniform(0.01, 0.04), &quot;imputer__imputation_order&quot;: [&quot;ascending&quot;, &quot;roman&quot;, &quot;random&quot;], &quot;rfc__n_estimators&quot;: randint(140, 200), &quot;rfc__max_depth&quot;: randint(6, 18), &quot;rfc__min_samples_split&quot;: randint(6, 14), &quot;rfc__min_impurity_decrease&quot;: uniform(0, .01), } # Define the search using parameter distros above rf4_search = RandomizedSearchCV( rf4_pipe, param_distributions=rf4_params, n_iter=15, cv=5, scoring=&#39;f1&#39;, verbose=10, return_train_score=True, n_jobs=-1, random_state=92, ) # Train default random forest rf4_search.fit(X_train, y_train) # Best combination of hyperparameters and their resulting f1 score print(&#39;F1 score&#39;, rf4_search.best_score_) print(&#39;Best hyperparameters:&#39;) for param, val in rf4_search.best_params_.items(): print( f&quot;{param}: {val}&quot;) . Fitting 5 folds for each of 15 candidates, totalling 75 fits . [Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers. [Parallel(n_jobs=-1)]: Done 5 tasks | elapsed: 5.9s [Parallel(n_jobs=-1)]: Done 10 tasks | elapsed: 8.5s [Parallel(n_jobs=-1)]: Done 17 tasks | elapsed: 14.5s [Parallel(n_jobs=-1)]: Done 24 tasks | elapsed: 17.9s [Parallel(n_jobs=-1)]: Done 33 tasks | elapsed: 25.6s [Parallel(n_jobs=-1)]: Done 42 tasks | elapsed: 31.3s [Parallel(n_jobs=-1)]: Done 53 tasks | elapsed: 39.3s [Parallel(n_jobs=-1)]: Done 64 tasks | elapsed: 46.6s [Parallel(n_jobs=-1)]: Done 75 out of 75 | elapsed: 59.7s finished . F1 score 0.7908519538332393 Best hyperparameters: imputer__imputation_order: ascending imputer__initial_strategy: median imputer__max_iter: 12 imputer__tol: 0.029121541514263292 rfc__max_depth: 12 rfc__min_impurity_decrease: 0.00010629484293753989 rfc__min_samples_split: 11 rfc__n_estimators: 195 . rf4 = rf4_search.best_estimator_ y_pred_test_rf4 = rf4.predict(X_test) # Compute evaluation metrics print(&quot;Random forest 4 eval metrics:&quot;) print(&quot; Accuracy:&quot;, accuracy_score(y_test, y_pred_test_rf4)) print(&quot; F1 score:&quot;, f1_score(y_test, y_pred_test_rf4)) . Random forest 4 eval metrics: Accuracy: 0.7623330607795039 F1 score: 0.7795753286147623 . . Results and Interpretation . At this point, I did not want to waste any more time or compute power trying to find that last little bit of optimization. So, it was time to evaluate the final model, interpret its predictions, then deploy it. . Evaluating the final model . Finally it was time to unlock the test data and see how the model does. The result was an F1 score slightly north of .78. . Here is the confusion matrix derived from the model&#39;s predictions on the test set: . y_pred_test_rf4 = rf4_search.predict(X_test) # Compute evaluation metrics print(&quot;Random forest 4 eval metrics:&quot;) print(&quot; Accuracy:&quot;, accuracy_score(y_test, y_pred_test_rf4)) print(&quot; F1 score:&quot;, f1_score(y_test, y_pred_test_rf4)) . Random forest 4 eval metrics: Accuracy: 0.7672390297083674 F1 score: 0.7833587011669203 . plt.figure(figsize=(14, 12)) plt.title(&quot;Confusion matrix: final random forest, test data&quot;) font = {&#39;family&#39; : &#39;normal&#39;, &#39;weight&#39; : &#39;bold&#39;, &#39;size&#39; : 22} plt.rc(&#39;font&#39;, **font) plot_confusion_matrix(y_test, y_pred_test_rf4); . train_id = X_train.reset_index()[&quot;index&quot;] test_id = X_test.reset_index()[&quot;index&quot;] . test_id.head() . 0 1042 1 10779 2 10072 3 15540 4 13007 Name: index, dtype: int64 . # Process the test data transformers_2 = Pipeline([ (&quot;encoder&quot;, rf4_search.best_estimator_[&quot;encoder&quot;]), (&quot;imputer&quot;, rf4_search.best_estimator_[&quot;imputer&quot;]), ]) # Encode and impute X_test_transform = transformers_2.transform(X_test) class_index = 1 # Make predictions with the trained random forest y_pred_proba_rf4 = rf4_search.predict_proba(X_test_transform)[:, class_index] # ROC AUC score ranges from 0-1; higher is better print(f&#39;Test ROC AUC for class {class_index}:&#39;) print(roc_auc_score(y_test, y_pred_proba_rf4)) . Test ROC AUC for class 1: 0.8489270700686261 . X_test = X_test.reset_index() X_test.head() . index num_ratings num_reviews avg_rating num_pages series 1_rating_count 2_rating_count 3_rating_count 4_rating_count 5_rating_count publish_year republish the_title has_subtitle title_char_count title_longest_word author_name_count rating_ratio . 0 1042 | 112871.0 | 4674.0 | 4.00 | 317.0 | 0 | 1593.0 | 5439.0 | 24255.0 | 41993.0 | 39591.0 | 1997.0 | 1 | 0 | 0 | 29 | 7 | 2 | 0.086193 | . 1 10779 | 364.0 | 47.0 | 3.97 | 431.0 | 0 | 12.0 | 16.0 | 72.0 | 134.0 | 130.0 | 2016.0 | 1 | 0 | 0 | 18 | 8 | 2 | 0.106061 | . 2 10072 | 49.0 | 12.0 | 4.18 | 294.0 | 0 | 0.0 | 3.0 | 10.0 | 11.0 | 25.0 | 1994.0 | 0 | 0 | 0 | 16 | 8 | 2 | 0.083333 | . 3 15540 | 150.0 | 14.0 | 3.87 | 200.0 | 1 | 0.0 | 2.0 | 46.0 | 71.0 | 31.0 | 1994.0 | 0 | 0 | 0 | 16 | 6 | 2 | 0.019608 | . 4 13007 | 19322.0 | 213.0 | 4.09 | 268.0 | 1 | 285.0 | 986.0 | 3758.0 | 5984.0 | 8309.0 | 1993.0 | 1 | 0 | 1 | 40 | 11 | 2 | 0.088925 | . X_test.shape, test_id.shape, y_pred_test_rf4.shape, y_pred_proba_rf4.shape, y_test.shape . ((3669, 19), (3669,), (3669,), (3669,), (3669,)) . y_test.reset_index().head() . index fiction . 0 1042 | 1 | . 1 10779 | 0 | . 2 10072 | 0 | . 3 15540 | 1 | . 4 13007 | 1 | . # Create new dataframe to compare the predictions to the actual df = pd.DataFrame({ &quot;index&quot;: test_id, &quot;pred&quot;: y_pred_test_rf4, &quot;pred_proba&quot;: y_pred_proba_rf4, }) print(df.shape) df.head() . (3669, 3) . index pred pred_proba . 0 1042 | 1 | 0.926309 | . 1 10779 | 1 | 0.550764 | . 2 10072 | 0 | 0.159053 | . 3 15540 | 1 | 0.561363 | . 4 13007 | 1 | 0.529370 | . df = df.merge(y_test.reset_index()) print(df.shape) df.head() . (3669, 4) . index pred pred_proba fiction . 0 1042 | 1 | 0.926309 | 1 | . 1 10779 | 1 | 0.550764 | 0 | . 2 10072 | 0 | 0.159053 | 0 | . 3 15540 | 1 | 0.561363 | 1 | . 4 13007 | 1 | 0.529370 | 1 | . df = df.merge( X_test, how=&#39;left&#39; ) print(df.shape) df.head() . (3669, 22) . index pred pred_proba fiction num_ratings num_reviews avg_rating num_pages series 1_rating_count 2_rating_count 3_rating_count 4_rating_count 5_rating_count publish_year republish the_title has_subtitle title_char_count title_longest_word author_name_count rating_ratio . 0 1042 | 1 | 0.926309 | 1 | 112871.0 | 4674.0 | 4.00 | 317.0 | 0 | 1593.0 | 5439.0 | 24255.0 | 41993.0 | 39591.0 | 1997.0 | 1 | 0 | 0 | 29 | 7 | 2 | 0.086193 | . 1 10779 | 1 | 0.550764 | 0 | 364.0 | 47.0 | 3.97 | 431.0 | 0 | 12.0 | 16.0 | 72.0 | 134.0 | 130.0 | 2016.0 | 1 | 0 | 0 | 18 | 8 | 2 | 0.106061 | . 2 10072 | 0 | 0.159053 | 0 | 49.0 | 12.0 | 4.18 | 294.0 | 0 | 0.0 | 3.0 | 10.0 | 11.0 | 25.0 | 1994.0 | 0 | 0 | 0 | 16 | 8 | 2 | 0.083333 | . 3 15540 | 1 | 0.561363 | 1 | 150.0 | 14.0 | 3.87 | 200.0 | 1 | 0.0 | 2.0 | 46.0 | 71.0 | 31.0 | 1994.0 | 0 | 0 | 0 | 16 | 6 | 2 | 0.019608 | . 4 13007 | 1 | 0.529370 | 1 | 19322.0 | 213.0 | 4.09 | 268.0 | 1 | 285.0 | 986.0 | 3758.0 | 5984.0 | 8309.0 | 1993.0 | 1 | 0 | 1 | 40 | 11 | 2 | 0.088925 | . df_wrong = df[df[&quot;pred&quot;] != df[&quot;fiction&quot;]] print(df_wrong.shape) df_wrong.head() . (854, 22) . index pred pred_proba fiction num_ratings num_reviews avg_rating num_pages series 1_rating_count 2_rating_count 3_rating_count 4_rating_count 5_rating_count publish_year republish the_title has_subtitle title_char_count title_longest_word author_name_count rating_ratio . 1 10779 | 1 | 0.550764 | 0 | 364.0 | 47.0 | 3.97 | 431.0 | 0 | 12.0 | 16.0 | 72.0 | 134.0 | 130.0 | 2016.0 | 1 | 0 | 0 | 18 | 8 | 2 | 0.106061 | . 7 5437 | 1 | 0.617281 | 0 | 2650.0 | 92.0 | 3.96 | 304.0 | 0 | 49.0 | 154.0 | 590.0 | 927.0 | 930.0 | 1987.0 | 1 | 1 | 0 | 45 | 9 | 2 | 0.109316 | . 9 9875 | 1 | 0.601987 | 0 | 3073.0 | 82.0 | 4.25 | 659.0 | 0 | 22.0 | 65.0 | 469.0 | 1091.0 | 1426.0 | 1976.0 | 1 | 1 | 0 | 17 | 8 | 2 | 0.034565 | . 10 2513 | 0 | 0.306036 | 1 | 1471.0 | 345.0 | 3.80 | 308.0 | 1 | 96.0 | 142.0 | 298.0 | 358.0 | 577.0 | 2011.0 | 0 | 0 | 0 | 8 | 8 | 2 | 0.254545 | . 13 3274 | 0 | 0.130823 | 1 | 60322.0 | 2623.0 | 4.04 | 365.0 | 0 | 377.0 | 2063.0 | 12538.0 | 25237.0 | 20107.0 | 2005.0 | 1 | 0 | 1 | 46 | 9 | 2 | 0.053811 | . df_wrong = df_wrong.merge(books.iloc[df_wrong[&quot;index&quot;]][&quot;title&quot;].reset_index()) print(df_wrong.shape) df_wrong.head() . (854, 23) . index pred pred_proba fiction num_ratings num_reviews avg_rating num_pages series 1_rating_count 2_rating_count 3_rating_count 4_rating_count 5_rating_count publish_year republish the_title has_subtitle title_char_count title_longest_word author_name_count rating_ratio title . 0 10779 | 1 | 0.550764 | 0 | 364.0 | 47.0 | 3.97 | 431.0 | 0 | 12.0 | 16.0 | 72.0 | 134.0 | 130.0 | 2016.0 | 1 | 0 | 0 | 18 | 8 | 2 | 0.106061 | Whispers of Heaven | . 1 5437 | 1 | 0.617281 | 0 | 2650.0 | 92.0 | 3.96 | 304.0 | 0 | 49.0 | 154.0 | 590.0 | 927.0 | 930.0 | 1987.0 | 1 | 1 | 0 | 45 | 9 | 2 | 0.109316 | The Condition of the Working Class in England | . 2 9875 | 1 | 0.601987 | 0 | 3073.0 | 82.0 | 4.25 | 659.0 | 0 | 22.0 | 65.0 | 469.0 | 1091.0 | 1426.0 | 1976.0 | 1 | 1 | 0 | 17 | 8 | 2 | 0.034565 | The Portable Jung | . 3 2513 | 0 | 0.306036 | 1 | 1471.0 | 345.0 | 3.80 | 308.0 | 1 | 96.0 | 142.0 | 298.0 | 358.0 | 577.0 | 2011.0 | 0 | 0 | 0 | 8 | 8 | 2 | 0.254545 | Creatura | . 4 3274 | 0 | 0.130823 | 1 | 60322.0 | 2623.0 | 4.04 | 365.0 | 0 | 377.0 | 2063.0 | 12538.0 | 25237.0 | 20107.0 | 2005.0 | 1 | 0 | 1 | 46 | 9 | 2 | 0.053811 | Smoke and Mirrors: Short Fiction and Illusions | . df_wrong.sample(n=60, random_state=92).sort_values(by=&quot;pred_proba&quot;) . index pred pred_proba fiction num_ratings num_reviews avg_rating num_pages series 1_rating_count 2_rating_count 3_rating_count 4_rating_count 5_rating_count publish_year republish the_title has_subtitle title_char_count title_longest_word author_name_count rating_ratio title . 113 6021 | 0 | 0.081860 | 1 | 42.0 | 19.0 | 3.83 | NaN | 0 | 2.0 | 1.0 | 9.0 | 20.0 | 10.0 | 2013.0 | 1 | 0 | 1 | 62 | 8 | 2 | 0.100000 | Raised by Hand, Lifted by the Tides: A Souther... | . 671 10557 | 0 | 0.151352 | 1 | 146.0 | 19.0 | 3.84 | 96.0 | 0 | 4.0 | 6.0 | 42.0 | 52.0 | 42.0 | 1999.0 | 1 | 0 | 1 | 38 | 10 | 2 | 0.106383 | Sanctuary: A Tale of Life in the Woods | . 589 7885 | 0 | 0.201978 | 1 | 24.0 | 8.0 | 3.75 | 270.0 | 0 | 3.0 | 0.0 | 6.0 | 6.0 | 9.0 | 2011.0 | 0 | 0 | 0 | 41 | 17 | 2 | 0.200000 | Our Father, Who Art Out There...Somewhere | . 601 17626 | 0 | 0.294917 | 1 | 334.0 | 41.0 | 4.17 | 228.0 | 0 | 6.0 | 7.0 | 55.0 | 121.0 | 145.0 | 1996.0 | 1 | 0 | 0 | 32 | 9 | 2 | 0.048872 | Farewell, I&#39;m Bound to Leave You | . 254 11700 | 0 | 0.295858 | 1 | 80.0 | 4.0 | 3.86 | 23.0 | 0 | 1.0 | 7.0 | 21.0 | 24.0 | 27.0 | 1980.0 | 0 | 0 | 0 | 49 | 15 | 3 | 0.156863 | Walt Disney&#39;s Winnie-the-Pooh and the Honey Patch | . 287 6715 | 0 | 0.298807 | 1 | 735.0 | 62.0 | 3.88 | 287.0 | 1 | 24.0 | 48.0 | 146.0 | 293.0 | 224.0 | 2015.0 | 0 | 0 | 0 | 11 | 6 | 2 | 0.139265 | Dark Waters | . 832 15914 | 0 | 0.311132 | 1 | 114.0 | 19.0 | 4.28 | 207.0 | 0 | 2.0 | 0.0 | 14.0 | 46.0 | 52.0 | 1991.0 | 1 | 0 | 0 | 21 | 9 | 2 | 0.020408 | John Bauers sagovärld | . 102 15751 | 0 | 0.348599 | 1 | 390.0 | 61.0 | 4.35 | 290.0 | 0 | 3.0 | 16.0 | 36.0 | 121.0 | 214.0 | 2010.0 | 1 | 0 | 0 | 7 | 7 | 2 | 0.056716 | Damaged | . 87 13051 | 0 | 0.348673 | 1 | 4661.0 | 582.0 | 4.03 | 320.0 | 1 | 20.0 | 131.0 | 1001.0 | 2043.0 | 1466.0 | 2012.0 | 0 | 0 | 1 | 48 | 9 | 2 | 0.043032 | A Blink of the Screen: Collected Shorter Fiction | . 103 2304 | 0 | 0.350009 | 1 | 61.0 | 18.0 | 3.98 | 430.0 | 0 | 1.0 | 6.0 | 9.0 | 22.0 | 23.0 | 2012.0 | 1 | 0 | 0 | 6 | 6 | 2 | 0.155556 | Eulogy | . 334 941 | 0 | 0.379592 | 1 | 48225.0 | 3677.0 | 3.82 | 96.0 | 0 | 1169.0 | 3440.0 | 12147.0 | 17776.0 | 13693.0 | 1992.0 | 1 | 0 | 1 | 38 | 10 | 3 | 0.146462 | Flatland: A Romance of Many Dimensions | . 634 15440 | 0 | 0.396910 | 1 | 1765.0 | 57.0 | 4.50 | 288.0 | 1 | 5.0 | 21.0 | 147.0 | 501.0 | 1091.0 | 2011.0 | 0 | 0 | 0 | 14 | 5 | 2 | 0.016332 | Silva Rerum II | . 814 13551 | 0 | 0.402496 | 1 | 582.0 | 37.0 | 4.44 | 208.0 | 1 | 4.0 | 11.0 | 59.0 | 157.0 | 351.0 | 2010.0 | 0 | 0 | 0 | 35 | 7 | 2 | 0.029528 | Sir Quinlan and the Swords of Valor | . 297 3884 | 0 | 0.418613 | 1 | 16641.0 | 1170.0 | 3.92 | 365.0 | 1 | 564.0 | 986.0 | 3299.0 | 6178.0 | 5614.0 | 2015.0 | 1 | 0 | 0 | 12 | 6 | 3 | 0.131445 | Split Second | . 82 7418 | 0 | 0.425630 | 1 | 194.0 | 15.0 | 4.29 | 241.0 | 1 | 3.0 | 0.0 | 19.0 | 87.0 | 85.0 | 2018.0 | 1 | 1 | 0 | 25 | 9 | 2 | 0.017442 | The Guardians of Eastgate | . 397 15351 | 0 | 0.425805 | 1 | 16018.0 | 176.0 | 4.40 | 30.0 | 1 | 177.0 | 454.0 | 2171.0 | 3256.0 | 9960.0 | 2002.0 | 0 | 0 | 0 | 29 | 8 | 2 | 0.047745 | Disney&#39;s Beauty and the Beast | . 189 7629 | 0 | 0.443745 | 1 | 416.0 | 45.0 | 3.90 | 321.0 | 1 | 9.0 | 28.0 | 101.0 | 134.0 | 144.0 | 2014.0 | 0 | 1 | 0 | 18 | 8 | 2 | 0.133094 | The Damascus Cover | . 682 2806 | 0 | 0.445734 | 1 | 33628.0 | 5291.0 | 3.96 | 247.0 | 1 | 1366.0 | 2091.0 | 6039.0 | 11055.0 | 13077.0 | 2011.0 | 0 | 1 | 0 | 66 | 15 | 3 | 0.143254 | The Girl Who Circumnavigated Fairyland in a Sh... | . 570 9871 | 0 | 0.447479 | 1 | 537.0 | 73.0 | 3.93 | 95.0 | 0 | 9.0 | 31.0 | 122.0 | 200.0 | 175.0 | 1994.0 | 1 | 0 | 1 | 19 | 7 | 2 | 0.106667 | Lost Boy: A Novella | . 21 77 | 0 | 0.448205 | 1 | 12371.0 | 604.0 | 4.39 | 1204.0 | 1 | 276.0 | 415.0 | 1238.0 | 2758.0 | 7684.0 | 2003.0 | 0 | 1 | 1 | 91 | 8 | 2 | 0.066175 | The Black Jewels Trilogy: Daughter of the Bloo... | . 354 12538 | 0 | 0.491755 | 1 | 136.0 | 26.0 | 3.18 | 246.0 | 1 | 15.0 | 28.0 | 36.0 | 31.0 | 26.0 | 2019.0 | 1 | 0 | 0 | 16 | 4 | 2 | 0.754386 | Cry for the Moon | . 425 2326 | 0 | 0.498150 | 1 | 5890.0 | 262.0 | 4.34 | 352.0 | 0 | 105.0 | 133.0 | 607.0 | 1883.0 | 3162.0 | 2002.0 | 0 | 0 | 0 | 13 | 7 | 2 | 0.047175 | गोदान [Godan] | . 127 13055 | 1 | 0.506951 | 0 | 1655.0 | 70.0 | 4.38 | 268.0 | 0 | 11.0 | 28.0 | 187.0 | 516.0 | 913.0 | 1998.0 | 1 | 1 | 0 | 30 | 9 | 2 | 0.027292 | The Collected Poems, 1957-1982 | . 607 11992 | 1 | 0.515010 | 0 | 319.0 | 129.0 | 3.21 | 261.0 | 0 | 35.0 | 65.0 | 81.0 | 75.0 | 63.0 | 2017.0 | 0 | 0 | 0 | 13 | 4 | 2 | 0.724638 | Bad Girl Gone | . 178 9523 | 1 | 0.525453 | 0 | 352.0 | 56.0 | 3.77 | 248.0 | 0 | 36.0 | 49.0 | 42.0 | 57.0 | 168.0 | 2007.0 | 1 | 1 | 0 | 20 | 9 | 3 | 0.377778 | The Way To Happiness | . 331 9214 | 1 | 0.526713 | 0 | 1145.0 | 142.0 | 4.07 | 400.0 | 0 | 13.0 | 44.0 | 227.0 | 432.0 | 429.0 | 2015.0 | 0 | 1 | 0 | 17 | 6 | 2 | 0.066202 | The Bright Effect | . 517 6526 | 1 | 0.530070 | 0 | 2334.0 | 212.0 | 4.48 | 272.0 | 1 | 9.0 | 22.0 | 195.0 | 729.0 | 1379.0 | 1993.0 | 1 | 0 | 0 | 17 | 6 | 2 | 0.014706 | Man of the Family | . 93 7070 | 1 | 0.530858 | 0 | 23008.0 | 4618.0 | 3.97 | 292.0 | 0 | 580.0 | 1521.0 | 4642.0 | 7526.0 | 8739.0 | 2012.0 | 0 | 0 | 0 | 10 | 5 | 3 | 0.129173 | Tiger Lily | . 299 18145 | 1 | 0.536059 | 0 | 10863.0 | 458.0 | 4.22 | 70.0 | 1 | 71.0 | 285.0 | 1859.0 | 3645.0 | 5003.0 | 2011.0 | 1 | 0 | 0 | 19 | 8 | 2 | 0.041166 | A Taste of Midnight | . 121 8047 | 1 | 0.565218 | 0 | 5554.0 | 235.0 | 4.27 | 288.0 | 0 | 80.0 | 154.0 | 774.0 | 1699.0 | 2847.0 | 2007.0 | 1 | 1 | 0 | 17 | 6 | 2 | 0.051474 | The Joy of Living | . 290 8710 | 1 | 0.570749 | 0 | 6641.0 | 517.0 | 4.11 | 384.0 | 0 | 240.0 | 294.0 | 911.0 | 2257.0 | 2939.0 | 1999.0 | 1 | 1 | 0 | 14 | 10 | 3 | 0.102771 | The Seamstress | . 506 10612 | 1 | 0.573332 | 0 | 18092.0 | 2547.0 | 4.04 | 213.0 | 1 | 268.0 | 854.0 | 3865.0 | 6094.0 | 7011.0 | 2010.0 | 1 | 0 | 0 | 12 | 6 | 2 | 0.085616 | Girl, Stolen | . 140 18189 | 1 | 0.576180 | 0 | 13032.0 | 549.0 | 3.99 | 133.0 | 1 | 146.0 | 544.0 | 2898.0 | 5117.0 | 4327.0 | 2013.0 | 0 | 0 | 0 | 7 | 7 | 2 | 0.073062 | Lockout | . 157 651 | 1 | 0.596103 | 0 | 31487.0 | 872.0 | 4.15 | 216.0 | 0 | 274.0 | 968.0 | 5389.0 | 12062.0 | 12794.0 | 2001.0 | 0 | 1 | 0 | 26 | 8 | 2 | 0.049968 | The Universe in a Nutshell | . 325 15922 | 1 | 0.606033 | 0 | 409.0 | 41.0 | 4.18 | 256.0 | 0 | 1.0 | 11.0 | 79.0 | 141.0 | 177.0 | 1987.0 | 1 | 1 | 0 | 11 | 7 | 2 | 0.037736 | The Gypsies | . 568 17045 | 1 | 0.610479 | 0 | 858.0 | 46.0 | 4.11 | 362.0 | 1 | 14.0 | 34.0 | 163.0 | 282.0 | 365.0 | 2008.0 | 1 | 0 | 0 | 9 | 4 | 2 | 0.074189 | Wild Jinx | . 37 1930 | 1 | 0.629782 | 0 | 24176.0 | 220.0 | 4.40 | 256.0 | 0 | 236.0 | 533.0 | 2924.0 | 6133.0 | 14350.0 | 2014.0 | 1 | 1 | 0 | 28 | 7 | 2 | 0.037543 | The Power of a Praying Woman | . 701 2625 | 1 | 0.653492 | 0 | 95178.0 | 8167.0 | 4.17 | 256.0 | 0 | 2090.0 | 4260.0 | 14235.0 | 28948.0 | 45645.0 | 2017.0 | 0 | 1 | 0 | 23 | 7 | 2 | 0.085129 | The Sun and Her Flowers | . 762 17691 | 1 | 0.655321 | 0 | 691.0 | 68.0 | 4.12 | 464.0 | 0 | 4.0 | 31.0 | 126.0 | 247.0 | 283.0 | 2005.0 | 1 | 1 | 0 | 10 | 6 | 2 | 0.066038 | The Armada | . 670 7554 | 1 | 0.661281 | 0 | 4004.0 | 123.0 | 4.09 | 260.0 | 1 | 75.0 | 205.0 | 721.0 | 1290.0 | 1713.0 | 2004.0 | 1 | 1 | 0 | 19 | 8 | 2 | 0.093240 | The Art of Dreaming | . 309 11141 | 1 | 0.689412 | 0 | 45130.0 | 5103.0 | 3.90 | 372.0 | 1 | 1070.0 | 2624.0 | 10165.0 | 17305.0 | 13966.0 | 2012.0 | 1 | 1 | 0 | 20 | 4 | 2 | 0.118129 | The Name of the Star | . 35 1949 | 1 | 0.709350 | 0 | 119301.0 | 1493.0 | 4.34 | 178.0 | 0 | 1253.0 | 2956.0 | 15995.0 | 33145.0 | 65952.0 | 2003.0 | 1 | 0 | 0 | 10 | 7 | 2 | 0.042474 | Falling Up | . 642 8398 | 1 | 0.723719 | 0 | 1245.0 | 81.0 | 3.89 | 350.0 | 1 | 18.0 | 69.0 | 306.0 | 494.0 | 358.0 | 2003.0 | 1 | 0 | 0 | 33 | 7 | 2 | 0.102113 | Dilbert and the Way of the Weasel | . 372 1672 | 1 | 0.729363 | 0 | 2467412.0 | 25823.0 | 4.13 | 283.0 | 0 | 56904.0 | 105275.0 | 408368.0 | 783656.0 | 1113209.0 | 1993.0 | 1 | 1 | 0 | 25 | 5 | 2 | 0.085498 | The Diary of a Young Girl | . 451 16099 | 1 | 0.733403 | 0 | 7491.0 | 149.0 | 4.08 | 358.0 | 1 | 80.0 | 293.0 | 1641.0 | 2390.0 | 3087.0 | 2002.0 | 0 | 0 | 0 | 8 | 8 | 2 | 0.068103 | Tapestry | . 25 11502 | 1 | 0.753236 | 0 | 8158.0 | 1142.0 | 4.07 | 224.0 | 0 | 52.0 | 314.0 | 1611.0 | 3216.0 | 2965.0 | 2018.0 | 1 | 1 | 0 | 17 | 5 | 2 | 0.059214 | The Order of Time | . 395 8714 | 1 | 0.760106 | 0 | 9966.0 | 761.0 | 3.93 | 316.0 | 0 | 210.0 | 680.0 | 2252.0 | 3266.0 | 3558.0 | 2000.0 | 0 | 0 | 0 | 9 | 5 | 2 | 0.130422 | Old Magic | . 576 14384 | 1 | 0.761098 | 0 | 12224.0 | 367.0 | 4.06 | 256.0 | 1 | 104.0 | 488.0 | 2724.0 | 4118.0 | 4790.0 | 2006.0 | 1 | 0 | 0 | 17 | 5 | 2 | 0.066457 | Once Upon a Curse | . 806 15838 | 1 | 0.786317 | 0 | 1385.0 | 116.0 | 3.40 | 292.0 | 1 | 79.0 | 187.0 | 444.0 | 448.0 | 227.0 | 2008.0 | 1 | 0 | 0 | 11 | 5 | 2 | 0.394074 | Night Child | . 438 16960 | 1 | 0.791309 | 0 | 2074.0 | 390.0 | 3.08 | 363.0 | 0 | 180.0 | 421.0 | 745.0 | 503.0 | 225.0 | 2014.0 | 1 | 0 | 0 | 4 | 4 | 2 | 0.825549 | Tape | . 459 14595 | 1 | 0.801759 | 0 | 18656.0 | 1395.0 | 4.11 | 310.0 | 0 | 109.0 | 568.0 | 3475.0 | 7492.0 | 7012.0 | 2002.0 | 1 | 1 | 0 | 17 | 5 | 2 | 0.046677 | The Water is Wide | . 99 12264 | 1 | 0.805392 | 0 | 13515.0 | 528.0 | 3.98 | 304.0 | 0 | 325.0 | 879.0 | 2907.0 | 4005.0 | 5399.0 | 2003.0 | 1 | 0 | 0 | 8 | 8 | 3 | 0.128031 | Journals | . 404 10196 | 1 | 0.808873 | 0 | 32726.0 | 6933.0 | 3.96 | 344.0 | 0 | 559.0 | 1679.0 | 6551.0 | 13614.0 | 10323.0 | 2018.0 | 0 | 0 | 0 | 17 | 7 | 2 | 0.093495 | To Kill a Kingdom | . 238 1310 | 1 | 0.850194 | 0 | 31074.0 | 1918.0 | 3.97 | 378.0 | 0 | 735.0 | 1526.0 | 6513.0 | 11551.0 | 10749.0 | 2002.0 | 1 | 0 | 0 | 13 | 6 | 2 | 0.101390 | Silent Spring | . 259 9934 | 1 | 0.851483 | 0 | 3142.0 | 142.0 | 4.07 | 320.0 | 1 | 36.0 | 130.0 | 630.0 | 1127.0 | 1219.0 | 1989.0 | 1 | 0 | 0 | 17 | 8 | 2 | 0.070759 | An Actor Prepares | . 191 17455 | 1 | 0.864813 | 0 | 3922.0 | 103.0 | 3.91 | 430.0 | 1 | 47.0 | 191.0 | 1014.0 | 1486.0 | 1184.0 | 2006.0 | 1 | 0 | 0 | 18 | 5 | 2 | 0.089139 | Ruler of the Realm | . 548 14559 | 1 | 0.865144 | 0 | 3351.0 | 217.0 | 3.71 | 384.0 | 0 | 77.0 | 302.0 | 1019.0 | 1055.0 | 898.0 | 2004.0 | 1 | 0 | 0 | 26 | 8 | 2 | 0.194060 | Sword of the Rightful King | . 702 14809 | 1 | 0.877158 | 0 | 3674.0 | 337.0 | 3.91 | 304.0 | 0 | 32.0 | 181.0 | 900.0 | 1529.0 | 1032.0 | 1997.0 | 1 | 1 | 0 | 27 | 7 | 2 | 0.083171 | The Serpent and the Rainbow | . 204 5310 | 1 | 0.885058 | 0 | 12596.0 | 1170.0 | 3.76 | 208.0 | 0 | 232.0 | 811.0 | 3636.0 | 4969.0 | 2948.0 | 1998.0 | 1 | 0 | 0 | 31 | 6 | 3 | 0.131742 | How Proust Can Change Your Life | . 310 6466 | 1 | 0.911392 | 0 | 12946.0 | 868.0 | 3.90 | 215.0 | 0 | 119.0 | 600.0 | 3215.0 | 5567.0 | 3445.0 | 2001.0 | 1 | 1 | 0 | 22 | 5 | 2 | 0.079783 | The Road to Wigan Pier | . Predicted probabilities . One way to interpret a trained model is to inspect the predictions that the model made on the test set, either individually or through various descriptive statistics and visualizations. A key component to this is the probabilities underlying each of the predictions. . By looking at predicted probabilities, I can look at instances when the model was very sure or not sure of its predictions, and if those predictions were correct or not. By looking at the cases where the model made mistakes, I can hopefully pick out patterns and glean some insight why the model made those mistakes. . If I find that the model consistently makes mistakes that don&#39;t make much sense, that could mean there is some more optimization to be done or weakness to be addressed. . False negatives . The first one that jumped out at me was subtitles. The model finds that feature very useful, yet of course there are some fiction books with subtitles. This mistake makes sense to me, and for the benefit that the feature adds to the model, it is worth incorrectly classifying some fiction books as nonfiction. One way to get around this would be to engineer another feature or three that attempts to be the complement to that feature, catching the instances when fiction books have subtitles. . Another feature that causes a similar type of error is the title character count. According to the feature importances table (and plot generated from the table&#39;s data, shown below) of the final random forest model, &#39;title_char_count&#39; is also a very important feature. I can see that many of the false negatives (predicted nonfiction, actually fiction) have a high title character count. . rf4 = rf4_search.best_estimator_[&quot;rfc&quot;] importances = pd.Series(rf4.feature_importances_, X_train.columns) # Plot feature importances n = 20 plt.figure(figsize=(10,n/2)) plt.title(f&#39;Top {n} features&#39;) importances.sort_values()[-n:].plot.barh(color=&#39;grey&#39;); . The following SHAP plots formalize my observations and provide more insight by showing the effect that the value of each feature had on the model&#39;s prediction. The following group are all false negative predictions, meaning the model predicted nonfiction when it was actually fiction. . I also wanted to see what features caused false negatives when the book had neither a high character count nor a subtitle. The third plot below shows the top three as republish, avg_rating, and publish_year. I was surprised to see republish, as it does not seem to be all that important to the model overall. The other two seem to be relatively good predictors, particularly avg_rating. . . False positives . The other side of the confusion matrix again corroborates my initial observation. Many of the same features that provided some additional predictive power also mean some additional mistakes. However, the overall effect was net positive. If it wasn&#39;t these features causing the incorrect predictions, it would be others, and there would be more incorrect predictions overall. . . . Deployment . So there you have it, the process of training a machine learning model, from start to finish. The only thing left to do is to deploy it. . I thought it would be interesting to be able to set up a dashboard that can be used to give dynamic inputs to the model and see the resulting prediction. I decided to build the dashboard using Plotly Dash and deploy it to Heroku. . So you don&#39;t have to scroll all the way up, here&#39;s another link to the live app: print(fiction). . Shoutout to HTML5UP for the template on which I based the design of the app. I made some minor modifications to it, but most of the design was straight from one of his templates. . If you made it this far, I salute you. . As always, thank you for reading and I&#39;ll see you in the next one! .",
            "url": "https://tobias-fyi.github.io/fastfyi/workshop/print-fiction",
            "relUrl": "/workshop/print-fiction",
            "date": " • Oct 25, 2019"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://tobias-fyi.github.io/fastfyi/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://tobias-fyi.github.io/fastfyi/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}