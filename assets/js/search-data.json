{
  
    
        "post0": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://tobias-fyi.github.io/fastfyi/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://tobias-fyi.github.io/fastfyi/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Post Here",
            "content": ". Introduction . The Problem . Reddit is an expansive site. Anyone who has spent any significant amount of time on it knows what I mean. There is a subreddit for seemingly every topic anyone could ever want to discuss or even think about (and many that most do not want think about). . Reddit is a powerful site; a tool for connecting and sharing information with like- or unlike-minded individuals around the world. When used well, it can be a very useful resource. . On the other hand, the deluge of information that&#39;s constantly piling into the pages of can be overwhelming and lead to wasted time. As with any tool, it can be used for good or for not-so-good. . A common problem that Redditors experience, particularly those who are relatively new to the site, is where to post content. Given that there are subreddits for just about everything, with wildly varying degrees of specificity it can be quite overwhelming trying to find the best place for each post. . Just to illustrate the point, some subreddits get weirdly specific. I won&#39;t go into the really weird or NSFW, but here are some good examples of what I mean by specific: . r/Borderporn | r/BreadStapledtoTrees | r/birdswitharms | r/totallynotrobots | . ...need I go on? (If you&#39;re curious and/or want to be entertained indefinitely, here is a thread with these and much, much more.) . Most of the time when a post is deemed irrelevant to a particular subreddit, it will simply be removed by moderators or a bot. However, depending on the subreddit and how welcoming they are to newbies, sometimes it can lead to very unfriendly responses and/or bans. . So how does one go about deciding where to post or pose a question? . Post Here aims to take the guesswork out of this process. . . The Solution . The goal with the Post Here app, as mentioned, is to provide a tool that makes it quick and easy to find the most appropriate subreddits for any given post. A user would simply provide the title and text of the their prospective post and the app would provide the user with a list of subreddit recommendations. . Recommendations are produced by a model attempts to predict which subreddit a given post would belong to. The model was built using Scikit-learn, and was trained on a large dataset of reddit posts. In order to serve the recommendations to the web app, an API was built using Flask and deployed to Heroku. . The live version of the app is linked below. . Post Here:The Subreddit Suggester . My Role . I worked on the Post Here app with a remote, interdisciplinary team of data scientists, machine learning engineers, and web developers. I was one of two machine learning engineers on the team, responsible for the entire process of building and training the machine learning models. The two data scientists on the team were primarily responsible for building and deploying the API. . The main challenge we ran into, which directed much of the iterative process, was scope management. . At this point in my machine learning journey, this was one of the larger datasets that I&#39;d taken on. Uncompressed, the dataset we used was over 800mb of mostly natural language text. The dataset and the time constraint—we had less than four full days of work to finish the project—were the primary causes of the challenges we ended up facing. . With such a dataset, one important concept we had to keep in mind was the curse of dimensionality, which is basically a title for the various problems and phenomena that occur when dealing with extremely highly dimensional datasets. When processed, a natural language dataset of this size would likely fall prey to this curse and may prove somewhat unwieldy without large amounts of processing power. . I ended up researching and applying various methods of addressing this problem in order to fit the processing/modeling pipeline on the free Heroku Dyno, with a memory limit of 500mb, while preserving adequate performance. Many of our deployments failed because the pipeline, when loaded into memory on the server, exceeded that limit. . One important tradeoff we had to wrangle with was how much, and in what ways we could limit the dataset—i.e. how many classes to try and predict, and how many observations per class to include when training. The original dataset contains data for 1,000 subreddits. It was not within the scope of a a four-day project to build a classification model of a caliber that could accurately classify 1,000 classes. . In the beginning, we did try to build a basic model trained on all 1,000 classes. But with the time and processing power I had, it proved to be untenable. In the end, we settled for a model that classified text into 305 subreddits with a test precision-at-k of .75, .88, and .92 for &#39;k&#39; of 1, 3, and 5, respectively. . Imports and Configuration . import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns import os import janitor . from sklearn.model_selection import train_test_split from sklearn.preprocessing import LabelEncoder from sklearn.ensemble import RandomForestClassifier from sklearn.naive_bayes import MultinomialNB from sklearn.feature_selection import chi2, SelectKBest # === NLP Imports === # from sklearn.feature_extraction.text import TfidfVectorizer . # Configure pandas display settings pd.options.display.max_colwidth = 100 # Set random seed seed = 92 . . The Data . The dataset we ended up using to train the recommendation system is called the Reddit Self-Post Classification Task dataset, available on Kaggle thanks to Evolution AI. The full dataset clocks in at over 800mb, containing 1,013,000 rows: 1,000 posts each from 1,013 subreddits. . For more details on the dataset, including a nice interactive plot of all of the subreddits, refer to Evolution AI&#39;s blog post. . Wrangling and Exploration . First, I needed to reduce the size of the dataset. I defined a subset of 12 categories which I thought were most relevant to the task at hand, and used that list to do the initial pruning. Those 12 categories left me with 305 unique subreddits and 305,000 rows. The list I used was as follows: . health | profession | electronics | hobby | writing/stories | advice/question | social_group | stem | parenting | books | finance/money | travel | . Next, I took a random sample of those 305,000 rows. The result was a dataset with 91,500 rows, now consisting of between 250 and 340 rows per subreddit. If I tried to use all of the features (tokens, or words) that resulted from this corpus, even in its reduced state, it would still result in a serialized vocabulary and/or model too large for our free Heroku Dyno. However, the features used in the final model can be chosen based on how useful they are for the classification. . According to the dataset preview on Kaggle, there are quite a large number of missing values in each of the features—12%, 25%, and 39% of the subreddit, title, and selftext columns, respectively. However, I did not find any sign of those null values in the dataset nor mention of them in the dataset&#39;s companion blog post or article. I chocked it up to an error in the Kaggle preview. . Finally, I went about doing some basic preprocessing to get the data ready for vectorization. As described in the description page on Kaggle, newline and tab characters were replaced with their HTML equivalents, &lt;lb&gt; and &lt;tab&gt;. I removed those and other HTML entities using a simple regular expression. I also concatenated title and selftext into a single text feature in order to process them together. . rspct = pd.read_csv(&quot;assets/data/rspct.tsv&quot;, sep=&quot; t&quot;) print(rspct.shape) rspct.head(3) . (1013000, 4) . id subreddit title selftext . 0 6d8knd | talesfromtechsupport | Remember your command line switches... | Hi there, &lt;lb&gt;The usual. Long time lerker, first time poster, be kind etc. Sorry if this isn&#39;t the right place...&lt;lb&gt;&lt;lb&gt;Alright. Here&#39;s the story. I&#39;m an independent developer who produces my ow... | . 1 58mbft | teenmom | So what was Matt &quot;addicted&quot; to? | Did he ever say what his addiction was or is he still chugging beers while talking about how sober he is?&lt;lb&gt;&lt;lb&gt;Edited to add: As an addict myself, anyone I know whose been an addict doesn&#39;t drin... | . 2 8f73s7 | Harley | No Club Colors | Funny story. I went to college in Las Vegas. This was before I knew anything about motorcycling whatsoever. Me and some college buddies would always go out on the strip to the dance clubs. We alwa... | . Nulls . Kaggle says that 12%, 25%, and 39% of the subreddit, title, and selftext columns are null, respectively. If that is indeed the case, they did not get read into the dataframe correctly. However, it could be an error on Kaggle&#39;s part, seeing as there is no mention of these anywhere else in the description or blog post or article, nor sign of them during my explorations. . rspct.isnull().sum() . id 0 subreddit 0 title 0 selftext 0 dtype: int64 . Preprocessing . To prune the list of subreddits, I&#39;ll load in the subreddit_info.csv file, join, then choose a certain number of categories (category_1) to filter on. . info = pd.read_csv(&quot;assets/data/subreddit_info.csv&quot;, usecols=[&quot;subreddit&quot;, &quot;category_1&quot;, &quot;category_2&quot;]) print(info.shape) info.head() . (3394, 3) . subreddit category_1 category_2 . 0 whatsthatbook | advice/question | book | . 1 CasualConversation | advice/question | broad | . 2 Clairvoyantreadings | advice/question | broad | . 3 DecidingToBeBetter | advice/question | broad | . 4 HelpMeFind | advice/question | broad | . rspct = pd.merge(rspct, info, on=&quot;subreddit&quot;).drop(columns=[&quot;id&quot;]) print(rspct.shape) rspct.head() . (1013000, 5) . subreddit title selftext category_1 category_2 . 0 talesfromtechsupport | Remember your command line switches... | Hi there, &lt;lb&gt;The usual. Long time lerker, first time poster, be kind etc. Sorry if this isn&#39;t the right place...&lt;lb&gt;&lt;lb&gt;Alright. Here&#39;s the story. I&#39;m an independent developer who produces my ow... | writing/stories | tech support | . 1 talesfromtechsupport | I work IT for a certain clothing company and they use iPod Touchs for scanning some items | [ME]- Thank you fro calling Store support, this is David. How may I help you?&lt;lb&gt;&lt;lb&gt;[Store]- Yeah, my iPod is frozen&lt;lb&gt;&lt;lb&gt;[ME]- Okay, can I have you hold down the power and the home button at t... | writing/stories | tech support | . 2 talesfromtechsupport | It... It says right there on the screen...? | Hi guys! &lt;lb&gt;&lt;lb&gt;&amp;amp;nbsp;&lt;lb&gt;&lt;lb&gt;LTL, FTP - all that jazz. Starting you off with a short one.&lt;lb&gt;&lt;lb&gt;&amp;amp;nbsp;&lt;lb&gt;&lt;lb&gt;I&#39;m the senior supporter at a smaller tech company with clients all over t... | writing/stories | tech support | . 3 talesfromtechsupport | The computers not working. FIX IT NOW! | Hey there TFTS! This is my second time posting. I don&#39;t work for any tech support company, but I do have friends, family and teachers at school that have no idea how stuff works.&lt;lb&gt;&lt;lb&gt;This tale ... | writing/stories | tech support | . 4 talesfromtechsupport | A Storm of Unreasonableness | Usual LTL, FTP. I have shared this story on a different site, but after reading TFTS for sometime I figured it&#39;d belong here as well. &lt;lb&gt;&lt;lb&gt;This is from when I worked at a 3rd party call center ... | writing/stories | tech support | . rspct.isnull().sum() # That&#39;s a good sign . subreddit 0 title 0 selftext 0 category_1 0 category_2 0 dtype: int64 . rspct[&quot;category_1&quot;].value_counts() . video_game 100000 tv_show 68000 health 58000 profession 56000 software 52000 electronics 51000 music 43000 sports 40000 sex/relationships 31000 hobby 30000 geo 29000 crypto 29000 company/website 28000 other 27000 anime/manga 26000 drugs 23000 writing/stories 22000 programming 21000 arts 21000 autos 20000 advice/question 18000 education 17000 animals 17000 politics/viewpoint 16000 social_group 16000 card_game 15000 food/drink 15000 stem 14000 hardware/tools 14000 parenting 13000 religion/supernatural 13000 books 12000 appearance 11000 finance/money 10000 board_game 9000 meta 9000 movies 7000 rpg 7000 travel 5000 Name: category_1, dtype: int64 . keep_cats = [ &quot;health&quot;, &quot;profession&quot;, &quot;electronics&quot;, &quot;hobby&quot;, &quot;writing/stories&quot;, &quot;advice/question&quot;, &quot;social_group&quot;, &quot;stem&quot;, &quot;parenting&quot;, &quot;books&quot;, &quot;finance/money&quot;, &quot;travel&quot;, ] # Prune dataset to above categories # Overwriting to save memory rspct = rspct[rspct[&quot;category_1&quot;].isin(keep_cats)] print(rspct.shape) print(&quot;Unique subreddits:&quot;, len(rspct[&quot;subreddit&quot;].unique())) rspct.head(2) . (305000, 5) Unique subreddits: 305 . subreddit title selftext category_1 category_2 . 0 talesfromtechsupport | Remember your command line switches... | Hi there, &lt;lb&gt;The usual. Long time lerker, first time poster, be kind etc. Sorry if this isn&#39;t the right place...&lt;lb&gt;&lt;lb&gt;Alright. Here&#39;s the story. I&#39;m an independent developer who produces my ow... | writing/stories | tech support | . 1 talesfromtechsupport | I work IT for a certain clothing company and they use iPod Touchs for scanning some items | [ME]- Thank you fro calling Store support, this is David. How may I help you?&lt;lb&gt;&lt;lb&gt;[Store]- Yeah, my iPod is frozen&lt;lb&gt;&lt;lb&gt;[ME]- Okay, can I have you hold down the power and the home button at t... | writing/stories | tech support | . rspct = rspct.sample(frac=.3, random_state=seed) print(rspct.shape) rspct.head() . (91500, 5) . subreddit title selftext category_1 category_2 . 594781 stepparents | Ex Wants Toddler Son (2M) to Meet Her AP/SO - x-post from /r/divorce | Quick background: My soon-to-be ex-wife (26F) and I (27M) have been separated for about 5 months now. She has been in a serious relationship her AP (23M) whom she met and cheated on me with 6 mont... | parenting | step parenting | . 617757 bigseo | Do we raise our pricing? | I took a management role at an agency. We&#39;re way, way under $500/mo for SEO pricing - and I&#39;m embarrassed to say that we&#39;re hurting for business. Seems to me that it&#39;s a struggle to get clients to... | profession | seo | . 642368 chemistry | Mac vs. PC? | Hello, all! I am currently a senior in high school and in the fall I will be going to SUNY Geneseo, majoring in chemistry and minoring in mathematics. &lt;lb&gt;&lt;lb&gt;Geneseo requires it’s students to get... | stem | chemistry | . 325221 migraine | Beer as an aural abortive? | Hiya folks,&lt;lb&gt;&lt;lb&gt;I&#39;ve been a migraine sufferer pretty much my whole life. For me intense auras, numbness, confusion, the inability to speak or see is BY FAR the worst aspect of the ordeal. When ... | health | migraine | . 524939 MouseReview | Recommend office mouse | I was hoping you folks could help me out. Here&#39;s my situation and requirements:&lt;lb&gt;&lt;lb&gt;* I don&#39;t play games at all&lt;lb&gt;* Budget $30.00 or less&lt;lb&gt;* Shape as close to old Microsoft Intellimouse Opti... | electronics | computer mouse | . # Concatenate title and selftext rspct[&quot;text&quot;] = rspct[&quot;title&quot;] + &quot; &quot; + rspct[&quot;selftext&quot;] # Drop categories rspct = rspct.drop(columns=[&quot;category_1&quot;, &quot;category_2&quot;, &quot;title&quot;, &quot;selftext&quot;]) . # NOTE: takes a couple minutes to run rspct[&quot;text&quot;] = rspct[&quot;text&quot;].str.replace(&quot;(&lt;lb&gt;)*|(&lt;tab&gt;)*|(&amp;amp;)*|(nbsp;)*&quot;, &quot;&quot;) rspct.head() . subreddit text . 594781 stepparents | Ex Wants Toddler Son (2M) to Meet Her AP/SO - x-post from /r/divorce Quick background: My soon-to-be ex-wife (26F) and I (27M) have been separated for about 5 months now. She has been in a serious... | . 617757 bigseo | Do we raise our pricing? I took a management role at an agency. We&#39;re way, way under $500/mo for SEO pricing - and I&#39;m embarrassed to say that we&#39;re hurting for business. Seems to me that it&#39;s a s... | . 642368 chemistry | Mac vs. PC? Hello, all! I am currently a senior in high school and in the fall I will be going to SUNY Geneseo, majoring in chemistry and minoring in mathematics. Geneseo requires it’s students to... | . 325221 migraine | Beer as an aural abortive? Hiya folks,I&#39;ve been a migraine sufferer pretty much my whole life. For me intense auras, numbness, confusion, the inability to speak or see is BY FAR the worst aspect o... | . 524939 MouseReview | Recommend office mouse I was hoping you folks could help me out. Here&#39;s my situation and requirements:* I don&#39;t play games at all* Budget $30.00 or less* Shape as close to old Microsoft Intellimou... | . subreddits = rspct[&quot;subreddit&quot;].unique() print(len(subreddits)) subreddits[:50] . 305 . array([&#39;stepparents&#39;, &#39;bigseo&#39;, &#39;chemistry&#39;, &#39;migraine&#39;, &#39;MouseReview&#39;, &#39;Malazan&#39;, &#39;Standup&#39;, &#39;preppers&#39;, &#39;Invisalign&#39;, &#39;whatsthisplant&#39;, &#39;CrohnsDisease&#39;, &#39;KingkillerChronicle&#39;, &#39;OccupationalTherapy&#39;, &#39;churning&#39;, &#39;Libraries&#39;, &#39;acting&#39;, &#39;eczema&#39;, &#39;Allergies&#39;, &#39;bigboobproblems&#39;, &#39;AskAnthropology&#39;, &#39;psychotherapy&#39;, &#39;WayfarersPub&#39;, &#39;synthesizers&#39;, &#39;StopGaming&#39;, &#39;stopsmoking&#39;, &#39;eroticauthors&#39;, &#39;amazonecho&#39;, &#39;TalesFromThePizzaGuy&#39;, &#39;rheumatoid&#39;, &#39;homestead&#39;, &#39;VoiceActing&#39;, &#39;FinancialCareers&#39;, &#39;Sleepparalysis&#39;, &#39;ProtectAndServe&#39;, &#39;short&#39;, &#39;Fibromyalgia&#39;, &#39;teaching&#39;, &#39;PlasticSurgery&#39;, &#39;insomnia&#39;, &#39;PLC&#39;, &#39;rapecounseling&#39;, &#39;peacecorps&#39;, &#39;paintball&#39;, &#39;autism&#39;, &#39;Nanny&#39;, &#39;Plumbing&#39;, &#39;Epilepsy&#39;, &#39;asmr&#39;, &#39;fatpeoplestories&#39;, &#39;Magic&#39;], dtype=object) . rspct[&quot;subreddit&quot;].value_counts() . Dreams 340 Gifts 337 HFY 333 Cubers 333 cassetteculture 333 ... foreignservice 265 WritingPrompts 263 immigration 263 TryingForABaby 262 Physics 250 Name: subreddit, Length: 305, dtype: int64 . . Modeling . Label Encoding . # This process naively transforms each class of the target into a number le = LabelEncoder() # Instantiate a new encoder instance le.fit(y_train) # Fit it on training label data # Transform both using the trained instance y_train = le.transform(y_train) y_val = le.transform(y_val) y_test = le.transform(y_test) y_train[:8] . array([ 92, 140, 65, 90, 278, 65, 272, 212]) . Vectorization . A vectorizer is used to extract numerical features (information) from a corpus of natural language text. I used a bag-of-words method of vectorization, which for the most part, disregards grammar. . The output of this vectorizer is a document-term matrix, with the documents (observations, or rows) on one axes and the terms (words, bigrams) on the other. This matrix can be thought of as a sort of vocabulary, or text-number translator. . At first, the &quot;vocabulary&quot; derived from the corpus using the vectorizer was the largest object when serialized. Luckily, there are many options and parameters available to reduce its size, most of which are simply different methods for reducing the number of features (terms) it contains. . One option is to put a hard limit of 100,000 on the number of features in the vocabulary. This is a simple, naive limit on the generated features, and thus, the resulting vocabulary size. . I decided to remove stopwords before vectorization in hopes that this would reduce the size of the vector vocabulary. To my initial surprise, removing the stop words (using NLTK&#39;s list) actually increased the size of the serialized vocab from 59mb to 76mb. . After some consideration, I found this to be a reasonable result. I figured that many of the stop words are short (&quot;I&quot;, &quot;me&quot;, &quot;my&quot;, etc.), and their removal caused the average length of words (and therefore bigrams as well) in the vocab to increase. While this may not account for the entirety of the difference, this provides some intuition as to why there is a difference. . Although the vocab without stop words was larger, I ended up using it anyways because it provided an extra ~0.01 in the precision-at-k score of the final model. . lengths = [] three_or_below = [] for word in stop_words: lengths.append(len(word)) if len(word) &lt;= 4: three_or_below.append(len(word)) print(f&quot;There are {len(stop_words)} stop words in the list.&quot;) print(f&quot;{len(three_or_below)} are 4 chars long or shorter.&quot;) print(f&quot;Average length is: {np.mean(lengths)}.&quot;) . There are 179 stop words in the list. 109 are 4 chars long or shorter. Average length is: 4.229050279329609. . tfidf = TfidfVectorizer( max_features=100000, min_df=10, ngram_range=(1,2), stop_words=stop_words, # Use nltk&#39;s stop words ) # Fit the vectorizer on the feature column to create vocab (doc-term matrix) vocab = tfidf.fit(X_train) # Get sparse document-term matrices X_train_sparse = vocab.transform(X_train) X_val_sparse = vocab.transform(X_val) X_test_sparse = vocab.transform(X_test) X_train_sparse.shape, X_val_sparse.shape, X_test_sparse.shape . ((65880, 63588), (7320, 63588), (18300, 63588)) . Feature Selection . As mentioned previously, the size of the corpus means the dimensionality of the featureset after vectorization will be very high. I passed in 100,000 as the maximum number of features to the vectorizer, limiting the initial size of the vocab. However, the features would have to be reduced more before training the model, as it is generally not good practice to have a larger number of features (100,000) than observations (91,500). . To reduce it down from that 100,000, I used a process called select k best, which does exactly what it sounds like: selects a certain number of the best features. The key aspect of this process is how to measure the value of the features; how to find which ones are the &quot;best&quot;. The scoring function I used in this case is called ch2 (chi-squared). . This function calculates chi-squared statistics between each feature and the target, measuring the dependence, or correlation, between them. The intuition here is that features which are more correlated with the target are more likely to be useful to the model. . I played around with some different values for the maximum number of features to be selected. Ultimately, I was once again limited by the size of the free Heroku Dyno, and settled on 20,000. This allowed the deployment to go smoothly while retaining enough information for the model to have adequate performance. . selector = SelectKBest(chi2, k=20000) selector.fit(X_train_sparse, y_train) X_train_select = selector.transform(X_train_sparse) X_val_select = selector.transform(X_val_sparse) X_test_select = selector.transform(X_test_sparse) X_train_select.shape, X_val_select.shape, X_test_select.shape . ((65880, 20000), (7320, 20000), (18300, 20000)) . Model validation . In this case, the model has a target that it is attempting to predict—a supervised problem. Therefore, the performance can be measured on validation and test sets. . To test out the recommendations I copied some posts and put them through the prediction pipeline to see what kinds of subreddits were getting recommended. For the most part, the predictions were decent. . The cases where the recommendations were a little less than ideal happened when I pulled example posts from subreddits that were not in the training data. The model generally did a good job recommending similar subreddits. . Baseline . For the baseline model, I decided to go with a basic random forest. This choice was somewhat arbitrary, though I was curious to see how a random forest would do with such a high target cardinality (number of classes/categories). . The baseline precision-at-k metrics for the random forest on the validation set were .54, .63, and .65, for k of 1, 3, and 5, respectively. . def precision_at_k(y_true, y_pred, k=5): y_true = np.array(y_true) y_pred = np.array(y_pred) y_pred = np.argsort(y_pred, axis=1) y_pred = y_pred[:, ::-1][:, :k] arr = [y in s for y, s in zip(y_true, y_pred)] return np.mean(arr) . rfc = RandomForestClassifier(max_depth=32, n_jobs=-1, n_estimators=200) rfc.fit(X_train_select, y_train) . RandomForestClassifier(max_depth=32, n_estimators=200, n_jobs=-1) . y_pred_proba_rfc = rfc.predict_proba(X_val_select) # For each prediction, find the index with the highest probability y_pred_rfc = np.argmax(y_pred_proba_rfc, axis=1) y_pred_rfc[:10] . array([296, 139, 177, 78, 12, 177, 161, 216, 40, 31]) . print(&quot;Validation scores:&quot;) print(&quot; precision@1 =&quot;, np.mean(y_val == y_pred_rfc)) print(&quot; precision@3 =&quot;, precision_at_k(y_val, y_pred_proba_rfc, 3)) print(&quot; precision@5 =&quot;, precision_at_k(y_val, y_pred_proba_rfc, 5)) . Validation scores: precision@1 = 0.5368852459016393 precision@3 = 0.6282786885245901 precision@5 = 0.6502732240437158 . Multinomial Naive Bayes . Multinomial naive Bayes is a probabilistic learning method for multinomially distributed data, and one of two classic naive Bayes algorithms used for text classification. I decided to iterate with this algorithm because it is meant for text classification tasks. . The precision-at-k metrics for the final Multinomial naive Bayes model on the validation set were .76, .88, and .9188, for k of 1, 3, and 5, respectively. Performance on the test set was nearly identical: .75, .88, and .9159. . nb = MultinomialNB(alpha=0.1) nb.fit(X_train_select, y_train) . MultinomialNB(alpha=0.1) . Evaluate on validation set . y_pred_proba_val = nb.predict_proba(X_val_select) # For each prediction, find index with highest probability y_pred_val = np.argmax(y_pred_proba_val, axis=1) y_pred_val[:10] . array([274, 139, 57, 78, 12, 17, 151, 216, 40, 171]) . print(&quot;Validation scores:&quot;) print(&quot; precision@1 =&quot;, np.mean(y_val == y_pred_val)) print(&quot; precision@3 =&quot;, precision_at_k(y_val, y_pred_proba_val, 3)) print(&quot; precision@5 =&quot;, precision_at_k(y_val, y_pred_proba_val, 5)) . Validation scores: precision@1 = 0.7599726775956284 precision@3 = 0.8834699453551913 precision@5 = 0.9188524590163935 . Evaluate on test set . y_pred_proba_test = nb.predict_proba(X_test_select) # For each prediction, find index with highest probability y_pred_test = np.argmax(y_pred_proba_test, axis=1) y_pred_test[:10] . array([ 97, 199, 116, 249, 43, 203, 263, 275, 96, 27]) . print(&quot;Test scores:&quot;) print(&quot; precision@1 =&quot;, np.mean(y_test == y_pred_test)) print(&quot; precision@3 =&quot;, precision_at_k(y_test, y_pred_proba_test, 3)) print(&quot; precision@5 =&quot;, precision_at_k(y_test, y_pred_proba_test, 5)) . Test scores: precision@1 = 0.7498360655737705 precision@3 = 0.8834426229508197 precision@5 = 0.9159562841530055 . Recommendations . The API should return a list of recommendations, not a single prediction. To accomplish this, I wrote a function that returns the top 5 most likely subreddits and their respective probabilities. . # The main functionality of the predict API endpoint def predict(title: str, submission_text: str, return_count: int = 5): &quot;&quot;&quot;Serve subreddit predictions. Parameters - title : string Title of post. submission_text : string Selftext that needs a home. return_count : integer The desired number of recommendations. Returns - Python dictionary formatted as follows: [{&#39;subreddit&#39;: &#39;PLC&#39;, &#39;proba&#39;: 0.014454}, ... {&#39;subreddit&#39;: &#39;Rowing&#39;, &#39;proba&#39;: 0.005206}] &quot;&quot;&quot; # Concatenate title and post text fulltext = str(title) + str(submission_text) # Vectorize the post -&gt; sparse doc-term matrix post_sparse = vocab.transform([fulltext]) # Feature selection post_select = selector.transform(post_sparse) # Generate predicted probabilities from trained model proba = nb.predict_proba(post_select) # Wrangle into correct format proba_dict = (pd .DataFrame(proba, columns=[le.classes_]) # Classes as column names .T # Transpose so column names become index .reset_index() # Pull out index into a column .rename(columns={&quot;level_0&quot;: &quot;name&quot;, 0: &quot;proba&quot;}) # Rename for aesthetics .sort_values(by=&quot;proba&quot;, ascending=False) # Sort by probability .iloc[:return_count] # n-top predictions to serve .to_dict(orient=&quot;records&quot;) ) proba_json = {&quot;predictions&quot;: proba_dict} return proba_json . title_science = &quot;&quot;&quot;Is there an evolutionary benefit to eating spicy food that lead to consumption across numerous cultures throughout history? Or do humans just like the sensation?&quot;&quot;&quot; post_science = &quot;&quot;&quot;I love spicy food and have done ever since I tried it. By spicy I mean HOT, like chilli peppers (we say spicy in England, I don&#39;t mean to state the obvious I&#39;m just not sure if that&#39;s a global term and I&#39;ve assumed too much before). I love a vast array of spicy foods from all around the world. I was just wondering if there was some evolutionary basis as to why spicy food managed to become some widely consumed historically. Though there seem to It way well be that we just like a tingly mouth, the simple things in life.&quot;&quot;&quot; science_recs = predict(title_science, post_science) science_recs . {&#39;predictions&#39;: [{&#39;name&#39;: &#39;GERD&#39;, &#39;proba&#39;: 0.009900622287634142}, {&#39;name&#39;: &#39;Allergies&#39;, &#39;proba&#39;: 0.009287774623361566}, {&#39;name&#39;: &#39;ibs&#39;, &#39;proba&#39;: 0.009150308633162811}, {&#39;name&#39;: &#39;AskAnthropology&#39;, &#39;proba&#39;: 0.009028660140513678}, {&#39;name&#39;: &#39;fatpeoplestories&#39;, &#39;proba&#39;: 0.00851982441049019}]} . title_pc = &quot;&quot;&quot;Looking for help with a build&quot;&quot;&quot; post_pc = &quot;&quot;&quot;I posted my wants for my build about 2 months ago. Ordered them and when I went to build it I was soooooo lost. It took 3 days to put things together because I was afraid I would break something when I finally got the parts together it wouldn’t start, I was so defeated. With virtually replacing everything yesterday it finally booted and I couldn’t be more excited!&quot;&quot;&quot; post_pc_recs = predict(title_pc, post_pc, 10) post_pc_recs . {&#39;predictions&#39;: [{&#39;name&#39;: &#39;lego&#39;, &#39;proba&#39;: 0.008418484170536294}, {&#39;name&#39;: &#39;rccars&#39;, &#39;proba&#39;: 0.008112076951648648}, {&#39;name&#39;: &#39;MechanicalKeyboards&#39;, &#39;proba&#39;: 0.0078335440606017}, {&#39;name&#39;: &#39;fightsticks&#39;, &#39;proba&#39;: 0.007633958584830632}, {&#39;name&#39;: &#39;Luthier&#39;, &#39;proba&#39;: 0.00716176615193545}, {&#39;name&#39;: &#39;modeltrains&#39;, &#39;proba&#39;: 0.007088134228361153}, {&#39;name&#39;: &#39;cade&#39;, &#39;proba&#39;: 0.007058109839673285}, {&#39;name&#39;: &#39;vandwellers&#39;, &#39;proba&#39;: 0.006700262151491209}, {&#39;name&#39;: &#39;cosplay&#39;, &#39;proba&#39;: 0.006536648725434882}, {&#39;name&#39;: &#39;homestead&#39;, &#39;proba&#39;: 0.006166832450007183}]} . post_title = &quot;&quot;&quot;What to do about java vs javascript&quot;&quot;&quot; post = &quot;&quot;&quot;I am a new grad looking for a job and currently in the process with a company for a junior backend engineer role. I was under the impression that the position was Javascript but instead it is actually Java. My general programming and &quot;leet code&quot; skills are pretty good, but my understanding of Java is pretty shallow. How can I use the next three days to best improve my general Java knowledge? Most resources on the web seem to be targeting complete beginners. Maybe a book I can skim through in the next few days? Edit: A lot of people are saying &quot;the company is a sinking ship don&#39;t even go to the interview&quot;. I just want to add that the position was always for a &quot;junior backend engineer&quot;. This company uses multiple languages and the recruiter just told me the incorrect language for the specific team I&#39;m interviewing for. I&#39;m sure they&#39;re mainly interested in seeing my understanding of good backend principles and software design, it&#39;s not a senior lead Java position.&quot;&quot;&quot; # === Test out the function === # post_pred = predict(post_title, post) # Default is 5 results post_pred . {&#39;predictions&#39;: [{&#39;name&#39;: &#39;cscareerquestions&#39;, &#39;proba&#39;: 0.516989539243874}, {&#39;name&#39;: &#39;devops&#39;, &#39;proba&#39;: 0.031462691062989795}, {&#39;name&#39;: &#39;interviews&#39;, &#39;proba&#39;: 0.02846504725703069}, {&#39;name&#39;: &#39;datascience&#39;, &#39;proba&#39;: 0.024227300545057697}, {&#39;name&#39;: &#39;bioinformatics&#39;, &#39;proba&#39;: 0.017516176338177075}]} . title_book = &quot;Looking for books with great plot twists&quot; # This one comes from r/suggestmeabook post2 = &quot;&quot;&quot;I&#39;ve been dreaming about writing my own stort story for a while but I want to give it an unexpected ending. I&#39;ve read lots of books, but none of them had the plot twist I want. I want to read books with the best plot twists, so that I can analyze what makes a good plot twist and write my own story based on that points. I don&#39;t like romance novels and I mostly enjoy sci-fi or historical books but anything beside romance novels would work for me, it doesn&#39;t have to be my type of novel. I&#39;m open to experience after all. I need your help guys. Thanks in advance.&quot;&quot;&quot; # === This time with 10 results === # post2_pred = predict(title_book, post2, 10) post2_pred . {&#39;predictions&#39;: [{&#39;name&#39;: &#39;suggestmeabook&#39;, &#39;proba&#39;: 0.4070015062748489}, {&#39;name&#39;: &#39;writing&#39;, &#39;proba&#39;: 0.14985778378113648}, {&#39;name&#39;: &#39;eroticauthors&#39;, &#39;proba&#39;: 0.07159411817054702}, {&#39;name&#39;: &#39;whatsthatbook&#39;, &#39;proba&#39;: 0.06062653422250441}, {&#39;name&#39;: &#39;ComicBookCollabs&#39;, &#39;proba&#39;: 0.027277418056905547}, {&#39;name&#39;: &#39;Malazan&#39;, &#39;proba&#39;: 0.019514923212723943}, {&#39;name&#39;: &#39;TheDarkTower&#39;, &#39;proba&#39;: 0.017162701613834493}, {&#39;name&#39;: &#39;DestructiveReaders&#39;, &#39;proba&#39;: 0.0151031907793204}, {&#39;name&#39;: &#39;WoT&#39;, &#39;proba&#39;: 0.011165890302931272}, {&#39;name&#39;: &#39;readyplayerone&#39;, &#39;proba&#39;: 0.007566597361383115}]} . Model deployment . As mentioned, the model, vocab, and feature selector were all serialized using Python&#39;s pickle module. In the Flask app, the pickled objects are loaded and ready for use, just like that. . I will go over the details of how the Flask app was set up in a separate blog post. . . Final Thoughts . For me, the most important and valuable aspects of this project were mainly surrounding the challenge of scope management. I constantly had to ask myself, &quot;What is the best version of this I can create given our limitations?&quot; . At first, I thought it would be feasible to predict all of the 1,000+ subreddits in the data, and wasted hours of valuable time attempting to do so. While I had tested various strategies of reducing the complexity of the model, the performance was rather terrible when it was trained on 100 or less examples of each of the complete list of subreddits. . The data scientist who I primarily worked with (we had one data scientist in addition to him and one other machine learning engineer, both of whom did not contribute significantly to the project) kept telling me that I should try reducing the number of classes first, allowing for more examples of each class and fewer classes for the model to predict. . Ultimately, this is the strategy that worked best, and I wasted valuable time by not listening to him the first few times he recommended that strategy. Good teamwork requires the members being humble and listening, something that I have taken to heart since the conclusion of this project. . Scope Management, Revisited . As time was very short while building this initial recommendation API, there are many things that we wished we could have done but simply did not have the time. Here are a few of the more obvious improvements that could be made. . The first, and most obvious one, is to simply deploy to a more powerful server, such as one hosted on AWS Elastic Beanstalk or EC2. This way, we could use the entire dataset to train an optimal model without worrying (as much) about memory limits. . Second, I could use a Scikit-learn pipeline to validate and tune hyperparameters using cross-validation, instead of a separate validation set. Also, this pipeline could be serialized as a single large object, rather than as separate pieces (encoder, vectorizer, feature selector, and classifier). As a final note for this particular train of thought, Joblib could potentially provide more efficient serialization than the Pickle module, allowing a more complex pipeline to be deployed on the same server. . Third, a model could&#39;ve been trained to classify the input post first into a broad category. Then, some sort of model could be used to to classify into a specific subreddit within that broad category. I&#39;m not sure about the feasibility of the second part of this idea, but thought it could be an interesting one to explore. . Lastly, different classes and calibers of models could have been tested for use in the various steps in the pipeline. In this case, I&#39;m referring primarily to using deep learning/neural networks. For example, word vectors could be generated with word embedding models such as Word2Vec. Or the process could be recreated with a library like PyTorch, and a framework like FastText. . I plan to explore at least some of these in separate blog posts. . As always, thank you for reading! I&#39;ll see you in the next one. .",
            "url": "https://tobias-fyi.github.io/fastfyi/2020/01/12/post-here.html",
            "relUrl": "/2020/01/12/post-here.html",
            "date": " • Jan 12, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://tobias-fyi.github.io/fastfyi/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://tobias-fyi.github.io/fastfyi/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}